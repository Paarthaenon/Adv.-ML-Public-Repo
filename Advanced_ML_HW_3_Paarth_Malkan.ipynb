{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Advanced ML HW #3 - Paarth Malkan",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Paarthaenon/Adv.-ML-Public-Repo/blob/master/Advanced_ML_HW_3_Paarth_Malkan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftx3zZt8OHvd",
        "colab_type": "text"
      },
      "source": [
        "## Can you use the following data to build....?\n",
        "1. A model with an embedding layer and dense layers (but w/ no layers meant for sequential data)\n",
        "2.  A model using Conv1d Layers\n",
        "3.  A model with one sequential layer (LSTM or GRU)\n",
        "4. A model with stacked sequential layers (LSTM or GRU)\n",
        "- A model with bidirectional sequential layers \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoqSwgStNzT4",
        "colab_type": "code",
        "outputId": "35d16952-b5e4-4082-a857-12413e1781a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Importing libraries \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn as sk\n",
        "from statistics import mean\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense\n",
        "from sklearn import preprocessing\n",
        "from keras.preprocessing import sequence\n",
        "from keras import layers\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.layers import SimpleRNN, LSTM"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iq_K4NCOKXSK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "e40104ef-e4b5-4538-9584-f37af8081f8b"
      },
      "source": [
        "# Importing data\n",
        "\n",
        "bbc_df = pd.read_csv(\"https://storage.googleapis.com/dataset-uploader/bbc/bbc-text.csv\")\n",
        "print (bbc_df.head())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        category                                               text\n",
            "0           tech  tv future in the hands of viewers with home th...\n",
            "1       business  worldcom boss  left books alone  former worldc...\n",
            "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
            "3          sport  yeading face newcastle in fa cup premiership s...\n",
            "4  entertainment  ocean s twelve raids box office ocean s twelve...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEvPiiYkIKWv",
        "colab_type": "text"
      },
      "source": [
        "### 1)  Visualize the categories of your target variable and describe the dataset generally (the data includes news articles from the BBC news.)  A simple description is fine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRmyYd_G0RIr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "6b3c95d5-6b97-4f21-bf5e-9bcc30ee4bbc"
      },
      "source": [
        "# Visualizing target (category) variable\n",
        "\n",
        "category_counts = list(bbc_df.category.value_counts())\n",
        "category_values = list(bbc_df.category.value_counts().keys()) \n",
        "print (\"The count of each category is: \\nsport:{0}, business:{1}, politics:{2}, tech:{3}, entertainment:{4}\".format(category_counts[0], category_counts[1], category_counts[2], category_counts[3], category_counts[4]))\n",
        "plt.bar(category_values, category_counts)\n",
        "plt.xlabel('\\n Categories')\n",
        "plt.ylabel('Counts')\n",
        "plt.show()\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The count of each category is: \n",
            "sport:511, business:510, politics:417, tech:401, entertainment:386\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEfFsAVnMJwp",
        "colab_type": "text"
      },
      "source": [
        "As seen in the bar chart above, the sport and business categories have the highest frequency in the dataset. There are a total of 511 sport-related texts and 510 business-related texts. For each of the other categories: poltics has 417 texts, tech has 401 texts and entertainment has 386 texts. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92cWY3ZUGuZ2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "3286cce2-e21d-4d85-ae24-8c6e5a2dd89d"
      },
      "source": [
        "# Descriptions of dataset\n",
        "\n",
        "text_counts = list()\n",
        "text_split = list()\n",
        "for text in bbc_df.text:\n",
        "  text_split.append(text.split(\" \"))\n",
        "  text_counts.append(len(text.split(\" \")))\n",
        "bbc_df[\"Words\"] = text_split\n",
        "bbc_df[\"Words_per_text\"] = text_counts\n",
        "\n",
        "print (\"Dataset shape: {0}\".format(bbc_df.shape))\n",
        "print()\n",
        "print (\"Dataset descriptions are: \\n{0}\".format(bbc_df[\"Words_per_text\"].describe()))\n",
        "print()\n",
        "print (\"Dataframe head is: \\n{0}\".format(bbc_df.head()))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset shape: (2225, 4)\n",
            "\n",
            "Dataset descriptions are: \n",
            "count    2225.000000\n",
            "mean      419.757303\n",
            "std       260.055935\n",
            "min        94.000000\n",
            "25%       268.000000\n",
            "50%       361.000000\n",
            "75%       514.000000\n",
            "max      4759.000000\n",
            "Name: Words_per_text, dtype: float64\n",
            "\n",
            "Dataframe head is: \n",
            "        category  ... Words_per_text\n",
            "0           tech  ...            806\n",
            "1       business  ...            332\n",
            "2          sport  ...            270\n",
            "3          sport  ...            390\n",
            "4  entertainment  ...            287\n",
            "\n",
            "[5 rows x 4 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwKSO1b0PVCh",
        "colab_type": "text"
      },
      "source": [
        "The shape of the dataset, after inserting the words per text column is (2225, 4) meaning there are 2225 observations and 4 features. The new dataframe with the individual words column and words per text column is printed. \n",
        "\n",
        "On average, there are about 420 words per text. The text with the lowest word count has 94 words. The text with the most words has 4759 words. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sdYjMCmQMsh",
        "colab_type": "text"
      },
      "source": [
        "### 2) Preprocess your data such that each document in the data is represented as a sequence of equal length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phKJXB6SMHDT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "674855a4-6734-413b-eeff-b2e9d47e59ed"
      },
      "source": [
        "maxlen = 94 # since the min number of words are 94, we'll cut the number words in a text at 94\n",
        "training_samples = 1000 # will training on 1000 docs, about half of total\n",
        "validation_samples = 100000 # will validate on 1000 samples, about half of total\n",
        "max_words = 100000 # will only consider 100,000 words, about 10% of total words\n",
        "\n",
        "tokenizer = Tokenizer(num_words = max_words)\n",
        "tokenizer.fit_on_texts(bbc_df.text)\n",
        "sequences = tokenizer.texts_to_sequences(bbc_df.text)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print (\"{0} of unique tokens\".format(len(word_index)))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "labels = preprocessing.LabelEncoder()\n",
        "labels = labels.fit_transform(bbc_df.category)\n",
        "labels = np.asarray(labels)\n",
        "\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "29726 of unique tokens\n",
            "Shape of data tensor: (2225, 94)\n",
            "Shape of label tensor: (2225,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmmLxHFptYTI",
        "colab_type": "text"
      },
      "source": [
        "### 3)  Use the data to fit separate models to each of the following architectures:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XOnclQbtdnC",
        "colab_type": "text"
      },
      "source": [
        "### 3.a. A model with an embedding layer and dense layers (but w/ no layers meant for sequential data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWDrDiAcuesD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Shuffling the data\n",
        "\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "# Splitting data into training and testing\n",
        "\n",
        "x_train = data[:training_samples] \n",
        "y_train = labels[:training_samples]\n",
        "x_val = data[training_samples: training_samples + validation_samples]\n",
        "y_val = labels[training_samples: training_samples + validation_samples]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkQcIXzDaWy8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 674
        },
        "outputId": "81bcceca-8ca8-46d6-b1b2-8e4a3bf4053a"
      },
      "source": [
        "# Model with embedding layer and dense layer\n",
        "\n",
        "model1 = Sequential()\n",
        "\n",
        "model1.add(Embedding(max_words, 8, input_length=maxlen))\n",
        "model1.add(Flatten())\n",
        "model1.add(Dense(1, activation ='sigmoid'))\n",
        "model1.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "model1.summary()\n",
        "\n",
        "history1 = model1.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=.2)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 94, 8)             800000    \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 752)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 753       \n",
            "=================================================================\n",
            "Total params: 800,753\n",
            "Trainable params: 800,753\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 800 samples, validate on 200 samples\n",
            "Epoch 1/10\n",
            "800/800 [==============================] - 0s 609us/step - loss: 0.5838 - acc: 0.1762 - val_loss: 0.3794 - val_acc: 0.1500\n",
            "Epoch 2/10\n",
            "800/800 [==============================] - 0s 408us/step - loss: -0.0149 - acc: 0.1762 - val_loss: -0.5160 - val_acc: 0.1500\n",
            "Epoch 3/10\n",
            "800/800 [==============================] - 0s 408us/step - loss: -1.1265 - acc: 0.1762 - val_loss: -1.9225 - val_acc: 0.1500\n",
            "Epoch 4/10\n",
            "800/800 [==============================] - 0s 440us/step - loss: -2.6775 - acc: 0.1762 - val_loss: -3.7697 - val_acc: 0.1500\n",
            "Epoch 5/10\n",
            "800/800 [==============================] - 0s 450us/step - loss: -4.6451 - acc: 0.1762 - val_loss: -6.1304 - val_acc: 0.1500\n",
            "Epoch 6/10\n",
            "800/800 [==============================] - 0s 435us/step - loss: -7.0715 - acc: 0.1762 - val_loss: -8.9467 - val_acc: 0.1500\n",
            "Epoch 7/10\n",
            "800/800 [==============================] - 0s 412us/step - loss: -10.0472 - acc: 0.1762 - val_loss: -12.4199 - val_acc: 0.1500\n",
            "Epoch 8/10\n",
            "800/800 [==============================] - 0s 441us/step - loss: -13.5975 - acc: 0.1762 - val_loss: -16.4744 - val_acc: 0.1500\n",
            "Epoch 9/10\n",
            "800/800 [==============================] - 0s 434us/step - loss: -17.6894 - acc: 0.1762 - val_loss: -21.0084 - val_acc: 0.1500\n",
            "Epoch 10/10\n",
            "800/800 [==============================] - 0s 396us/step - loss: -22.2166 - acc: 0.1762 - val_loss: -26.0365 - val_acc: 0.1500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_PUnCOtwNpx",
        "colab_type": "text"
      },
      "source": [
        "### 3.b. A model using an Embedding layer with Conv1d Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0keiHfVy7vG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        },
        "outputId": "e2fbe0fd-0c6a-4c38-e81b-51f8f0d6052c"
      },
      "source": [
        "# Model with 1D Convnet\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(layers.Embedding(max_words, 128, input_length=maxlen))\n",
        "model2.add(layers.Conv1D(32, 7, activation='relu'))\n",
        "model2.add(layers.MaxPooling1D(5))\n",
        "model2.add(layers.Conv1D(32, 7, activation ='relu'))\n",
        "model2.add(layers.GlobalMaxPooling1D())\n",
        "model2.add(layers.Dense(1))\n",
        "\n",
        "model2.summary()\n",
        "\n",
        "model2.compile(optimizer=RMSprop(lr=1e-4), loss='binary_crossentropy', metrics=['acc'])\n",
        "history2 = model2.fit(x_train, y_train, epochs=10, batch_size=128, validation_split=.2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 94, 128)           12800000  \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 88, 32)            28704     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 17, 32)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 11, 32)            7200      \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_1 (Glob (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 12,835,937\n",
            "Trainable params: 12,835,937\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 800 samples, validate on 200 samples\n",
            "Epoch 1/10\n",
            "800/800 [==============================] - 2s 2ms/step - loss: 2.7962 - acc: 0.2350 - val_loss: 2.3918 - val_acc: 0.2350\n",
            "Epoch 2/10\n",
            "800/800 [==============================] - 1s 2ms/step - loss: 1.9942 - acc: 0.2350 - val_loss: 1.9179 - val_acc: 0.2350\n",
            "Epoch 3/10\n",
            "800/800 [==============================] - 1s 2ms/step - loss: 1.5196 - acc: 0.2350 - val_loss: 1.4881 - val_acc: 0.2350\n",
            "Epoch 4/10\n",
            "800/800 [==============================] - 2s 2ms/step - loss: 1.0966 - acc: 0.2350 - val_loss: 1.1194 - val_acc: 0.2350\n",
            "Epoch 5/10\n",
            "800/800 [==============================] - 2s 2ms/step - loss: 0.7209 - acc: 0.2688 - val_loss: 0.7560 - val_acc: 0.2000\n",
            "Epoch 6/10\n",
            "800/800 [==============================] - 2s 2ms/step - loss: 0.3418 - acc: 0.3237 - val_loss: 0.3872 - val_acc: 0.1600\n",
            "Epoch 7/10\n",
            "800/800 [==============================] - 2s 2ms/step - loss: -0.0603 - acc: 0.1988 - val_loss: -0.0145 - val_acc: 0.1500\n",
            "Epoch 8/10\n",
            "800/800 [==============================] - 2s 2ms/step - loss: -0.5313 - acc: 0.1762 - val_loss: -0.5016 - val_acc: 0.1500\n",
            "Epoch 9/10\n",
            "800/800 [==============================] - 2s 2ms/step - loss: -1.1876 - acc: 0.1762 - val_loss: -1.1828 - val_acc: 0.1500\n",
            "Epoch 10/10\n",
            "800/800 [==============================] - 2s 2ms/step - loss: -4.4700 - acc: 0.1762 - val_loss: -5.2833 - val_acc: 0.1500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LrrfOOd4Lxn",
        "colab_type": "text"
      },
      "source": [
        "### 3.c. A model using an Embedding layer with one sequential layer (LSTM or GRU)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgNXcV4X4QIy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "cd0bb4f2-4d9a-467a-ddd7-7cfaa51e683c"
      },
      "source": [
        "# Model with an LSTM layer\n",
        "\n",
        "model3 = Sequential()\n",
        "model3.add(Embedding(max_words, 32))\n",
        "model3.add(LSTM(32))\n",
        "model3.add(Dense(1, activation ='sigmoid'))\n",
        "\n",
        "model3.compile(optimizer=RMSprop(lr=1e-4), loss='binary_crossentropy', metrics = ['acc'])\n",
        "history = model3.fit(x_train, y_train, epochs = 10, batch_size=32, validation_split =.2)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 800 samples, validate on 200 samples\n",
            "Epoch 1/10\n",
            "800/800 [==============================] - 3s 4ms/step - loss: 0.6663 - acc: 0.1975 - val_loss: 0.6434 - val_acc: 0.1500\n",
            "Epoch 2/10\n",
            "800/800 [==============================] - 3s 4ms/step - loss: 0.6261 - acc: 0.1762 - val_loss: 0.5958 - val_acc: 0.1500\n",
            "Epoch 3/10\n",
            "800/800 [==============================] - 3s 4ms/step - loss: 0.5733 - acc: 0.1762 - val_loss: 0.5290 - val_acc: 0.1500\n",
            "Epoch 4/10\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.4962 - acc: 0.1762 - val_loss: 0.4243 - val_acc: 0.1500\n",
            "Epoch 5/10\n",
            "800/800 [==============================] - 2s 3ms/step - loss: 0.3618 - acc: 0.1762 - val_loss: 0.2169 - val_acc: 0.1500\n",
            "Epoch 6/10\n",
            "800/800 [==============================] - 2s 3ms/step - loss: -0.0023 - acc: 0.1762 - val_loss: -0.4987 - val_acc: 0.1500\n",
            "Epoch 7/10\n",
            "800/800 [==============================] - 2s 3ms/step - loss: -0.9433 - acc: 0.1762 - val_loss: -1.6098 - val_acc: 0.1500\n",
            "Epoch 8/10\n",
            "800/800 [==============================] - 2s 3ms/step - loss: -1.7609 - acc: 0.1762 - val_loss: -2.2707 - val_acc: 0.1500\n",
            "Epoch 9/10\n",
            "800/800 [==============================] - 2s 3ms/step - loss: -2.2671 - acc: 0.1762 - val_loss: -2.7355 - val_acc: 0.1500\n",
            "Epoch 10/10\n",
            "800/800 [==============================] - 2s 3ms/step - loss: -2.6430 - acc: 0.1762 - val_loss: -3.1008 - val_acc: 0.1500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_JrVQir8UWp",
        "colab_type": "text"
      },
      "source": [
        "### 3.d. A model using an Embedding layer with stacked sequential layers (LSTM or GRU)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7Cm-uvB8asV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "9252c3db-b015-47af-e625-403b5abb7e67"
      },
      "source": [
        "model4 = Sequential()\n",
        "model4.add(Embedding(max_words, 32))\n",
        "model4.add(LSTM(32, return_sequences=True))\n",
        "model4.add(LSTM(32, return_sequences=False))\n",
        "model4.add(Dense(1, activation ='sigmoid'))\n",
        "\n",
        "\n",
        "model4.compile(optimizer=RMSprop(lr=1e-4), loss='binary_crossentropy', metrics = ['acc'])\n",
        "history = model4.fit(x_train, y_train, epochs = 10, batch_size=32, validation_split =.2)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 800 samples, validate on 200 samples\n",
            "Epoch 1/10\n",
            "800/800 [==============================] - 4s 5ms/step - loss: 0.6405 - acc: 0.1800 - val_loss: 0.5760 - val_acc: 0.1500\n",
            "Epoch 2/10\n",
            "800/800 [==============================] - 3s 4ms/step - loss: 0.5018 - acc: 0.1762 - val_loss: 0.3753 - val_acc: 0.1500\n",
            "Epoch 3/10\n",
            "800/800 [==============================] - 3s 4ms/step - loss: 0.2130 - acc: 0.1762 - val_loss: -0.1021 - val_acc: 0.1500\n",
            "Epoch 4/10\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -0.6620 - acc: 0.1762 - val_loss: -1.6592 - val_acc: 0.1500\n",
            "Epoch 5/10\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -2.3180 - acc: 0.1762 - val_loss: -3.2546 - val_acc: 0.1500\n",
            "Epoch 6/10\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -3.2090 - acc: 0.1762 - val_loss: -3.7847 - val_acc: 0.1500\n",
            "Epoch 7/10\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -3.5974 - acc: 0.1762 - val_loss: -4.1661 - val_acc: 0.1500\n",
            "Epoch 8/10\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -3.9232 - acc: 0.1762 - val_loss: -4.5092 - val_acc: 0.1500\n",
            "Epoch 9/10\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -4.2232 - acc: 0.1762 - val_loss: -4.8221 - val_acc: 0.1500\n",
            "Epoch 10/10\n",
            "800/800 [==============================] - 4s 5ms/step - loss: -4.4993 - acc: 0.1762 - val_loss: -5.1200 - val_acc: 0.1500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRWx_SkQ7V-c",
        "colab_type": "text"
      },
      "source": [
        "### 3.e. A model using an Embedding layer with bidirectional sequential layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lE4O0h_B7Pyp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "82b1610b-6184-4ef6-ece7-809e4f184d29"
      },
      "source": [
        "# Model with a bidirectional layer\n",
        "\n",
        "model5 = Sequential()\n",
        "model5.add(layers.Embedding(max_words, 32))\n",
        "model5.add(layers.Bidirectional(layers.LSTM(32)))\n",
        "model5.add(layers.Dense(1, activation ='sigmoid'))\n",
        "\n",
        "model5.compile(optimizer=RMSprop(lr=1e-4), loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model5.fit(x_train, y_train, epochs=10, batch_size=128, validation_split=.2)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 800 samples, validate on 200 samples\n",
            "Epoch 1/10\n",
            "800/800 [==============================] - 2s 3ms/step - loss: 0.6799 - acc: 0.2200 - val_loss: 0.6581 - val_acc: 0.1550\n",
            "Epoch 2/10\n",
            "800/800 [==============================] - 1s 2ms/step - loss: 0.6526 - acc: 0.1775 - val_loss: 0.6349 - val_acc: 0.1500\n",
            "Epoch 3/10\n",
            "800/800 [==============================] - 1s 2ms/step - loss: 0.6302 - acc: 0.1762 - val_loss: 0.6114 - val_acc: 0.1500\n",
            "Epoch 4/10\n",
            "800/800 [==============================] - 1s 2ms/step - loss: 0.6068 - acc: 0.1762 - val_loss: 0.5855 - val_acc: 0.1500\n",
            "Epoch 5/10\n",
            "800/800 [==============================] - 1s 2ms/step - loss: 0.5811 - acc: 0.1762 - val_loss: 0.5581 - val_acc: 0.1500\n",
            "Epoch 6/10\n",
            "800/800 [==============================] - 1s 2ms/step - loss: 0.5537 - acc: 0.1762 - val_loss: 0.5286 - val_acc: 0.1500\n",
            "Epoch 7/10\n",
            "800/800 [==============================] - 1s 2ms/step - loss: 0.5237 - acc: 0.1762 - val_loss: 0.4946 - val_acc: 0.1500\n",
            "Epoch 8/10\n",
            "800/800 [==============================] - 1s 2ms/step - loss: 0.4896 - acc: 0.1762 - val_loss: 0.4555 - val_acc: 0.1500\n",
            "Epoch 9/10\n",
            "800/800 [==============================] - 1s 2ms/step - loss: 0.4504 - acc: 0.1762 - val_loss: 0.4116 - val_acc: 0.1500\n",
            "Epoch 10/10\n",
            "800/800 [==============================] - 1s 2ms/step - loss: 0.4064 - acc: 0.1762 - val_loss: 0.3639 - val_acc: 0.1500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKp8gCEr8Krg",
        "colab_type": "text"
      },
      "source": [
        "### 3.f. Now retrain your best model from C, D, and E using dropout (you may need to increase epochs!)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo0MXs188O2o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9a4608e7-903e-471f-c8a0-c42c611357a3"
      },
      "source": [
        "# Model 3 retrained\n",
        "\n",
        "model6 = Sequential()\n",
        "model6.add(Embedding(max_words, 32))\n",
        "model6.add(LSTM(32, dropout=.2))\n",
        "model6.add(Dense(1, activation ='sigmoid'))\n",
        "\n",
        "model6.compile(optimizer=RMSprop(lr=1e-4), loss='binary_crossentropy', metrics = ['acc'])\n",
        "history = model6.fit(x_train, y_train, epochs = 50, batch_size=32, validation_split =.2)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 800 samples, validate on 200 samples\n",
            "Epoch 1/50\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.6632 - acc: 0.1762 - val_loss: 0.6327 - val_acc: 0.1500\n",
            "Epoch 2/50\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.6094 - acc: 0.1750 - val_loss: 0.5718 - val_acc: 0.1500\n",
            "Epoch 3/50\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.5455 - acc: 0.1762 - val_loss: 0.4919 - val_acc: 0.1500\n",
            "Epoch 4/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: 0.4519 - acc: 0.1762 - val_loss: 0.3657 - val_acc: 0.1500\n",
            "Epoch 5/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: 0.2969 - acc: 0.1762 - val_loss: 0.1479 - val_acc: 0.1500\n",
            "Epoch 6/50\n",
            "800/800 [==============================] - 3s 3ms/step - loss: -0.0110 - acc: 0.1762 - val_loss: -0.3829 - val_acc: 0.1500\n",
            "Epoch 7/50\n",
            "800/800 [==============================] - 3s 3ms/step - loss: -0.8860 - acc: 0.1762 - val_loss: -1.7383 - val_acc: 0.1500\n",
            "Epoch 8/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -1.9890 - acc: 0.1762 - val_loss: -2.6188 - val_acc: 0.1500\n",
            "Epoch 9/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -2.6477 - acc: 0.1762 - val_loss: -3.2230 - val_acc: 0.1500\n",
            "Epoch 10/50\n",
            "800/800 [==============================] - 3s 3ms/step - loss: -3.1287 - acc: 0.1762 - val_loss: -3.6807 - val_acc: 0.1500\n",
            "Epoch 11/50\n",
            "800/800 [==============================] - 3s 3ms/step - loss: -3.5014 - acc: 0.1762 - val_loss: -4.0509 - val_acc: 0.1500\n",
            "Epoch 12/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -3.8110 - acc: 0.1762 - val_loss: -4.3573 - val_acc: 0.1500\n",
            "Epoch 13/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -4.0645 - acc: 0.1762 - val_loss: -4.6213 - val_acc: 0.1500\n",
            "Epoch 14/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -4.2926 - acc: 0.1762 - val_loss: -4.8598 - val_acc: 0.1500\n",
            "Epoch 15/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -4.5080 - acc: 0.1762 - val_loss: -5.0885 - val_acc: 0.1500\n",
            "Epoch 16/50\n",
            "800/800 [==============================] - 3s 3ms/step - loss: -4.7043 - acc: 0.1762 - val_loss: -5.2984 - val_acc: 0.1500\n",
            "Epoch 17/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -4.8963 - acc: 0.1762 - val_loss: -5.5093 - val_acc: 0.1500\n",
            "Epoch 18/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -5.0794 - acc: 0.1762 - val_loss: -5.7074 - val_acc: 0.1500\n",
            "Epoch 19/50\n",
            "800/800 [==============================] - 4s 4ms/step - loss: -5.2628 - acc: 0.1762 - val_loss: -5.9105 - val_acc: 0.1500\n",
            "Epoch 20/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -5.4509 - acc: 0.1762 - val_loss: -6.1219 - val_acc: 0.1500\n",
            "Epoch 21/50\n",
            "800/800 [==============================] - 3s 3ms/step - loss: -5.6465 - acc: 0.1762 - val_loss: -6.3391 - val_acc: 0.1500\n",
            "Epoch 22/50\n",
            "800/800 [==============================] - 3s 3ms/step - loss: -5.8375 - acc: 0.1762 - val_loss: -6.5415 - val_acc: 0.1500\n",
            "Epoch 23/50\n",
            "800/800 [==============================] - 3s 3ms/step - loss: -6.0234 - acc: 0.1762 - val_loss: -6.7461 - val_acc: 0.1500\n",
            "Epoch 24/50\n",
            "800/800 [==============================] - 3s 3ms/step - loss: -6.2023 - acc: 0.1762 - val_loss: -6.9394 - val_acc: 0.1500\n",
            "Epoch 25/50\n",
            "800/800 [==============================] - 2s 3ms/step - loss: -6.3721 - acc: 0.1762 - val_loss: -7.1197 - val_acc: 0.1500\n",
            "Epoch 26/50\n",
            "800/800 [==============================] - 3s 3ms/step - loss: -6.5385 - acc: 0.1762 - val_loss: -7.3032 - val_acc: 0.1500\n",
            "Epoch 27/50\n",
            "800/800 [==============================] - 3s 3ms/step - loss: -6.7016 - acc: 0.1762 - val_loss: -7.4792 - val_acc: 0.1500\n",
            "Epoch 28/50\n",
            "800/800 [==============================] - 3s 3ms/step - loss: -6.8573 - acc: 0.1762 - val_loss: -7.6476 - val_acc: 0.1500\n",
            "Epoch 29/50\n",
            "800/800 [==============================] - 3s 3ms/step - loss: -7.0056 - acc: 0.1762 - val_loss: -7.8062 - val_acc: 0.1500\n",
            "Epoch 30/50\n",
            "800/800 [==============================] - 2s 3ms/step - loss: -7.1503 - acc: 0.1762 - val_loss: -7.9672 - val_acc: 0.1500\n",
            "Epoch 31/50\n",
            "800/800 [==============================] - 2s 3ms/step - loss: -7.2952 - acc: 0.1762 - val_loss: -8.1249 - val_acc: 0.1500\n",
            "Epoch 32/50\n",
            "800/800 [==============================] - 2s 3ms/step - loss: -7.4331 - acc: 0.1762 - val_loss: -8.2736 - val_acc: 0.1500\n",
            "Epoch 33/50\n",
            "800/800 [==============================] - 2s 3ms/step - loss: -7.5713 - acc: 0.1762 - val_loss: -8.4234 - val_acc: 0.1500\n",
            "Epoch 34/50\n",
            "800/800 [==============================] - 2s 3ms/step - loss: -7.7032 - acc: 0.1762 - val_loss: -8.5666 - val_acc: 0.1500\n",
            "Epoch 35/50\n",
            "800/800 [==============================] - 2s 3ms/step - loss: -7.8296 - acc: 0.1762 - val_loss: -8.7045 - val_acc: 0.1500\n",
            "Epoch 36/50\n",
            "800/800 [==============================] - 2s 3ms/step - loss: -7.9542 - acc: 0.1762 - val_loss: -8.8413 - val_acc: 0.1500\n",
            "Epoch 37/50\n",
            "800/800 [==============================] - 2s 3ms/step - loss: -8.0744 - acc: 0.1762 - val_loss: -8.9715 - val_acc: 0.1500\n",
            "Epoch 38/50\n",
            "800/800 [==============================] - 2s 3ms/step - loss: -8.1907 - acc: 0.1762 - val_loss: -9.0978 - val_acc: 0.1500\n",
            "Epoch 39/50\n",
            "800/800 [==============================] - 2s 3ms/step - loss: -8.3065 - acc: 0.1762 - val_loss: -9.2244 - val_acc: 0.1500\n",
            "Epoch 40/50\n",
            "800/800 [==============================] - 3s 3ms/step - loss: -8.4216 - acc: 0.1762 - val_loss: -9.3494 - val_acc: 0.1500\n",
            "Epoch 41/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -8.5295 - acc: 0.1762 - val_loss: -9.4673 - val_acc: 0.1500\n",
            "Epoch 42/50\n",
            "800/800 [==============================] - 3s 3ms/step - loss: -8.6344 - acc: 0.1762 - val_loss: -9.5814 - val_acc: 0.1500\n",
            "Epoch 43/50\n",
            "800/800 [==============================] - 2s 3ms/step - loss: -8.7405 - acc: 0.1762 - val_loss: -9.6984 - val_acc: 0.1500\n",
            "Epoch 44/50\n",
            "800/800 [==============================] - 2s 3ms/step - loss: -8.8422 - acc: 0.1762 - val_loss: -9.8094 - val_acc: 0.1500\n",
            "Epoch 45/50\n",
            "800/800 [==============================] - 2s 3ms/step - loss: -8.9449 - acc: 0.1762 - val_loss: -9.9229 - val_acc: 0.1500\n",
            "Epoch 46/50\n",
            "800/800 [==============================] - 2s 3ms/step - loss: -9.0402 - acc: 0.1762 - val_loss: -10.0256 - val_acc: 0.1500\n",
            "Epoch 47/50\n",
            "800/800 [==============================] - 2s 3ms/step - loss: -9.1355 - acc: 0.1762 - val_loss: -10.1311 - val_acc: 0.1500\n",
            "Epoch 48/50\n",
            "800/800 [==============================] - 2s 3ms/step - loss: -9.2300 - acc: 0.1762 - val_loss: -10.2334 - val_acc: 0.1500\n",
            "Epoch 49/50\n",
            "800/800 [==============================] - 2s 3ms/step - loss: -9.3230 - acc: 0.1762 - val_loss: -10.3358 - val_acc: 0.1500\n",
            "Epoch 50/50\n",
            "800/800 [==============================] - 2s 3ms/step - loss: -9.4150 - acc: 0.1762 - val_loss: -10.4383 - val_acc: 0.1500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vOrYH3sULVZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "957b4f88-1419-4fe6-c6dc-1c89cca5b06b"
      },
      "source": [
        "# Model 4 Retrained\n",
        "\n",
        "model7 = Sequential()\n",
        "model7.add(Embedding(max_words, 32))\n",
        "model7.add(LSTM(32, return_sequences=True, dropout=.2))\n",
        "model7.add(LSTM(32, return_sequences=False, dropout=.2))\n",
        "model7.add(Dense(1, activation ='sigmoid'))\n",
        "\n",
        "\n",
        "model7.compile(optimizer=RMSprop(lr=1e-4), loss='binary_crossentropy', metrics = ['acc'])\n",
        "history = model7.fit(x_train, y_train, epochs = 50, batch_size=32, validation_split =.2)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 800 samples, validate on 200 samples\n",
            "Epoch 1/50\n",
            "800/800 [==============================] - 5s 6ms/step - loss: 0.6470 - acc: 0.1750 - val_loss: 0.5886 - val_acc: 0.1500\n",
            "Epoch 2/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: 0.5294 - acc: 0.1762 - val_loss: 0.4246 - val_acc: 0.1500\n",
            "Epoch 3/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: 0.3096 - acc: 0.1762 - val_loss: 0.0765 - val_acc: 0.1500\n",
            "Epoch 4/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -0.2308 - acc: 0.1762 - val_loss: -0.8917 - val_acc: 0.1500\n",
            "Epoch 5/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -1.6302 - acc: 0.1762 - val_loss: -2.7769 - val_acc: 0.1500\n",
            "Epoch 6/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -3.0587 - acc: 0.1762 - val_loss: -3.9626 - val_acc: 0.1500\n",
            "Epoch 7/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -3.7890 - acc: 0.1762 - val_loss: -4.4620 - val_acc: 0.1500\n",
            "Epoch 8/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -4.1933 - acc: 0.1762 - val_loss: -4.8470 - val_acc: 0.1500\n",
            "Epoch 9/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -4.5237 - acc: 0.1762 - val_loss: -5.2080 - val_acc: 0.1500\n",
            "Epoch 10/50\n",
            "800/800 [==============================] - 5s 6ms/step - loss: -4.8512 - acc: 0.1762 - val_loss: -5.5653 - val_acc: 0.1500\n",
            "Epoch 11/50\n",
            "800/800 [==============================] - 5s 6ms/step - loss: -5.1692 - acc: 0.1762 - val_loss: -5.9017 - val_acc: 0.1500\n",
            "Epoch 12/50\n",
            "800/800 [==============================] - 5s 6ms/step - loss: -5.4759 - acc: 0.1762 - val_loss: -6.2343 - val_acc: 0.1500\n",
            "Epoch 13/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -5.7518 - acc: 0.1762 - val_loss: -6.5392 - val_acc: 0.1500\n",
            "Epoch 14/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -6.0293 - acc: 0.1762 - val_loss: -6.8399 - val_acc: 0.1500\n",
            "Epoch 15/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -6.2785 - acc: 0.1762 - val_loss: -7.1193 - val_acc: 0.1500\n",
            "Epoch 16/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -6.5303 - acc: 0.1762 - val_loss: -7.3573 - val_acc: 0.1500\n",
            "Epoch 17/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -6.7493 - acc: 0.1762 - val_loss: -7.5928 - val_acc: 0.1500\n",
            "Epoch 18/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -6.9514 - acc: 0.1762 - val_loss: -7.8150 - val_acc: 0.1500\n",
            "Epoch 19/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -7.1415 - acc: 0.1762 - val_loss: -8.0144 - val_acc: 0.1500\n",
            "Epoch 20/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -7.3265 - acc: 0.1762 - val_loss: -8.2099 - val_acc: 0.1500\n",
            "Epoch 21/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -7.5037 - acc: 0.1762 - val_loss: -8.3954 - val_acc: 0.1500\n",
            "Epoch 22/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -7.6585 - acc: 0.1762 - val_loss: -8.5684 - val_acc: 0.1500\n",
            "Epoch 23/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -7.8140 - acc: 0.1762 - val_loss: -8.7323 - val_acc: 0.1500\n",
            "Epoch 24/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -7.9614 - acc: 0.1762 - val_loss: -8.8801 - val_acc: 0.1500\n",
            "Epoch 25/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -8.0900 - acc: 0.1762 - val_loss: -9.0228 - val_acc: 0.1500\n",
            "Epoch 26/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -8.2227 - acc: 0.1762 - val_loss: -9.1579 - val_acc: 0.1500\n",
            "Epoch 27/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -8.3400 - acc: 0.1762 - val_loss: -9.2892 - val_acc: 0.1500\n",
            "Epoch 28/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -8.4541 - acc: 0.1762 - val_loss: -9.4091 - val_acc: 0.1500\n",
            "Epoch 29/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -8.5584 - acc: 0.1762 - val_loss: -9.5265 - val_acc: 0.1500\n",
            "Epoch 30/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -8.6725 - acc: 0.1762 - val_loss: -9.6368 - val_acc: 0.1500\n",
            "Epoch 31/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -8.7648 - acc: 0.1762 - val_loss: -9.7440 - val_acc: 0.1500\n",
            "Epoch 32/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -8.8652 - acc: 0.1762 - val_loss: -9.8493 - val_acc: 0.1500\n",
            "Epoch 33/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -8.9570 - acc: 0.1762 - val_loss: -9.9466 - val_acc: 0.1500\n",
            "Epoch 34/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -9.0444 - acc: 0.1762 - val_loss: -10.0431 - val_acc: 0.1500\n",
            "Epoch 35/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -9.1317 - acc: 0.1762 - val_loss: -10.1370 - val_acc: 0.1500\n",
            "Epoch 36/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -9.2188 - acc: 0.1762 - val_loss: -10.2310 - val_acc: 0.1500\n",
            "Epoch 37/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -9.3044 - acc: 0.1762 - val_loss: -10.3221 - val_acc: 0.1500\n",
            "Epoch 38/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -9.3859 - acc: 0.1762 - val_loss: -10.4109 - val_acc: 0.1500\n",
            "Epoch 39/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -9.4669 - acc: 0.1762 - val_loss: -10.5000 - val_acc: 0.1500\n",
            "Epoch 40/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -9.5442 - acc: 0.1762 - val_loss: -10.5847 - val_acc: 0.1500\n",
            "Epoch 41/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -9.6200 - acc: 0.1762 - val_loss: -10.6685 - val_acc: 0.1500\n",
            "Epoch 42/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -9.7015 - acc: 0.1762 - val_loss: -10.7587 - val_acc: 0.1500\n",
            "Epoch 43/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -9.7787 - acc: 0.1762 - val_loss: -10.8423 - val_acc: 0.1500\n",
            "Epoch 44/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -9.8547 - acc: 0.1762 - val_loss: -10.9238 - val_acc: 0.1500\n",
            "Epoch 45/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -9.9287 - acc: 0.1762 - val_loss: -11.0060 - val_acc: 0.1500\n",
            "Epoch 46/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -10.0032 - acc: 0.1762 - val_loss: -11.0887 - val_acc: 0.1500\n",
            "Epoch 47/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -10.0787 - acc: 0.1762 - val_loss: -11.1708 - val_acc: 0.1500\n",
            "Epoch 48/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -10.1530 - acc: 0.1762 - val_loss: -11.2535 - val_acc: 0.1500\n",
            "Epoch 49/50\n",
            "800/800 [==============================] - 3s 4ms/step - loss: -10.2271 - acc: 0.1762 - val_loss: -11.3336 - val_acc: 0.1500\n",
            "Epoch 50/50\n",
            "800/800 [==============================] - 4s 5ms/step - loss: -10.3007 - acc: 0.1762 - val_loss: -11.4161 - val_acc: 0.1500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1b8c7407-b7e3-4a3b-806a-214629b8a478",
        "id": "91eugZUiespQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Model 5 retrained\n",
        "\n",
        "model5 = Sequential()\n",
        "model5.add(layers.Embedding(max_words, 32))\n",
        "model5.add(layers.Bidirectional(layers.LSTM(32, dropout=.2)))\n",
        "model5.add(layers.Dense(1, activation ='sigmoid'))\n",
        "\n",
        "model5.compile(optimizer=RMSprop(lr=1e-4), loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model5.fit(x_train, y_train, epochs=50, batch_size=128, validation_split=.2)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 800 samples, validate on 200 samples\n",
            "Epoch 1/50\n",
            "800/800 [==============================] - 2s 3ms/step - loss: 0.6877 - acc: 0.2138 - val_loss: 0.6768 - val_acc: 0.1400\n",
            "Epoch 2/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6679 - acc: 0.1825 - val_loss: 0.6604 - val_acc: 0.1550\n",
            "Epoch 3/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6518 - acc: 0.1762 - val_loss: 0.6445 - val_acc: 0.1500\n",
            "Epoch 4/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6357 - acc: 0.1762 - val_loss: 0.6279 - val_acc: 0.1500\n",
            "Epoch 5/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6196 - acc: 0.1762 - val_loss: 0.6103 - val_acc: 0.1500\n",
            "Epoch 6/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6016 - acc: 0.1762 - val_loss: 0.5913 - val_acc: 0.1500\n",
            "Epoch 7/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.5824 - acc: 0.1762 - val_loss: 0.5709 - val_acc: 0.1500\n",
            "Epoch 8/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.5616 - acc: 0.1762 - val_loss: 0.5491 - val_acc: 0.1500\n",
            "Epoch 9/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.5393 - acc: 0.1762 - val_loss: 0.5254 - val_acc: 0.1500\n",
            "Epoch 10/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.5153 - acc: 0.1762 - val_loss: 0.4988 - val_acc: 0.1500\n",
            "Epoch 11/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4879 - acc: 0.1762 - val_loss: 0.4702 - val_acc: 0.1500\n",
            "Epoch 12/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4597 - acc: 0.1762 - val_loss: 0.4393 - val_acc: 0.1500\n",
            "Epoch 13/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4277 - acc: 0.1762 - val_loss: 0.4025 - val_acc: 0.1500\n",
            "Epoch 14/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3897 - acc: 0.1762 - val_loss: 0.3614 - val_acc: 0.1500\n",
            "Epoch 15/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3486 - acc: 0.1762 - val_loss: 0.3156 - val_acc: 0.1500\n",
            "Epoch 16/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3014 - acc: 0.1762 - val_loss: 0.2634 - val_acc: 0.1500\n",
            "Epoch 17/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.2455 - acc: 0.1762 - val_loss: 0.2019 - val_acc: 0.1500\n",
            "Epoch 18/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.1798 - acc: 0.1762 - val_loss: 0.1266 - val_acc: 0.1500\n",
            "Epoch 19/50\n",
            "800/800 [==============================] - 1s 2ms/step - loss: 0.1051 - acc: 0.1762 - val_loss: 0.0388 - val_acc: 0.1500\n",
            "Epoch 20/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.0123 - acc: 0.1762 - val_loss: -0.0733 - val_acc: 0.1500\n",
            "Epoch 21/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: -0.1104 - acc: 0.1762 - val_loss: -0.2284 - val_acc: 0.1500\n",
            "Epoch 22/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: -0.2756 - acc: 0.1762 - val_loss: -0.4378 - val_acc: 0.1500\n",
            "Epoch 23/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: -0.5080 - acc: 0.1762 - val_loss: -0.7528 - val_acc: 0.1500\n",
            "Epoch 24/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: -0.8363 - acc: 0.1762 - val_loss: -1.1471 - val_acc: 0.1500\n",
            "Epoch 25/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: -1.2212 - acc: 0.1762 - val_loss: -1.6058 - val_acc: 0.1500\n",
            "Epoch 26/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: -1.6452 - acc: 0.1762 - val_loss: -2.0640 - val_acc: 0.1500\n",
            "Epoch 27/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: -2.1118 - acc: 0.1762 - val_loss: -2.6696 - val_acc: 0.1500\n",
            "Epoch 28/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: -2.6996 - acc: 0.1762 - val_loss: -3.3391 - val_acc: 0.1500\n",
            "Epoch 29/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: -3.2372 - acc: 0.1762 - val_loss: -3.7881 - val_acc: 0.1500\n",
            "Epoch 30/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: -3.5773 - acc: 0.1762 - val_loss: -4.0777 - val_acc: 0.1500\n",
            "Epoch 31/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: -3.8171 - acc: 0.1762 - val_loss: -4.3441 - val_acc: 0.1500\n",
            "Epoch 32/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: -4.0399 - acc: 0.1762 - val_loss: -4.5578 - val_acc: 0.1500\n",
            "Epoch 33/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: -4.2291 - acc: 0.1762 - val_loss: -4.7694 - val_acc: 0.1500\n",
            "Epoch 34/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: -4.4147 - acc: 0.1762 - val_loss: -4.9654 - val_acc: 0.1500\n",
            "Epoch 35/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: -4.5878 - acc: 0.1762 - val_loss: -5.1589 - val_acc: 0.1500\n",
            "Epoch 36/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: -4.7547 - acc: 0.1762 - val_loss: -5.3404 - val_acc: 0.1500\n",
            "Epoch 37/50\n",
            "800/800 [==============================] - 1s 2ms/step - loss: -4.9147 - acc: 0.1762 - val_loss: -5.5116 - val_acc: 0.1500\n",
            "Epoch 38/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: -5.0662 - acc: 0.1762 - val_loss: -5.6704 - val_acc: 0.1500\n",
            "Epoch 39/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: -5.2078 - acc: 0.1762 - val_loss: -5.8234 - val_acc: 0.1500\n",
            "Epoch 40/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: -5.3439 - acc: 0.1762 - val_loss: -5.9741 - val_acc: 0.1500\n",
            "Epoch 41/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: -5.4769 - acc: 0.1762 - val_loss: -6.1048 - val_acc: 0.1500\n",
            "Epoch 42/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: -5.5936 - acc: 0.1762 - val_loss: -6.2357 - val_acc: 0.1500\n",
            "Epoch 43/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: -5.7130 - acc: 0.1762 - val_loss: -6.3752 - val_acc: 0.1500\n",
            "Epoch 44/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: -5.8322 - acc: 0.1762 - val_loss: -6.4891 - val_acc: 0.1500\n",
            "Epoch 45/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: -5.9367 - acc: 0.1762 - val_loss: -6.6114 - val_acc: 0.1500\n",
            "Epoch 46/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: -6.0465 - acc: 0.1762 - val_loss: -6.7252 - val_acc: 0.1500\n",
            "Epoch 47/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: -6.1486 - acc: 0.1762 - val_loss: -6.8431 - val_acc: 0.1500\n",
            "Epoch 48/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: -6.2519 - acc: 0.1762 - val_loss: -6.9555 - val_acc: 0.1500\n",
            "Epoch 49/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: -6.3541 - acc: 0.1762 - val_loss: -7.0685 - val_acc: 0.1500\n",
            "Epoch 50/50\n",
            "800/800 [==============================] - 1s 1ms/step - loss: -6.4537 - acc: 0.1762 - val_loss: -7.1711 - val_acc: 0.1500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcQGS4Vpi_Yy",
        "colab_type": "text"
      },
      "source": [
        "### 4) Discuss 1) which model(s) performed best and speculate about 2) how you might try to further improve the predictive power of your model (e.g. Glove embeddings? More layers? Combining Conv1D with LSTM layers? More LSTM hidden nodes?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ma6ibo3DrtJf",
        "colab_type": "text"
      },
      "source": [
        "Strangely, my validation accuracy for every model, with every epoch, is .15. I training and validation sizes, which changes the validation accuracy but every ,model still has the same. The loss for each epoch, in each model, is different, so I know that the same model is not being run each time. Given that the validation accuracy is the same for each model, I look to the validation loss to determine the best model. I expect the model with the least deviation from 0 to have the least loss. Given that, my model with bidirectional sequential layers and no dropout (model 5). It returned a validation loss of 0.3639.\n",
        "\n",
        "To improve the predictive power od the model, I'd use glove embeddings, which would use pretrained weights. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ameaGrm4vLqw",
        "colab_type": "text"
      },
      "source": [
        "Github link - "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7cQQrWFjC1A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}