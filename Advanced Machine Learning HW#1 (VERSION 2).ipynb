{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projects in Advanced Machine Learning HW#1 - Paarth Malkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import sklearn as sk\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore Warnings\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Happiness_level  Happiness_ordinal Country or region Abbreviation Continent  \\\n",
      "0       Very High                  5           Finland          FIN    Europe   \n",
      "1       Very High                  5           Denmark          DNK    Europe   \n",
      "2       Very High                  5            Norway          NOR    Europe   \n",
      "3       Very High                  5           Iceland          ISL    Europe   \n",
      "4       Very High                  5       Netherlands          NLD    Europe   \n",
      "\n",
      "         Subregion  GDPcapita  SocialSupport  HealthyLifeExpect  \\\n",
      "0  Northern Europe      1.340          1.587              0.986   \n",
      "1  Northern Europe      1.383          1.573              0.996   \n",
      "2  Northern Europe      1.488          1.582              1.028   \n",
      "3  Northern Europe      1.380          1.624              1.026   \n",
      "4   Western Europe      1.396          1.522              0.999   \n",
      "\n",
      "   FreedomLifeChoices  Generosity  PerceptionsCorruptions  \n",
      "0               0.596       0.153                   0.393  \n",
      "1               0.592       0.252                   0.410  \n",
      "2               0.603       0.271                   0.341  \n",
      "3               0.591       0.354                   0.118  \n",
      "4               0.557       0.322                   0.298  \n",
      "---------------------------------------------------------------------------------\n",
      "       Happiness_ordinal   GDPcapita  SocialSupport  HealthyLifeExpect  \\\n",
      "count         156.000000  156.000000     156.000000         156.000000   \n",
      "mean            3.006410    0.905147       1.208814           0.725244   \n",
      "std             1.416478    0.398389       0.299191           0.242124   \n",
      "min             1.000000    0.000000       0.000000           0.000000   \n",
      "25%             2.000000    0.602750       1.055750           0.547750   \n",
      "50%             3.000000    0.960000       1.271500           0.789000   \n",
      "75%             4.000000    1.232500       1.452500           0.881750   \n",
      "max             5.000000    1.684000       1.624000           1.141000   \n",
      "\n",
      "       FreedomLifeChoices  Generosity  PerceptionsCorruptions  \n",
      "count          156.000000  156.000000              156.000000  \n",
      "mean             0.392571    0.184846                0.110603  \n",
      "std              0.143289    0.095254                0.094538  \n",
      "min              0.000000    0.000000                0.000000  \n",
      "25%              0.308000    0.108750                0.047000  \n",
      "50%              0.417000    0.177500                0.085500  \n",
      "75%              0.507250    0.248250                0.141250  \n",
      "max              0.631000    0.566000                0.453000  \n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(r\"C:\\Users\\paart_000\\Documents\\Columbia\\Advanced Machine Learning/MLHW1_World_Regions_csv.csv\")\n",
    "print (data.head())\n",
    "print ('---------------------------------------------------------------------------------')\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Explore bivariate results (Use visualizations!). Describe any relationships you see between particular features and the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:      Happiness_ordinal   R-squared:                       0.545\n",
      "Model:                            OLS   Adj. R-squared:                  0.543\n",
      "Method:                 Least Squares   F-statistic:                     184.8\n",
      "Date:                Sun, 16 Feb 2020   Prob (F-statistic):           3.69e-28\n",
      "Time:                        23:00:36   Log-Likelihood:                -213.66\n",
      "No. Observations:                 156   AIC:                             431.3\n",
      "Df Residuals:                     154   BIC:                             437.4\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=================================================================================\n",
      "                    coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------\n",
      "Intercept        -1.2204      0.320     -3.811      0.000      -1.853      -0.588\n",
      "SocialSupport     3.4967      0.257     13.595      0.000       2.989       4.005\n",
      "==============================================================================\n",
      "Omnibus:                        4.377   Durbin-Watson:                   1.028\n",
      "Prob(Omnibus):                  0.112   Jarque-Bera (JB):                4.357\n",
      "Skew:                          -0.371   Prob(JB):                        0.113\n",
      "Kurtosis:                       2.656   Cond. No.                         8.43\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# OLS Regression between social support (ind. variable) and happiness level (dep. variable)\n",
    "\n",
    "socsupport_happiness_lr = smf.ols(formula = \"Happiness_ordinal ~ SocialSupport\", data=data).fit()\n",
    "print (socsupport_happiness_lr.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To explore a bivariate result, I ran an OLS regression between the independent variable of social support and the target variable of happiness. In the dataset, I created another column where I set ordinal values that correspond the happiness level. 5 corresponds to \"very high\" and 1 corresponds to \"very low.\" I hypothesized that as levels of social support go up, happiness increases. According to the regression results, I was correct in my findings. As social support goes up by 1, happiness goes up by 3.497. This is highly statistically significant given the p-score of 0. This means that there is 0% chance this relationship exists randomly. Additionally, the model fits the data relatively well, with an r-squared value of .545. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Bar chart of Happiness (1 = very low, 5 = very high) and Social Support\n",
    "\n",
    "plt.bar(data.Happiness_ordinal, data.SocialSupport)\n",
    "plt.xticks(data.Happiness_ordinal)\n",
    "plt.ylabel(\"Social Support\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As seen in the bar chart above, countries with \"very high\" levels of happines (5) enjoy more social support than countries with \"very low\" levels of happiness, who have the lowest levels of social support. This suggests that the more social support a country provides it's citizens, the happier the citizens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:      Happiness_ordinal   R-squared:                       0.624\n",
      "Model:                            OLS   Adj. R-squared:                  0.621\n",
      "Method:                 Least Squares   F-statistic:                     255.1\n",
      "Date:                Sun, 16 Feb 2020   Prob (F-statistic):           1.74e-34\n",
      "Time:                        23:00:40   Log-Likelihood:                -198.97\n",
      "No. Observations:                 156   AIC:                             401.9\n",
      "Df Residuals:                     154   BIC:                             408.0\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      0.4652      0.174      2.677      0.008       0.122       0.808\n",
      "GDPcapita      2.8076      0.176     15.970      0.000       2.460       3.155\n",
      "==============================================================================\n",
      "Omnibus:                        2.557   Durbin-Watson:                   1.398\n",
      "Prob(Omnibus):                  0.278   Jarque-Bera (JB):                2.277\n",
      "Skew:                          -0.294   Prob(JB):                        0.320\n",
      "Kurtosis:                       3.068   Cond. No.                         4.77\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# OLS Regression between GDP per capita (ind. variable) and happiness level (dep. variable)\n",
    "\n",
    "GDPcapita_happiness_lr = smf.ols(formula = \"Happiness_ordinal~ GDPcapita\", data=data).fit()\n",
    "print (GDPcapita_happiness_lr.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ##### To another bivariate result, I ran an OLS regression between the independent variable of GDP per capita and the dependent variable of happiness. I hypothesized that as GDP per capita goes up, happiness increases. According to the regression results, I was correct in my findings. As GDP per capita goes up by 1, happiness goes up by 2.808. This is highly statistically significant given the p-score of 0. This means that there is 0% chance this relationship exists randomly. Additionally, the model fits the data well, with an r-squared value of .624. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Bar chart of Happiness (1 = very low, 5 = very high) and GDP per capita\n",
    "\n",
    "plt.bar(data.Happiness_ordinal, data.GDPcapita)\n",
    "plt.xticks(data.Happiness_ordinal)\n",
    "plt.ylabel(\"GDP per capita\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As seen in the bar chart above, countries with higher happiness levels enjoy significantly higher GDP per capita than countries with lower levels of happiness. This suggests that the more wealth a country has, the happier it's citizens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156, 7)\n",
      "         Subregion  GDPcapita  SocialSupport  HealthyLifeExpect  \\\n",
      "0  Northern Europe      1.340          1.587              0.986   \n",
      "1  Northern Europe      1.383          1.573              0.996   \n",
      "2  Northern Europe      1.488          1.582              1.028   \n",
      "3  Northern Europe      1.380          1.624              1.026   \n",
      "4   Western Europe      1.396          1.522              0.999   \n",
      "\n",
      "   FreedomLifeChoices  Generosity  PerceptionsCorruptions  \n",
      "0               0.596       0.153                   0.393  \n",
      "1               0.592       0.252                   0.410  \n",
      "2               0.603       0.271                   0.341  \n",
      "3               0.591       0.354                   0.118  \n",
      "4               0.557       0.322                   0.298  \n",
      "(156,)\n",
      "0    Very High\n",
      "1    Very High\n",
      "2    Very High\n",
      "3    Very High\n",
      "4    Very High\n",
      "Name: Happiness_level, dtype: object\n",
      "(117, 7)\n",
      "(117,)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into target and feature variables to prepare for one hot encoding and subsequent prediction models\n",
    "\n",
    "X = data.drop([\"Happiness_level\", \"Happiness_ordinal\", \"Country or region\", \"Abbreviation\", \"Continent\"], axis = 1)\n",
    "y = data[\"Happiness_level\"]\n",
    "\n",
    "print (X.shape)\n",
    "print (X.head())\n",
    "print (y.shape)\n",
    "print (y.head())\n",
    "\n",
    "# Train, test, splitting of target and feature variables\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(117, 20)\n",
      "(117, 5)\n"
     ]
    }
   ],
   "source": [
    "# Developing preprocessing pipelines \n",
    "\n",
    "numeric_features=X.columns.tolist()\n",
    "numeric_features.remove('Subregion')\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_features = ['Subregion']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "\n",
    "#Fitting preprocessor object\n",
    "prediction_input_preprocessor=preprocessor.fit(X_train) \n",
    "\n",
    "pickle.dump(prediction_input_preprocessor, open( \"preprocessor.pkl\", \"wb\" ) )\n",
    "\n",
    "# Looking at post-processed shapes\n",
    "print (prediction_input_preprocessor.transform(X_train).shape)\n",
    "print (pd.get_dummies(y_train).shape)\n",
    "\n",
    "# Assigning processed X and y train data to objects\n",
    "\n",
    "processed_X_train = prediction_input_preprocessor.transform(X_train)\n",
    "processed_y_train = pd.get_dummies(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine features that predict happiness categories using one or more models that allow for automatic feature selection. Explain any meaningful findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.07871879 0.93739894 1.83191933 1.85504487 0.21958188 0.26051596]\n",
      "(6,)\n"
     ]
    }
   ],
   "source": [
    "# Lasso regression to observe predictive variables with target variable of happiness\n",
    "\n",
    "X_predictive = data.drop([\"Happiness_level\", \"Happiness_ordinal\", \"Country or region\", \"Abbreviation\", \"Continent\", \"Subregion\"], axis = 1)\n",
    "y_predictive = data[\"Happiness_ordinal\"]\n",
    "\n",
    "vari_lasso = Lasso(alpha = 0, normalize = True)\n",
    "vari_lasso_coef = vari_lasso.fit(X_predictive, y_predictive).coef_\n",
    "print(vari_lasso_coef)\n",
    "print(vari_lasso_coef.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Above, I ran an lasso regression to determine which feature variables are most predictive of the target variable. The features are: GDP per capita, social support, healthy life expectancy, freedom of life choices, generosity, and perceptions of corruption. The target variable is the ordinal ranking of happiness in a range of 1 to 5. 5 corresponds to \"very high\" happiness and 1 corresponds to \"very low\" happiness. According to the lasso regression, the features that are most predictive, in order from greatest to least, are: 1) freedom of life choices, 2) healthy life expectancy, 3) GDP per capita, 4) social support, 5) perceptions of corruption, 6) generosity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Run at least three prediction models to try to predict World Happiness well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best gridsearch hyperparameter value: {'n_neighbors': 1}\n",
      "KNN classifier gridsearch cross-validation score: 0.5213675213675214\n",
      "KNN classifier test data accuracy: 0.3076923076923077\n"
     ]
    }
   ],
   "source": [
    "# KNN classifier with gridsearch and 10 fold CV\n",
    "\n",
    "param_grid = {\"n_neighbors\": np.arange(1, 50)}\n",
    "knnc = KNeighborsClassifier()\n",
    "knnc_gscv = GridSearchCV(knnc, param_grid, cv = 10)\n",
    "knnc_gscv.fit(processed_X_train, processed_y_train)\n",
    "\n",
    "print (\"Best gridsearch hyperparameter value:\", knnc_gscv.best_params_)\n",
    "print (\"KNN classifier gridsearch cross-validation score:\", knnc_gscv.best_score_)\n",
    "print (\"KNN classifier test data accuracy:\", knnc_gscv.score(prediction_input_preprocessor.transform(X_test), pd.get_dummies(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Above, I ran a KNN classifier model with gridsearch and 10 fold CV. I set the n_neighbors parameters grid range from 1 to 50. After running the model with 10 folds, the best model returned used 1 neighbor as the hyperparameter. The best CV score is .521 and the test-set accuracy is .308."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN classifier w/3 neighbors mean CV score: 0.4446969696969697\n",
      "KNN classifier w/3 neighbors test data accuracy: 0.38461538461538464\n"
     ]
    }
   ],
   "source": [
    "# KNN classifier with 3 neighbors and 10 fold cv\n",
    "# (NOTE: I'm an extra KNN model b/c I want to see the scores if I set the n_neighbors to 3)\n",
    "\n",
    "knnc_2 = KNeighborsClassifier(n_neighbors = 3).fit(processed_X_train, processed_y_train)\n",
    "knn_cv_scores = cross_val_score(knnc_2, processed_X_train, processed_y_train, cv =10)\n",
    "print (\"KNN classifier w/3 neighbors mean CV score:\", np.mean(knn_cv_scores))\n",
    "print (\"KNN classifier w/3 neighbors test data accuracy:\", knnc_2.score(prediction_input_preprocessor.transform(X_test), pd.get_dummies(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Above, I developed another KNN classifier model, this time relinquishing the gridsearch and setting the neighbors value to 3. I did so because in the previous KNN model, the gridsearch returned the best hyperparameter as 1, and I wanted to see if it would return a higher accuracy with more neighbors. The best CV score decreased to .445 however the test-set accuracy increased to .385. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best gridsearch hyperparameter value: {'n_estimators': 1}\n",
      "Random forests classifier gridsearch cross-validation score: 0.5128205128205128\n",
      "Random forests classifier test data accuracy: 0.28205128205128205\n"
     ]
    }
   ],
   "source": [
    "# Random forest classifier with gridsearch and 10 fold CV\n",
    "\n",
    "param_grid_2 = {\"n_estimators\": np.arange(1, 100)}\n",
    "rfc = RandomForestClassifier()\n",
    "rfc_gscv = GridSearchCV(rfc, param_grid_2, cv=10)\n",
    "rfc_gscv.fit(processed_X_train, processed_y_train)\n",
    "\n",
    "print (\"Best gridsearch hyperparameter value:\", rfc_gscv.best_params_)\n",
    "print (\"Random forests classifier gridsearch cross-validation score:\", rfc_gscv.best_score_)\n",
    "print (\"Random forests classifier test data accuracy:\", rfc_gscv.score(prediction_input_preprocessor.transform(X_test), pd.get_dummies(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Above, I ran a random forest model with gridsearch and 10 fold CV. I set the n_estimators hyperparameter range from 1 to 100. The best model returned used an n_estimators value of 1. The best CV score is .513 and the test-set accuracy is .282. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best gridsearch hyperparameter value: {'C': 1}\n",
      "Logistic regression gridsearch cross-validation score: 0.5726495726495726\n",
      "Logistic regression test data accuracy: 0.38461538461538464\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression with gridsearch and 10 fold CV\n",
    "\n",
    "param_grid_3 = {\"C\": np.arange(1, 1000)}\n",
    "logreg = LogisticRegression()\n",
    "log_gscv = GridSearchCV(logreg, param_grid_3, cv=5)\n",
    "log_gscv.fit(processed_X_train, y_train)\n",
    "log_gscv.best_score_\n",
    "\n",
    "print (\"Best gridsearch hyperparameter value:\", log_gscv.best_params_)\n",
    "print (\"Logistic regression gridsearch cross-validation score:\", log_gscv.best_score_)\n",
    "print (\"Logistic regression test data accuracy:\", log_gscv.score(prediction_input_preprocessor.transform(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Above, I ran a logistic regression model with gridsearch and 10 fold CV. I set the C hyperparameter range from 1 to 1000. The best model returned used a C value of 1. The best CV score is .573 and the test-set accuracy is .385."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Of every model I ran, the logistic regression with a hyperparameter of 1 and 10 folds returned the highest accuracy. I has a CV score of .573 and test-set accuracy of .385. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction of target data using prior logistic regression model\n",
    "\n",
    "y_log_gscv_pred=log_gscv.predict(prediction_input_preprocessor.transform(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posting to World Happiness AI Model Share Leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preparing model results for sharing\n",
    "\n",
    "# # prediction_index=logreg.predict_classes(prediction_input_preprocessor.transform(X_test))\n",
    "\n",
    "# #Now lets run some code to get keras to return the label rather than the index...\n",
    "\n",
    "# # get labels from one hot encoded y_train data\n",
    "# labels=processed_y_train.columns\n",
    "\n",
    "# # Function to use to return label from column index location\n",
    "# def index_to_label(labels,index_n): \n",
    "#     return labels[index_n]\n",
    "    \n",
    "# # Example: return label at predicted index location 1\n",
    "# index_to_label(labels,1)\n",
    "\n",
    "# # Iterate through all predicted indices using map method\n",
    "\n",
    "# predicted_labels=list(map(lambda x: labels[x], y_pred))\n",
    "# print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>mse</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mae</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.368218</td>\n",
       "      <td>0.41859</td>\n",
       "      <td>0.408333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  f1_score  precision    recall  mse  rmse  mae  r2\n",
       "0  0.384615  0.368218    0.41859  0.408333    0     0    0   0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "\n",
    "def model_eval_metrics(y_true, y_pred,classification=\"TRUE\"):\n",
    "     if classification==\"TRUE\":\n",
    "        accuracy_eval = accuracy_score(y_true, y_pred)\n",
    "        f1_score_eval = f1_score(y_true, y_pred,average=\"macro\")\n",
    "        precision_eval = precision_score(y_true, y_pred,average=\"macro\")\n",
    "        recall_eval = recall_score(y_true, y_pred,average=\"macro\")\n",
    "        mse_eval = 0\n",
    "        rmse_eval = 0\n",
    "        mae_eval = 0\n",
    "        r2_eval = 0\n",
    "        metricdata = {'accuracy': [accuracy_eval], 'f1_score': [f1_score_eval], 'precision': [precision_eval], 'recall': [recall_eval], 'mse': [mse_eval], 'rmse': [rmse_eval], 'mae': [mae_eval], 'r2': [r2_eval]}\n",
    "        finalmetricdata = pd.DataFrame.from_dict(metricdata)\n",
    "     else:\n",
    "        accuracy_eval = 0\n",
    "        f1_score_eval = 0\n",
    "        precision_eval = 0\n",
    "        recall_eval = 0\n",
    "        mse_eval = mean_squared_error(y_true, y_pred)\n",
    "        rmse_eval = sqrt(mean_squared_error(y_true, y_pred))\n",
    "        mae_eval = mean_absolute_error(y_true, y_pred)\n",
    "        r2_eval = r2_score(y_true, y_pred)\n",
    "        metricdata = {'accuracy': [accuracy_eval], 'f1_score': [f1_score_eval], 'precision': [precision_eval], 'recall': [recall_eval], 'mse': [mse_eval], 'rmse': [rmse_eval], 'mae': [mae_eval], 'r2': [r2_eval]}\n",
    "        finalmetricdata = pd.DataFrame.from_dict(metricdata)\n",
    "     return finalmetricdata\n",
    "\n",
    "model_eval_metrics(y_test,y_log_gscv_pred,classification=\"TRUE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/mikedparrott/aimodelshare/blob/master/aimodelshare-0.0.2.tar.gz?raw=true\n",
      "  Using cached https://github.com/mikedparrott/aimodelshare/blob/master/aimodelshare-0.0.2.tar.gz?raw=true\n",
      "Requirement already satisfied (use --upgrade to upgrade): aimodelshare==0.0.2 from https://github.com/mikedparrott/aimodelshare/blob/master/aimodelshare-0.0.2.tar.gz?raw=true in c:\\users\\paart_000\\anaconda3\\lib\\site-packages\n",
      "Building wheels for collected packages: aimodelshare\n",
      "  Building wheel for aimodelshare (setup.py): started\n",
      "  Building wheel for aimodelshare (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\paart_000\\AppData\\Local\\pip\\Cache\\wheels\\31\\8d\\ac\\09cb6ef7374ec79e02843c347195e5478144006b11def6799a\n",
      "Successfully built aimodelshare\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install https://github.com/mikedparrott/aimodelshare/blob/master/aimodelshare-0.0.2.tar.gz?raw=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting skl2onnx\n",
      "  Downloading https://files.pythonhosted.org/packages/11/8b/21a32b9f4669959678db5abe41774b2ce0544a92ed42683cf79b065d7a8b/skl2onnx-1.6.0-py2.py3-none-any.whl (161kB)\n",
      "Requirement already satisfied: six in c:\\users\\paart_000\\anaconda3\\lib\\site-packages (from skl2onnx) (1.12.0)\n",
      "Collecting onnx>=1.2.1 (from skl2onnx)\n",
      "  Downloading https://files.pythonhosted.org/packages/cd/79/6b7cf92999b1646116e13df4172a74f19c8c4797ab4ad3040076eacdef8b/onnx-1.6.0-cp37-cp37m-win_amd64.whl (4.5MB)\n",
      "Collecting protobuf (from skl2onnx)\n",
      "  Downloading https://files.pythonhosted.org/packages/92/30/1b7ccde09bf0c535d11f18a574ed7d7572c729a8f754fd568b297be08b61/protobuf-3.11.3-cp37-cp37m-win_amd64.whl (1.0MB)\n",
      "Requirement already satisfied: scikit-learn>=0.19 in c:\\users\\paart_000\\anaconda3\\lib\\site-packages (from skl2onnx) (0.21.2)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\paart_000\\anaconda3\\lib\\site-packages (from skl2onnx) (1.16.4)\n",
      "Collecting onnxconverter-common>=1.5.1 (from skl2onnx)\n",
      "  Downloading https://files.pythonhosted.org/packages/77/3d/6112c19223d1eabbedf1b063567034e1463a11d7c82d1820f26b75d14e3c/onnxconverter_common-1.6.0-py2.py3-none-any.whl (43kB)\n",
      "Collecting typing-extensions>=3.6.2.1 (from onnx>=1.2.1->skl2onnx)\n",
      "  Downloading https://files.pythonhosted.org/packages/03/92/705fe8aca27678e01bbdd7738173b8e7df0088a2202c80352f664630d638/typing_extensions-3.7.4.1-py3-none-any.whl\n",
      "Requirement already satisfied: setuptools in c:\\users\\paart_000\\anaconda3\\lib\\site-packages (from protobuf->skl2onnx) (41.0.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\paart_000\\anaconda3\\lib\\site-packages (from scikit-learn>=0.19->skl2onnx) (0.13.2)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\users\\paart_000\\anaconda3\\lib\\site-packages (from scikit-learn>=0.19->skl2onnx) (1.2.1)\n",
      "Installing collected packages: protobuf, typing-extensions, onnx, onnxconverter-common, skl2onnx\n",
      "Successfully installed onnx-1.6.0 onnxconverter-common-1.6.0 protobuf-3.11.3 skl2onnx-1.6.0 typing-extensions-3.7.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install skl2onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting from sklearn to onnx\n",
    "import os\n",
    "\n",
    "if not os.path.exists('mymodel.onnx'):\n",
    "    from skl2onnx import convert_sklearn\n",
    "    from skl2onnx.common.data_types import FloatTensorType\n",
    "    initial_type = [('mymodel.onnx', FloatTensorType([None, 4]))]\n",
    "    onx = convert_sklearn(log_gscv, initial_types=initial_type)\n",
    "    with open(\"mymodel.onnx\", \"wb\") as f:\n",
    "        f.write(onx.SerializeToString())\n",
    "        \n",
    "# from skl2onnx import convert_sklearn\n",
    "# from skl2onnx.common.data_types import FloatTensorType\n",
    "# initial_type = [('float_input', FloatTensorType([None, 4]))]\n",
    "# onx = convert_sklearn(log_gscv, initial_types=initial_type)\n",
    "# with open(\"rf_iris.onnx\", \"wb\") as f:\n",
    "#     f.write(onx.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'worldhappiness_modelsubmission_keys.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-c2ab8949a2a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Saving logistic regression model to pkl file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0maws_key_password_region\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34m\"worldhappiness_modelsubmission_keys.pkl\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m# pickle.dump(log_gscv, open( \"rff_model.pkl\", \"wb\" ) )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'worldhappiness_modelsubmission_keys.pkl'"
     ]
    }
   ],
   "source": [
    "# Saving logistic regression model to pkl file\n",
    "\n",
    "aws_key_password_region = pickle.load(open( \"worldhappiness_modelsubmission_keys.pkl\", \"rb\" ) ) \n",
    "# pickle.dump(log_gscv, open( \"rff_model.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnsupportedOperation",
     "evalue": "read",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnsupportedOperation\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-b5b5102006b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m#Importing from object that stores keys so we do not print out keys for others to see.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0maws_key_password_region\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"worldhappiness_modelsubmission_keys.pkl\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0maws_key\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maws_key_password_region\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnsupportedOperation\u001b[0m: read"
     ]
    }
   ],
   "source": [
    "apiurl=\"https://btuvanmi55.execute-api.us-east-1.amazonaws.com/prod/m\"\n",
    "username = \n",
    "password = \n",
    "\n",
    "region='us-east-1'\n",
    "model_filepath=\"mymodel.onnx\"   \n",
    "preprocessor_filepath=\"preprocessor.pkl\"\n",
    "preprocessor=\"TRUE\"\n",
    "\n",
    "trainingdata=X_train\n",
    "\n",
    "# Set aws keys for this project (these keys give you access to collaborate on a single project)\n",
    "\n",
    "#Importing from object that stores keys so we do not print out keys for others to see.\n",
    "\n",
    "aws_key_password_region = pickle.load( open(\"worldhappiness_modelsubmission_keys.pkl\", \"wb\" ) )\n",
    "\n",
    "aws_key=aws_key_password_region[0]\n",
    "aws_password=aws_key_password_region[1]\n",
    "region=aws_key_password_region[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
