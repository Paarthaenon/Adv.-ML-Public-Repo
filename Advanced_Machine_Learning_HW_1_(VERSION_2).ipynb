{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Advanced Machine Learning HW#1 (VERSION 2).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Paarthaenon/Adv.-ML-Public-Repo/blob/master/Advanced_Machine_Learning_HW_1_(VERSION_2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55tdTuc7JIFh",
        "colab_type": "text"
      },
      "source": [
        "# Projects in Advanced Machine Learning HW#1 - Paarth Malkan"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_s4qYMHJIFu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "import sklearn as sk\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oy6X3nVIJIGK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ignore Warnings\n",
        "\n",
        "from warnings import filterwarnings\n",
        "filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruwS3_nMJIGg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "c4879e80-fade-4f73-ffd6-b8127085ef68"
      },
      "source": [
        "data = pd.read_csv(\"MLHW1_World_Regions_csv.csv\")\n",
        "print (data.head())\n",
        "print ('---------------------------------------------------------------------------------')\n",
        "print(data.describe())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Happiness_level  Happiness_ordinal  ... Generosity PerceptionsCorruptions\n",
            "0       Very High                  5  ...      0.153                  0.393\n",
            "1       Very High                  5  ...      0.252                  0.410\n",
            "2       Very High                  5  ...      0.271                  0.341\n",
            "3       Very High                  5  ...      0.354                  0.118\n",
            "4       Very High                  5  ...      0.322                  0.298\n",
            "\n",
            "[5 rows x 12 columns]\n",
            "---------------------------------------------------------------------------------\n",
            "       Happiness_ordinal   GDPcapita  ...  Generosity  PerceptionsCorruptions\n",
            "count         156.000000  156.000000  ...  156.000000              156.000000\n",
            "mean            3.006410    0.905147  ...    0.184846                0.110603\n",
            "std             1.416478    0.398389  ...    0.095254                0.094538\n",
            "min             1.000000    0.000000  ...    0.000000                0.000000\n",
            "25%             2.000000    0.602750  ...    0.108750                0.047000\n",
            "50%             3.000000    0.960000  ...    0.177500                0.085500\n",
            "75%             4.000000    1.232500  ...    0.248250                0.141250\n",
            "max             5.000000    1.684000  ...    0.566000                0.453000\n",
            "\n",
            "[8 rows x 7 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwgVGXTsJIG7",
        "colab_type": "text"
      },
      "source": [
        "### 1. Explore bivariate results (Use visualizations!). Describe any relationships you see between particular features and the target variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7e2rWYJfJIHC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "outputId": "f2e9b91b-8fea-4e3a-8f37-fa7dcfe9a494"
      },
      "source": [
        "# OLS Regression between social support (ind. variable) and happiness level (dep. variable)\n",
        "\n",
        "socsupport_happiness_lr = smf.ols(formula = \"Happiness_ordinal ~ SocialSupport\", data=data).fit()\n",
        "print (socsupport_happiness_lr.summary())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:      Happiness_ordinal   R-squared:                       0.545\n",
            "Model:                            OLS   Adj. R-squared:                  0.543\n",
            "Method:                 Least Squares   F-statistic:                     184.8\n",
            "Date:                Fri, 21 Feb 2020   Prob (F-statistic):           3.69e-28\n",
            "Time:                        01:04:35   Log-Likelihood:                -213.66\n",
            "No. Observations:                 156   AIC:                             431.3\n",
            "Df Residuals:                     154   BIC:                             437.4\n",
            "Df Model:                           1                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=================================================================================\n",
            "                    coef    std err          t      P>|t|      [0.025      0.975]\n",
            "---------------------------------------------------------------------------------\n",
            "Intercept        -1.2204      0.320     -3.811      0.000      -1.853      -0.588\n",
            "SocialSupport     3.4967      0.257     13.595      0.000       2.989       4.005\n",
            "==============================================================================\n",
            "Omnibus:                        4.377   Durbin-Watson:                   1.028\n",
            "Prob(Omnibus):                  0.112   Jarque-Bera (JB):                4.357\n",
            "Skew:                          -0.371   Prob(JB):                        0.113\n",
            "Kurtosis:                       2.656   Cond. No.                         8.43\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGkoioPjJIHW",
        "colab_type": "text"
      },
      "source": [
        "##### To explore a bivariate result, I ran an OLS regression between the independent variable of social support and the target variable of happiness. In the dataset, I created another column where I set ordinal values that correspond the happiness level. 5 corresponds to \"very high\" and 1 corresponds to \"very low.\" I hypothesized that as levels of social support go up, happiness increases. According to the regression results, I was correct in my findings. As social support goes up by 1, happiness goes up by 3.497. This is highly statistically significant given the p-score of 0. This means that there is 0% chance this relationship exists randomly. Additionally, the model fits the data relatively well, with an r-squared value of .545. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0zIQxlXJIHa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "outputId": "0453c23e-7cbb-4795-97b6-6c8c8e9dec67"
      },
      "source": [
        "# Bar chart of Happiness (1 = very low, 5 = very high) and Social Support\n",
        "\n",
        "plt.bar(data.Happiness_ordinal, data.SocialSupport)\n",
        "plt.xticks(data.Happiness_ordinal)\n",
        "plt.ylabel(\"Social Support\")\n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4SEXjIHJIHu",
        "colab_type": "text"
      },
      "source": [
        "##### As seen in the bar chart above, countries with \"very high\" levels of happines (5) enjoy more social support than countries with \"very low\" levels of happiness, who have the lowest levels of social support. This suggests that the more social support a country provides it's citizens, the happier the citizens. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1ksPh37JIHz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "outputId": "e26b16a3-fad8-4208-e194-e7f1b36cc608"
      },
      "source": [
        "# OLS Regression between GDP per capita (ind. variable) and happiness level (dep. variable)\n",
        "\n",
        "GDPcapita_happiness_lr = smf.ols(formula = \"Happiness_ordinal~ GDPcapita\", data=data).fit()\n",
        "print (GDPcapita_happiness_lr.summary())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:      Happiness_ordinal   R-squared:                       0.624\n",
            "Model:                            OLS   Adj. R-squared:                  0.621\n",
            "Method:                 Least Squares   F-statistic:                     255.1\n",
            "Date:                Fri, 21 Feb 2020   Prob (F-statistic):           1.74e-34\n",
            "Time:                        01:04:39   Log-Likelihood:                -198.97\n",
            "No. Observations:                 156   AIC:                             401.9\n",
            "Df Residuals:                     154   BIC:                             408.0\n",
            "Df Model:                           1                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "Intercept      0.4652      0.174      2.677      0.008       0.122       0.808\n",
            "GDPcapita      2.8076      0.176     15.970      0.000       2.460       3.155\n",
            "==============================================================================\n",
            "Omnibus:                        2.557   Durbin-Watson:                   1.398\n",
            "Prob(Omnibus):                  0.278   Jarque-Bera (JB):                2.277\n",
            "Skew:                          -0.294   Prob(JB):                        0.320\n",
            "Kurtosis:                       3.068   Cond. No.                         4.77\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A95YchBJIIE",
        "colab_type": "text"
      },
      "source": [
        "##### ##### To another bivariate result, I ran an OLS regression between the independent variable of GDP per capita and the dependent variable of happiness. I hypothesized that as GDP per capita goes up, happiness increases. According to the regression results, I was correct in my findings. As GDP per capita goes up by 1, happiness goes up by 2.808. This is highly statistically significant given the p-score of 0. This means that there is 0% chance this relationship exists randomly. Additionally, the model fits the data well, with an r-squared value of .624. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vk9TqKPlJIIM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "outputId": "617d8cab-69e6-4de7-ea32-69ca32296cb5"
      },
      "source": [
        "# Bar chart of Happiness (1 = very low, 5 = very high) and GDP per capita\n",
        "\n",
        "plt.bar(data.Happiness_ordinal, data.GDPcapita)\n",
        "plt.xticks(data.Happiness_ordinal)\n",
        "plt.ylabel(\"GDP per capita\")\n",
        "plt.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAASj0lEQVR4nO3dfbRld13f8fcnkwwopqLOpdJMwgSb\nqFFxIZdAsV2EpzoBmlGJyoBYWmBqZVwtuChxtU1qWF2KWG2rARx0DD4RMbjCLImGJU9pxYS5ARry\nYOgQApnAci6BWkgqwyTf/nH2dR3v3Hvunpm7z8m9v/drrbNmP/zOPt899677Ob/92w+pKiRJ7Tpt\n1gVIkmbLIJCkxhkEktQ4g0CSGmcQSFLjTp91ASdq27ZttWPHjlmXIUkbyi233PKFqppbad2GC4Id\nO3awsLAw6zIkaUNJ8pnV1nloSJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CS\nGrfhriyWpBOx47L3zLqEdXPPL7xgkO3aI5CkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNGywIkuxP\nciTJbRPaXJTk40luT/KhoWqRJK1uyB7B1cDO1VYmeSzwZuCSqvou4EcGrEWStIrBgqCqbgS+OKHJ\nS4A/qqrPdu2PDFWLJGl1sxwjOB/4piQfTHJLkp9YrWGSPUkWkiwsLi5OsURJ2vxmGQSnA08BXgD8\nAPAfk5y/UsOq2ldV81U1Pzc3N80aJWnTm+W9hg4D91fVA8ADSW4Evhf45AxrkqTmzDII3g38WpLT\nga3A04BfmWE90qa1WW68NtRN11o3WBAkeQdwEbAtyWHgCuAMgKp6a1XdmeRPgVuBh4HfqKpVTzWV\nJA1jsCCoqt092rwJeNNQNUiS1uaVxZLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTG\nGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcbN8MI00NZvlwSzgw1m0/gbrESTZn+RIkokPm0ny\n1CTHklw6VC2SpNUNeWjoamDnpAZJtgBvBN47YB2SpAkGC4KquhH44hrNfhp4F3BkqDokSZPNbLA4\nyVnADwFvmVUNkqTZnjX0X4HXV9XDazVMsifJQpKFxcXFKZQmSe2Y5VlD88A1SQC2Ac9Pcqyqrlve\nsKr2AfsA5ufna6pVStImN7MgqKpzl6aTXA388UohIEka1mBBkOQdwEXAtiSHgSuAMwCq6q1Dfa4k\n6cQMFgRVtfsE2r58qDokSZN5iwlJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwC\nSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMGC4Ik+5McSXLbKutfmuTWJJ9I8uEk\n3ztULZKk1Q3ZI7ga2Dlh/aeBZ1bV9wBvoHs4vSRpuoZ8VOWNSXZMWP/hsdmbgO1D1SJJWt0jZYzg\nFcCfrLYyyZ4kC0kWFhcXp1iWJG1+Mw+CJM9iFASvX61NVe2rqvmqmp+bm5tecZLUgMEODfWR5EnA\nbwAXV9X9s6xFklrVOwiSPA549NJ8VX32VD44yTnAHwEvq6pPnsq2JEknb80gSHIJ8F+AfwAcAZ4A\n3Al81xrvewdwEbAtyWHgCuAMgKp6K3A58C3Am5MAHKuq+ZPdEUnSyenTI3gD8HTgz6rqyd0x/R9f\n601VtXuN9a8EXtmrSknSYPoMFn+tO35/WpLTquoDgN/cJWmT6NMj+D9JvgG4Efi9JEeAB4YtS5I0\nLX16BLuAB4HXAH8KfAp44ZBFSZKmp08QXF5VD1fVsap6e1X9dyac8y9J2lj6BMHzVlh28XoXIkma\njVXHCJL8a+CngCcmuXVs1ZnAnw9dmCRpOiYNFv8+o/v//Dxw2djyL1fVFwetSpI0NZOCoKrqniSv\nXr4iyTcbBhvPjsveM+sS1s09v/CCWZcgbRpr9QheCNwCFJCxdQU8ccC6JElTsmoQVNULu3/PnV45\nkqRp63XTuSQ/DPxjRj2B/1FV1w1alSRpatY8fTTJm4GfBD4B3Ab8ZJKrhi5MkjQdfXoEzwa+s6oK\nIMnbgdsHrUqSNDV9Lig7BJwzNn92t0yStAn06RGcCdyZ5COMxgguBBaSHACoqksGrG9defqkJB2v\nTxBcPngVkqSZWTMIqupDJ7PhJPsZXYdwpKq+e4X1Af4b8HxGdzd9eVV99GQ+S5J08vqcNfT0JAeT\nfCXJ0SQPJfm/PbZ9NbBzwvqLgfO61x7gLX0KliStrz6Dxb8G7Ab+N/B1jB4vuebpo1V1IzDpNhS7\ngN+ukZuAxyZ5fI96JEnrqE8QUFWHgC1V9VBV/RaTv+n3dRZw79j84W7ZcZLsSbKQZGFxcXEdPlqS\ntKTPYPGDSbYCH0/yi8Dn6Rkg66Wq9gH7AObn52uany1Jm12fP+gv69rtZfSs4rOBF63DZ9/XbWvJ\n9m6ZJGmK+vQIvgAcraq/AX4uyRbgUevw2QeAvUmuAZ4G/HVVfX4dtitJOgF9guB9wHOBr3TzXwe8\nF3jGpDcleQdwEbAtyWHgCuAMgKp6K3A9o1NHDzE6ffRfnHj5kqRT1ScIHl1VSyFAVX0lydev9aaq\n2r3G+gKOe+iNJGm6+owRPJDk+5ZmkjwF+H/DlSRJmqY+PYJ/C/xhks8xekrZtwI/NmhVkqSp6XOL\niYNJvgP49m7RXVX1tWHLkiRNS68nlHV/+G8buBZJ0gxM9cIwSdIjz8QgyMjZk9pIkja2iUHQneJ5\n/ZRqkSTNQJ9DQx9N8tTBK5EkzUSfweKnAS9N8hlG9xoKo87CkwatTJI0FX2C4AcGr0KSNDNrHhqq\nqs8wukvos7vpB/u8T5K0MfR5VOUVwOuBn+0WnQH87pBFSZKmp883+x8CLmE0PkBVfQ44c8iiJEnT\n0ycIjnankRZAkscMW5IkaZr6BME7k/w6o4fLvwr4M+Btw5YlSZqWPoPFvwRcC7wLOB+4vKp+tc/G\nk+xMcleSQ0kuW2H9OUk+kORjSW5N8vwT3QFJ0qnpddM54BOMnkxW3fSaukdaXgU8DzgMHExyoKru\nGGv2H4B3VtVbklzA6CrmHT1rkiStgz5nDb0S+Ajww8ClwE1J/mWPbV8IHKqqu6vqKHANsGtZmwL+\nXjf9jcDn+hYuSVoffXoErwOeXFX3AyT5FuDDwP413ncWcO/Y/GFGVymP+0/Ae5P8NPAYRs9GPk6S\nPcAegHPOOadHyZKkvvoMFt8PfHls/svdsvWwG7i6qrYzepD97yQ5rqaq2ldV81U1Pzc3t04fLUmC\nfj2CQ8DNSd7N6FDOLuDWJK8FqKpfXuV99zG6InnJ9m7ZuFcAO7vt/EWSRwPbgCO990CSdEr69Ag+\nBVxHdx0B8G7g04wuKpt0YdlB4Lwk5ybZCrwYOLCszWeB5wAk+U7g0cBi7+olSaeszzOLf+5kNlxV\nx5LsBW4AtgD7q+r2JFcCC1V1APgZ4G1JXsMoaF7eXbwmSZqSvqePnpSqup5lD7apqsvHpu8Avn/I\nGiRJk3kXUUlqXJ/rCLZNoxBJ0mysGgRJ/lmSReATSQ4necYU65IkTcmkHsF/Bv5JVT0eeBHw89Mp\nSZI0TZOC4FhV/SVAVd2MzyCQpE1p0llDj1u6aGyl+QkXkkmSNpBJQfA2/m4vYPm8JGkTWDUITvZC\nMknSxjLx9NEkz0ryriS3d69rk1w0pdokSVMw6fTRFzC61fQfAy8BXsroKuH9PklMkjaPSWMErwN+\nsKr+19iyjydZAH6VZbeOkCRtTJMODX3rshAAoKpuBf7+cCVJkqZpUhA8cJLrJEkbyKRDQ9+WZPnz\nAwACPHGgeiRJUzYpCJY/aH7cL613IZKk2Zh0HcGHplmIJGk2Jp0+uivJq8fmb05yd/e6tM/Gk+xM\ncleSQ0kuW6XNjya5o7tO4fdPfBckSadi0qGhf8foOcNLHgU8FXgM8FvAtZM2nGQLcBXwPOAwcDDJ\nge6pZEttzgN+Fvj+qvpSksed1F5Ikk7apLOGtlbVvWPz/7Oq7q+qzzIKg7VcCByqqrur6ihwDceP\nO7wKuKqqvgRQVUdOoHZJ0jqYFATfND5TVXvHZud6bPssYDxIDnfLxp0PnJ/kz5PclGTnShtKsifJ\nQpKFxcXFHh8tSeprUhDcnORVyxcm+VfAR9bp808HzgMuAnYDb0vy2OWNqmpfVc1X1fzcXJ8MkiT1\nNWmM4DXAdUleAny0W/YURmMFP9hj2/cBZ4/Nb++WjTsM3FxVXwM+neSTjILhYI/tS5LWwao9gqo6\nUlXPAN4A3NO9rqyqf1RVf9Vj2weB85Kcm2Qro4Hn5ReoXceoN0CSbYwOFd19gvsgSToFk3oEAFTV\n+4H3n+iGq+pYkr3ADcAWYH9V3Z7kSmChqg506/5pkjuAh4DXVdX9J/pZkqSTt2YQnIqqup5ldymt\nqsvHpgt4bfeSJM3AxAfTSJI2P4NAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiD\nQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDVu0CBIsjPJXUkOJblsQrsXJakk80PWI0k63mBBkGQL\ncBVwMXABsDvJBSu0OxP4N8DNQ9UiSVrdkD2CC4FDVXV3VR0FrgF2rdDuDcAbgb8ZsBZJ0iqGDIKz\ngHvH5g93y/5Wku8Dzq6q9wxYhyRpgpkNFic5Dfhl4Gd6tN2TZCHJwuLi4vDFSVJDhgyC+4Czx+a3\nd8uWnAl8N/DBJPcATwcOrDRgXFX7qmq+qubn5uYGLFmS2jNkEBwEzktybpKtwIuBA0srq+qvq2pb\nVe2oqh3ATcAlVbUwYE2SpGUGC4KqOgbsBW4A7gTeWVW3J7kyySVDfa4k6cScPuTGq+p64Pplyy5f\npe1FQ9YiSVqZVxZLUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxB\nIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkho3aBAk2ZnkriSHkly2wvrXJrkjya1J3pfkCUPWI0k6\n3mBBkGQLcBVwMXABsDvJBcuafQyYr6onAdcCvzhUPZKklQ3ZI7gQOFRVd1fVUeAaYNd4g6r6QFU9\n2M3eBGwfsB5J0gqGDIKzgHvH5g93y1bzCuBPVlqRZE+ShSQLi4uL61iiJOkRMVic5MeBeeBNK62v\nqn1VNV9V83Nzc9MtTpI2udMH3PZ9wNlj89u7ZX9HkucC/x54ZlV9dcB6JEkrGLJHcBA4L8m5SbYC\nLwYOjDdI8mTg14FLqurIgLVIklYxWBBU1TFgL3ADcCfwzqq6PcmVSS7pmr0J+AbgD5N8PMmBVTYn\nSRrIkIeGqKrrgeuXLbt8bPq5Q36+JGltj4jBYknS7BgEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgk\nqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNW7QIEiyM8ldSQ4luWyF\n9Y9K8gfd+puT7BiyHknS8QYLgiRbgKuAi4ELgN1JLljW7BXAl6rqHwK/ArxxqHokSSsbskdwIXCo\nqu6uqqPANcCuZW12AW/vpq8FnpMkA9YkSVomVTXMhpNLgZ1V9cpu/mXA06pq71ib27o2h7v5T3Vt\nvrBsW3uAPd3stwN3DVL0+nkKcMusi5iRlvcd2t5/9/2R7QlVNbfSikEfXr9eqmofsG/WdfSVpKpq\nftZ1zELL+w5t77/7vnH3fchDQ/cBZ4/Nb++WrdgmyenANwL3D1iTJGmZIYPgIHBeknOTbAVeDBxY\n1uYA8M+76UuB99dQx6okSSsa7NBQVR1Lshe4AdgC7K+q25NcCSxU1QHgN4HfSXII+CKjsNgMWg6z\nlvcd2t5/932DGmywWJK0MXhlsSQ1ziCQpMYZBOsoSS29Zl3LtCV5cHz/kzw865qmJcmbl+17iz//\nZzW87xv+Z28QrK+HgGOzLmJGjgLHqirAXwBJcnTGNU3Lqxmd8ZZumiSt/R68b9YFzNjnqyrd78CG\nYxCso6o6HWjmm/C4qnpsVZ3RTT+jW7xlhiVNTY08p5t99kyLmYEkDwCh0d/9zWBDXFmsjSXJV7vJ\nj820kClK8kzgg0vz3ZeCVnw9o55wy18sH98dFqqq2nD/DxuuYD2yJXkfsBV4eCNfcn+iqupD3WGB\nvwRI8rUZlzQVSR4CWOoNNuor3c/+C4wOiW64w4JeR7DOum/DWzfqscJTkeSnGN16fEN+K1ovSwPl\nLfwfrDY42uLvP/zt/8eG+/3fUMXqkau7ffhV0MYfwHFJ/irJnd30bzM6Xt7EN6ylAdLuD/9SADYT\nAknek+TDS9Pd4g33s7dHsI5W+Ha04b4ZnKyuO7x8cPjhqtr0A8bdYaDxMYFmfu7jusNEpzUWBF9l\ndCh0yYb82RsEktS4DZdckqT1ZRBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxv1/rVYpWSx2UOcA\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lB5CA4bnJIIa",
        "colab_type": "text"
      },
      "source": [
        "##### As seen in the bar chart above, countries with higher happiness levels enjoy significantly higher GDP per capita than countries with lower levels of happiness. This suggests that the more wealth a country has, the happier it's citizens. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7o9ediJDJIIe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "5e5a3c1a-c00b-42ee-edcd-4b10891e40ef"
      },
      "source": [
        "# Splitting the data into target and feature variables to prepare for one hot encoding and subsequent prediction models\n",
        "\n",
        "X = data.drop([\"Happiness_level\", \"Happiness_ordinal\", \"Country or region\", \"Abbreviation\", \"Continent\"], axis = 1)\n",
        "y = data[\"Happiness_level\"]\n",
        "\n",
        "print (X.shape)\n",
        "print (X.head())\n",
        "print (y.shape)\n",
        "print (y.head())\n",
        "\n",
        "# Train, test, splitting of target and feature variables\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(156, 7)\n",
            "         Subregion  GDPcapita  ...  Generosity  PerceptionsCorruptions\n",
            "0  Northern Europe      1.340  ...       0.153                   0.393\n",
            "1  Northern Europe      1.383  ...       0.252                   0.410\n",
            "2  Northern Europe      1.488  ...       0.271                   0.341\n",
            "3  Northern Europe      1.380  ...       0.354                   0.118\n",
            "4   Western Europe      1.396  ...       0.322                   0.298\n",
            "\n",
            "[5 rows x 7 columns]\n",
            "(156,)\n",
            "0    Very High\n",
            "1    Very High\n",
            "2    Very High\n",
            "3    Very High\n",
            "4    Very High\n",
            "Name: Happiness_level, dtype: object\n",
            "(117, 7)\n",
            "(117,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCdl1EwIJIIw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "6d3c44eb-96c4-4367-adb5-9bc1a08a54b2"
      },
      "source": [
        "# Developing preprocessing pipelines \n",
        "\n",
        "numeric_features=X.columns.tolist()\n",
        "numeric_features.remove('Subregion')\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "categorical_features = ['Subregion']\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "\n",
        "#Fitting preprocessor object\n",
        "prediction_input_preprocessor=preprocessor.fit(X_train) \n",
        "\n",
        "pickle.dump(prediction_input_preprocessor, open( \"preprocessor.pkl\", \"wb\" ) )\n",
        "\n",
        "# Looking at post-processed shapes\n",
        "print (prediction_input_preprocessor.transform(X_train).shape)\n",
        "print (pd.get_dummies(y_train).shape)\n",
        "\n",
        "# Assigning processed X and y train data to objects\n",
        "\n",
        "processed_X_train = prediction_input_preprocessor.transform(X_train)\n",
        "processed_y_train = pd.get_dummies(y_train)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(117, 20)\n",
            "(117, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYFzapW4JIJA",
        "colab_type": "text"
      },
      "source": [
        "### 2. Examine features that predict happiness categories using one or more models that allow for automatic feature selection. Explain any meaningful findings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgvvUt3sJIJF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "604009fb-aa97-45eb-d93d-4d24c290a4a5"
      },
      "source": [
        "# Lasso regression to observe predictive variables with target variable of happiness\n",
        "\n",
        "X_predictive = data.drop([\"Happiness_level\", \"Happiness_ordinal\", \"Country or region\", \"Abbreviation\", \"Continent\", \"Subregion\"], axis = 1)\n",
        "y_predictive = data[\"Happiness_ordinal\"]\n",
        "\n",
        "vari_lasso = Lasso(alpha = 0, normalize = True)\n",
        "vari_lasso_coef = vari_lasso.fit(X_predictive, y_predictive).coef_\n",
        "print(vari_lasso_coef)\n",
        "print(vari_lasso_coef.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.07871879 0.93739894 1.83191933 1.85504487 0.21958188 0.26051596]\n",
            "(6,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TvtmqT-JIJV",
        "colab_type": "text"
      },
      "source": [
        "##### Above, I ran an lasso regression to determine which feature variables are most predictive of the target variable. The features are: GDP per capita, social support, healthy life expectancy, freedom of life choices, generosity, and perceptions of corruption. The target variable is the ordinal ranking of happiness in a range of 1 to 5. 5 corresponds to \"very high\" happiness and 1 corresponds to \"very low\" happiness. According to the lasso regression, the features that are most predictive, in order from greatest to least, are: 1) freedom of life choices, 2) healthy life expectancy, 3) GDP per capita, 4) social support, 5) perceptions of corruption, 6) generosity. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdUIsPbyJIJZ",
        "colab_type": "text"
      },
      "source": [
        "### 3. Run at least three prediction models to try to predict World Happiness well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0APPZETJIJe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "0969b194-483c-4a48-af63-b71d9139e0c9"
      },
      "source": [
        "# KNN classifier with gridsearch and 10 fold CV\n",
        "\n",
        "param_grid = {\"n_neighbors\": np.arange(1, 50)}\n",
        "knnc = KNeighborsClassifier()\n",
        "knnc_gscv = GridSearchCV(knnc, param_grid, cv = 10)\n",
        "knnc_gscv.fit(processed_X_train, processed_y_train)\n",
        "\n",
        "print (\"Best gridsearch hyperparameter value:\", knnc_gscv.best_params_)\n",
        "print (\"KNN classifier gridsearch cross-validation score:\", knnc_gscv.best_score_)\n",
        "print (\"KNN classifier test data accuracy:\", knnc_gscv.score(prediction_input_preprocessor.transform(X_test), pd.get_dummies(y_test)))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best gridsearch hyperparameter value: {'n_neighbors': 1}\n",
            "KNN classifier gridsearch cross-validation score: 0.5204545454545453\n",
            "KNN classifier test data accuracy: 0.3076923076923077\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWviXU-iJIJq",
        "colab_type": "text"
      },
      "source": [
        "##### Above, I ran a KNN classifier model with gridsearch and 10 fold CV. I set the n_neighbors parameters grid range from 1 to 50. After running the model with 10 folds, the best model returned used 1 neighbor as the hyperparameter. The best CV score is .521 and the test-set accuracy is .308."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BctHHuhrJIJv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e4d81994-ded7-49b2-cfda-34ba6c8fd031"
      },
      "source": [
        "# KNN classifier with 3 neighbors and 10 fold cv\n",
        "# (NOTE: I'm an extra KNN model b/c I want to see the scores if I set the n_neighbors to 3)\n",
        "\n",
        "knnc_2 = KNeighborsClassifier(n_neighbors = 3).fit(processed_X_train, processed_y_train)\n",
        "knn_cv_scores = cross_val_score(knnc_2, processed_X_train, processed_y_train, cv =10)\n",
        "print (\"KNN classifier w/3 neighbors mean CV score:\", np.mean(knn_cv_scores))\n",
        "print (\"KNN classifier w/3 neighbors test data accuracy:\", knnc_2.score(prediction_input_preprocessor.transform(X_test), pd.get_dummies(y_test)))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KNN classifier w/3 neighbors mean CV score: 0.4446969696969697\n",
            "KNN classifier w/3 neighbors test data accuracy: 0.38461538461538464\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nf1YrwvAJIKD",
        "colab_type": "text"
      },
      "source": [
        "##### Above, I developed another KNN classifier model, this time relinquishing the gridsearch and setting the neighbors value to 3. I did so because in the previous KNN model, the gridsearch returned the best hyperparameter as 1, and I wanted to see if it would return a higher accuracy with more neighbors. The best CV score decreased to .445 however the test-set accuracy increased to .385. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mgKpYUzJIKO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "d465f536-97b3-41d0-c2bc-5d08334f4bb8"
      },
      "source": [
        "# Random forest classifier with gridsearch and 10 fold CV\n",
        "\n",
        "param_grid_2 = {\"n_estimators\": np.arange(1, 100)}\n",
        "rfc = RandomForestClassifier()\n",
        "rfc_gscv = GridSearchCV(rfc, param_grid_2, cv=10)\n",
        "rfc_gscv.fit(processed_X_train, processed_y_train)\n",
        "\n",
        "print (\"Best gridsearch hyperparameter value:\", rfc_gscv.best_params_)\n",
        "print (\"Random forests classifier gridsearch cross-validation score:\", rfc_gscv.best_score_)\n",
        "print (\"Random forests classifier test data accuracy:\", rfc_gscv.score(prediction_input_preprocessor.transform(X_test), pd.get_dummies(y_test)))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best gridsearch hyperparameter value: {'n_estimators': 7}\n",
            "Random forests classifier gridsearch cross-validation score: 0.48863636363636365\n",
            "Random forests classifier test data accuracy: 0.358974358974359\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt1ugZUVJIKb",
        "colab_type": "text"
      },
      "source": [
        "##### Above, I ran a random forest model with gridsearch and 10 fold CV. I set the n_estimators hyperparameter range from 1 to 100. The best model returned used an n_estimators value of 1. The best CV score is .513 and the test-set accuracy is .282. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APyrOKelJIKg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "039fe22c-171a-48db-e83f-ad44ffa76700"
      },
      "source": [
        "# Logistic regression with gridsearch and 10 fold CV\n",
        "\n",
        "param_grid_3 = {\"C\": np.arange(1, 1000)}\n",
        "logreg = LogisticRegression()\n",
        "log_gscv = GridSearchCV(logreg, param_grid_3, cv=10)\n",
        "log_gscv.fit(processed_X_train, y_train)\n",
        "log_gscv.best_score_\n",
        "\n",
        "print (\"Best gridsearch hyperparameter value:\", log_gscv.best_params_)\n",
        "print (\"Logistic regression gridsearch cross-validation score:\", log_gscv.best_score_)\n",
        "print (\"Logistic regression test data accuracy:\", log_gscv.score(prediction_input_preprocessor.transform(X_test), y_test))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best gridsearch hyperparameter value: {'C': 5}\n",
            "Logistic regression gridsearch cross-validation score: 0.5666666666666668\n",
            "Logistic regression test data accuracy: 0.46153846153846156\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VlqwTZmJIK0",
        "colab_type": "text"
      },
      "source": [
        "##### Above, I ran a logistic regression model with gridsearch and 10 fold CV. I set the C hyperparameter range from 1 to 1000. The best model returned used a C value of 5. The best CV score is .566 and the test-set accuracy is .462."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldUj01ksJIK5",
        "colab_type": "text"
      },
      "source": [
        "##### Of every model I ran, the logistic regression with a hyperparameter of 1 and 10 folds returned the highest accuracy. I has a CV score of .566 and test-set accuracy of .462. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFE14uxUJIK9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prediction of target data using prior logistic regression model\n",
        "\n",
        "y_log_gscv_pred=log_gscv.predict(prediction_input_preprocessor.transform(X_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJtmPA7xNIW9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "outputId": "d07aab5c-b75b-482b-8c4b-b137eba03acc"
      },
      "source": [
        "# Model evaluation for logistic regression results\n",
        "\n",
        "model_eval_metrics(y_test,y_log_gscv_pred,classification=\"TRUE\")"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>f1_score</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>mse</th>\n",
              "      <th>rmse</th>\n",
              "      <th>mae</th>\n",
              "      <th>r2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.435897</td>\n",
              "      <td>0.431814</td>\n",
              "      <td>0.496923</td>\n",
              "      <td>0.463889</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   accuracy  f1_score  precision    recall  mse  rmse  mae  r2\n",
              "0  0.435897  0.431814   0.496923  0.463889    0     0    0   0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b6IQeEY7vM1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2d4be356-d5b5-469f-a9f4-f43498abd085"
      },
      "source": [
        "# Vanilla neural network\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "import keras\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "nn = Sequential()\n",
        "nn.add(Dense(124, input_dim=20, activation='relu'))\n",
        "nn.add(Dense(124, activation='relu'))\n",
        "nn.add(Dense(124, activation='relu'))\n",
        "\n",
        "nn.add(Dense(5, activation='softmax')) \n",
        "                                            \n",
        "nn.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "\n",
        "nn.fit(processed_X_train, processed_y_train, \n",
        "               batch_size = 60, \n",
        "               epochs = 500, validation_split=0.2) "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 93 samples, validate on 24 samples\n",
            "Epoch 1/500\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 1.6029 - acc: 0.2581 - val_loss: 1.6023 - val_acc: 0.2083\n",
            "Epoch 2/500\n",
            "93/93 [==============================] - 0s 195us/step - loss: 1.5949 - acc: 0.2796 - val_loss: 1.5965 - val_acc: 0.2083\n",
            "Epoch 3/500\n",
            "93/93 [==============================] - 0s 143us/step - loss: 1.5875 - acc: 0.3011 - val_loss: 1.5908 - val_acc: 0.2917\n",
            "Epoch 4/500\n",
            "93/93 [==============================] - 0s 175us/step - loss: 1.5798 - acc: 0.3548 - val_loss: 1.5849 - val_acc: 0.2917\n",
            "Epoch 5/500\n",
            "93/93 [==============================] - 0s 129us/step - loss: 1.5723 - acc: 0.3763 - val_loss: 1.5793 - val_acc: 0.3333\n",
            "Epoch 6/500\n",
            "93/93 [==============================] - 0s 168us/step - loss: 1.5652 - acc: 0.3871 - val_loss: 1.5737 - val_acc: 0.3333\n",
            "Epoch 7/500\n",
            "93/93 [==============================] - 0s 162us/step - loss: 1.5583 - acc: 0.4086 - val_loss: 1.5679 - val_acc: 0.3333\n",
            "Epoch 8/500\n",
            "93/93 [==============================] - 0s 137us/step - loss: 1.5511 - acc: 0.4409 - val_loss: 1.5620 - val_acc: 0.3750\n",
            "Epoch 9/500\n",
            "93/93 [==============================] - 0s 145us/step - loss: 1.5444 - acc: 0.4516 - val_loss: 1.5567 - val_acc: 0.3750\n",
            "Epoch 10/500\n",
            "93/93 [==============================] - 0s 125us/step - loss: 1.5372 - acc: 0.4516 - val_loss: 1.5506 - val_acc: 0.3750\n",
            "Epoch 11/500\n",
            "93/93 [==============================] - 0s 160us/step - loss: 1.5301 - acc: 0.4516 - val_loss: 1.5452 - val_acc: 0.3750\n",
            "Epoch 12/500\n",
            "93/93 [==============================] - 0s 157us/step - loss: 1.5238 - acc: 0.4516 - val_loss: 1.5396 - val_acc: 0.3750\n",
            "Epoch 13/500\n",
            "93/93 [==============================] - 0s 154us/step - loss: 1.5166 - acc: 0.4516 - val_loss: 1.5343 - val_acc: 0.3750\n",
            "Epoch 14/500\n",
            "93/93 [==============================] - 0s 149us/step - loss: 1.5099 - acc: 0.4409 - val_loss: 1.5288 - val_acc: 0.3750\n",
            "Epoch 15/500\n",
            "93/93 [==============================] - 0s 169us/step - loss: 1.5033 - acc: 0.4516 - val_loss: 1.5233 - val_acc: 0.3750\n",
            "Epoch 16/500\n",
            "93/93 [==============================] - 0s 191us/step - loss: 1.4967 - acc: 0.4409 - val_loss: 1.5178 - val_acc: 0.3750\n",
            "Epoch 17/500\n",
            "93/93 [==============================] - 0s 175us/step - loss: 1.4905 - acc: 0.4516 - val_loss: 1.5122 - val_acc: 0.4167\n",
            "Epoch 18/500\n",
            "93/93 [==============================] - 0s 137us/step - loss: 1.4835 - acc: 0.4516 - val_loss: 1.5066 - val_acc: 0.4167\n",
            "Epoch 19/500\n",
            "93/93 [==============================] - 0s 184us/step - loss: 1.4768 - acc: 0.4409 - val_loss: 1.5011 - val_acc: 0.4167\n",
            "Epoch 20/500\n",
            "93/93 [==============================] - 0s 174us/step - loss: 1.4704 - acc: 0.4409 - val_loss: 1.4950 - val_acc: 0.4167\n",
            "Epoch 21/500\n",
            "93/93 [==============================] - 0s 166us/step - loss: 1.4632 - acc: 0.4516 - val_loss: 1.4892 - val_acc: 0.4167\n",
            "Epoch 22/500\n",
            "93/93 [==============================] - 0s 132us/step - loss: 1.4563 - acc: 0.4516 - val_loss: 1.4835 - val_acc: 0.4167\n",
            "Epoch 23/500\n",
            "93/93 [==============================] - 0s 119us/step - loss: 1.4498 - acc: 0.4516 - val_loss: 1.4781 - val_acc: 0.4167\n",
            "Epoch 24/500\n",
            "93/93 [==============================] - 0s 126us/step - loss: 1.4429 - acc: 0.4516 - val_loss: 1.4724 - val_acc: 0.4167\n",
            "Epoch 25/500\n",
            "93/93 [==============================] - 0s 129us/step - loss: 1.4361 - acc: 0.4516 - val_loss: 1.4673 - val_acc: 0.4167\n",
            "Epoch 26/500\n",
            "93/93 [==============================] - 0s 137us/step - loss: 1.4296 - acc: 0.4731 - val_loss: 1.4621 - val_acc: 0.4583\n",
            "Epoch 27/500\n",
            "93/93 [==============================] - 0s 121us/step - loss: 1.4231 - acc: 0.4731 - val_loss: 1.4568 - val_acc: 0.4583\n",
            "Epoch 28/500\n",
            "93/93 [==============================] - 0s 130us/step - loss: 1.4166 - acc: 0.4731 - val_loss: 1.4513 - val_acc: 0.4583\n",
            "Epoch 29/500\n",
            "93/93 [==============================] - 0s 113us/step - loss: 1.4100 - acc: 0.4624 - val_loss: 1.4461 - val_acc: 0.4583\n",
            "Epoch 30/500\n",
            "93/93 [==============================] - 0s 135us/step - loss: 1.4037 - acc: 0.4624 - val_loss: 1.4405 - val_acc: 0.4583\n",
            "Epoch 31/500\n",
            "93/93 [==============================] - 0s 134us/step - loss: 1.3973 - acc: 0.4624 - val_loss: 1.4351 - val_acc: 0.4583\n",
            "Epoch 32/500\n",
            "93/93 [==============================] - 0s 129us/step - loss: 1.3911 - acc: 0.4731 - val_loss: 1.4296 - val_acc: 0.4583\n",
            "Epoch 33/500\n",
            "93/93 [==============================] - 0s 128us/step - loss: 1.3846 - acc: 0.4731 - val_loss: 1.4245 - val_acc: 0.4583\n",
            "Epoch 34/500\n",
            "93/93 [==============================] - 0s 120us/step - loss: 1.3783 - acc: 0.4731 - val_loss: 1.4189 - val_acc: 0.4583\n",
            "Epoch 35/500\n",
            "93/93 [==============================] - 0s 123us/step - loss: 1.3721 - acc: 0.4731 - val_loss: 1.4139 - val_acc: 0.4583\n",
            "Epoch 36/500\n",
            "93/93 [==============================] - 0s 125us/step - loss: 1.3660 - acc: 0.4839 - val_loss: 1.4089 - val_acc: 0.4583\n",
            "Epoch 37/500\n",
            "93/93 [==============================] - 0s 120us/step - loss: 1.3599 - acc: 0.4839 - val_loss: 1.4041 - val_acc: 0.4583\n",
            "Epoch 38/500\n",
            "93/93 [==============================] - 0s 138us/step - loss: 1.3540 - acc: 0.4946 - val_loss: 1.3988 - val_acc: 0.4583\n",
            "Epoch 39/500\n",
            "93/93 [==============================] - 0s 132us/step - loss: 1.3478 - acc: 0.4946 - val_loss: 1.3940 - val_acc: 0.4583\n",
            "Epoch 40/500\n",
            "93/93 [==============================] - 0s 179us/step - loss: 1.3421 - acc: 0.4839 - val_loss: 1.3890 - val_acc: 0.4583\n",
            "Epoch 41/500\n",
            "93/93 [==============================] - 0s 140us/step - loss: 1.3361 - acc: 0.4839 - val_loss: 1.3840 - val_acc: 0.4583\n",
            "Epoch 42/500\n",
            "93/93 [==============================] - 0s 123us/step - loss: 1.3304 - acc: 0.4946 - val_loss: 1.3791 - val_acc: 0.4583\n",
            "Epoch 43/500\n",
            "93/93 [==============================] - 0s 131us/step - loss: 1.3248 - acc: 0.4946 - val_loss: 1.3739 - val_acc: 0.5000\n",
            "Epoch 44/500\n",
            "93/93 [==============================] - 0s 134us/step - loss: 1.3193 - acc: 0.5054 - val_loss: 1.3693 - val_acc: 0.5000\n",
            "Epoch 45/500\n",
            "93/93 [==============================] - 0s 136us/step - loss: 1.3134 - acc: 0.5161 - val_loss: 1.3642 - val_acc: 0.5000\n",
            "Epoch 46/500\n",
            "93/93 [==============================] - 0s 138us/step - loss: 1.3077 - acc: 0.5161 - val_loss: 1.3592 - val_acc: 0.5000\n",
            "Epoch 47/500\n",
            "93/93 [==============================] - 0s 134us/step - loss: 1.3021 - acc: 0.5161 - val_loss: 1.3544 - val_acc: 0.4583\n",
            "Epoch 48/500\n",
            "93/93 [==============================] - 0s 136us/step - loss: 1.2966 - acc: 0.5161 - val_loss: 1.3499 - val_acc: 0.4583\n",
            "Epoch 49/500\n",
            "93/93 [==============================] - 0s 156us/step - loss: 1.2913 - acc: 0.5161 - val_loss: 1.3447 - val_acc: 0.4583\n",
            "Epoch 50/500\n",
            "93/93 [==============================] - 0s 156us/step - loss: 1.2858 - acc: 0.5161 - val_loss: 1.3403 - val_acc: 0.4583\n",
            "Epoch 51/500\n",
            "93/93 [==============================] - 0s 134us/step - loss: 1.2805 - acc: 0.5161 - val_loss: 1.3358 - val_acc: 0.4583\n",
            "Epoch 52/500\n",
            "93/93 [==============================] - 0s 145us/step - loss: 1.2753 - acc: 0.5161 - val_loss: 1.3316 - val_acc: 0.4583\n",
            "Epoch 53/500\n",
            "93/93 [==============================] - 0s 145us/step - loss: 1.2702 - acc: 0.5161 - val_loss: 1.3271 - val_acc: 0.4583\n",
            "Epoch 54/500\n",
            "93/93 [==============================] - 0s 136us/step - loss: 1.2654 - acc: 0.5161 - val_loss: 1.3227 - val_acc: 0.4583\n",
            "Epoch 55/500\n",
            "93/93 [==============================] - 0s 134us/step - loss: 1.2602 - acc: 0.5161 - val_loss: 1.3182 - val_acc: 0.4583\n",
            "Epoch 56/500\n",
            "93/93 [==============================] - 0s 133us/step - loss: 1.2551 - acc: 0.5161 - val_loss: 1.3135 - val_acc: 0.4583\n",
            "Epoch 57/500\n",
            "93/93 [==============================] - 0s 130us/step - loss: 1.2501 - acc: 0.5161 - val_loss: 1.3090 - val_acc: 0.4583\n",
            "Epoch 58/500\n",
            "93/93 [==============================] - 0s 123us/step - loss: 1.2453 - acc: 0.5161 - val_loss: 1.3046 - val_acc: 0.4583\n",
            "Epoch 59/500\n",
            "93/93 [==============================] - 0s 147us/step - loss: 1.2408 - acc: 0.5161 - val_loss: 1.3006 - val_acc: 0.4583\n",
            "Epoch 60/500\n",
            "93/93 [==============================] - 0s 211us/step - loss: 1.2356 - acc: 0.5161 - val_loss: 1.2962 - val_acc: 0.4167\n",
            "Epoch 61/500\n",
            "93/93 [==============================] - 0s 121us/step - loss: 1.2307 - acc: 0.5269 - val_loss: 1.2921 - val_acc: 0.4167\n",
            "Epoch 62/500\n",
            "93/93 [==============================] - 0s 141us/step - loss: 1.2263 - acc: 0.5161 - val_loss: 1.2878 - val_acc: 0.4583\n",
            "Epoch 63/500\n",
            "93/93 [==============================] - 0s 149us/step - loss: 1.2214 - acc: 0.5269 - val_loss: 1.2838 - val_acc: 0.4583\n",
            "Epoch 64/500\n",
            "93/93 [==============================] - 0s 149us/step - loss: 1.2168 - acc: 0.5269 - val_loss: 1.2795 - val_acc: 0.4583\n",
            "Epoch 65/500\n",
            "93/93 [==============================] - 0s 136us/step - loss: 1.2125 - acc: 0.5269 - val_loss: 1.2755 - val_acc: 0.4583\n",
            "Epoch 66/500\n",
            "93/93 [==============================] - 0s 150us/step - loss: 1.2075 - acc: 0.5269 - val_loss: 1.2712 - val_acc: 0.4583\n",
            "Epoch 67/500\n",
            "93/93 [==============================] - 0s 146us/step - loss: 1.2030 - acc: 0.5269 - val_loss: 1.2674 - val_acc: 0.4583\n",
            "Epoch 68/500\n",
            "93/93 [==============================] - 0s 138us/step - loss: 1.1994 - acc: 0.5269 - val_loss: 1.2638 - val_acc: 0.4583\n",
            "Epoch 69/500\n",
            "93/93 [==============================] - 0s 144us/step - loss: 1.1946 - acc: 0.5376 - val_loss: 1.2594 - val_acc: 0.4583\n",
            "Epoch 70/500\n",
            "93/93 [==============================] - 0s 132us/step - loss: 1.1904 - acc: 0.5376 - val_loss: 1.2555 - val_acc: 0.4583\n",
            "Epoch 71/500\n",
            "93/93 [==============================] - 0s 151us/step - loss: 1.1862 - acc: 0.5376 - val_loss: 1.2516 - val_acc: 0.4583\n",
            "Epoch 72/500\n",
            "93/93 [==============================] - 0s 151us/step - loss: 1.1815 - acc: 0.5376 - val_loss: 1.2475 - val_acc: 0.4583\n",
            "Epoch 73/500\n",
            "93/93 [==============================] - 0s 138us/step - loss: 1.1774 - acc: 0.5376 - val_loss: 1.2436 - val_acc: 0.4583\n",
            "Epoch 74/500\n",
            "93/93 [==============================] - 0s 146us/step - loss: 1.1732 - acc: 0.5376 - val_loss: 1.2397 - val_acc: 0.4583\n",
            "Epoch 75/500\n",
            "93/93 [==============================] - 0s 115us/step - loss: 1.1690 - acc: 0.5376 - val_loss: 1.2358 - val_acc: 0.4583\n",
            "Epoch 76/500\n",
            "93/93 [==============================] - 0s 151us/step - loss: 1.1650 - acc: 0.5484 - val_loss: 1.2321 - val_acc: 0.4583\n",
            "Epoch 77/500\n",
            "93/93 [==============================] - 0s 158us/step - loss: 1.1612 - acc: 0.5484 - val_loss: 1.2284 - val_acc: 0.4583\n",
            "Epoch 78/500\n",
            "93/93 [==============================] - 0s 138us/step - loss: 1.1572 - acc: 0.5591 - val_loss: 1.2249 - val_acc: 0.4583\n",
            "Epoch 79/500\n",
            "93/93 [==============================] - 0s 131us/step - loss: 1.1531 - acc: 0.5591 - val_loss: 1.2214 - val_acc: 0.4583\n",
            "Epoch 80/500\n",
            "93/93 [==============================] - 0s 139us/step - loss: 1.1492 - acc: 0.5699 - val_loss: 1.2178 - val_acc: 0.4583\n",
            "Epoch 81/500\n",
            "93/93 [==============================] - 0s 143us/step - loss: 1.1456 - acc: 0.5699 - val_loss: 1.2141 - val_acc: 0.4583\n",
            "Epoch 82/500\n",
            "93/93 [==============================] - 0s 147us/step - loss: 1.1419 - acc: 0.5591 - val_loss: 1.2108 - val_acc: 0.5000\n",
            "Epoch 83/500\n",
            "93/93 [==============================] - 0s 156us/step - loss: 1.1381 - acc: 0.5699 - val_loss: 1.2073 - val_acc: 0.5000\n",
            "Epoch 84/500\n",
            "93/93 [==============================] - 0s 135us/step - loss: 1.1346 - acc: 0.5806 - val_loss: 1.2041 - val_acc: 0.5000\n",
            "Epoch 85/500\n",
            "93/93 [==============================] - 0s 119us/step - loss: 1.1308 - acc: 0.5806 - val_loss: 1.2005 - val_acc: 0.5000\n",
            "Epoch 86/500\n",
            "93/93 [==============================] - 0s 139us/step - loss: 1.1269 - acc: 0.5914 - val_loss: 1.1971 - val_acc: 0.5000\n",
            "Epoch 87/500\n",
            "93/93 [==============================] - 0s 150us/step - loss: 1.1236 - acc: 0.5806 - val_loss: 1.1938 - val_acc: 0.5000\n",
            "Epoch 88/500\n",
            "93/93 [==============================] - 0s 175us/step - loss: 1.1203 - acc: 0.5914 - val_loss: 1.1910 - val_acc: 0.5000\n",
            "Epoch 89/500\n",
            "93/93 [==============================] - 0s 122us/step - loss: 1.1161 - acc: 0.6022 - val_loss: 1.1877 - val_acc: 0.5000\n",
            "Epoch 90/500\n",
            "93/93 [==============================] - 0s 145us/step - loss: 1.1128 - acc: 0.5914 - val_loss: 1.1843 - val_acc: 0.5000\n",
            "Epoch 91/500\n",
            "93/93 [==============================] - 0s 159us/step - loss: 1.1093 - acc: 0.5914 - val_loss: 1.1813 - val_acc: 0.5000\n",
            "Epoch 92/500\n",
            "93/93 [==============================] - 0s 160us/step - loss: 1.1061 - acc: 0.6022 - val_loss: 1.1787 - val_acc: 0.5000\n",
            "Epoch 93/500\n",
            "93/93 [==============================] - 0s 156us/step - loss: 1.1026 - acc: 0.6022 - val_loss: 1.1759 - val_acc: 0.5000\n",
            "Epoch 94/500\n",
            "93/93 [==============================] - 0s 161us/step - loss: 1.0992 - acc: 0.6022 - val_loss: 1.1729 - val_acc: 0.5000\n",
            "Epoch 95/500\n",
            "93/93 [==============================] - 0s 171us/step - loss: 1.0959 - acc: 0.6022 - val_loss: 1.1697 - val_acc: 0.5000\n",
            "Epoch 96/500\n",
            "93/93 [==============================] - 0s 150us/step - loss: 1.0924 - acc: 0.6129 - val_loss: 1.1666 - val_acc: 0.5000\n",
            "Epoch 97/500\n",
            "93/93 [==============================] - 0s 175us/step - loss: 1.0890 - acc: 0.6129 - val_loss: 1.1634 - val_acc: 0.5000\n",
            "Epoch 98/500\n",
            "93/93 [==============================] - 0s 169us/step - loss: 1.0858 - acc: 0.6022 - val_loss: 1.1606 - val_acc: 0.5000\n",
            "Epoch 99/500\n",
            "93/93 [==============================] - 0s 193us/step - loss: 1.0825 - acc: 0.6237 - val_loss: 1.1578 - val_acc: 0.5000\n",
            "Epoch 100/500\n",
            "93/93 [==============================] - 0s 170us/step - loss: 1.0794 - acc: 0.6237 - val_loss: 1.1548 - val_acc: 0.5000\n",
            "Epoch 101/500\n",
            "93/93 [==============================] - 0s 150us/step - loss: 1.0763 - acc: 0.6237 - val_loss: 1.1521 - val_acc: 0.5000\n",
            "Epoch 102/500\n",
            "93/93 [==============================] - 0s 153us/step - loss: 1.0731 - acc: 0.6129 - val_loss: 1.1494 - val_acc: 0.5000\n",
            "Epoch 103/500\n",
            "93/93 [==============================] - 0s 148us/step - loss: 1.0702 - acc: 0.6129 - val_loss: 1.1466 - val_acc: 0.5000\n",
            "Epoch 104/500\n",
            "93/93 [==============================] - 0s 176us/step - loss: 1.0674 - acc: 0.6237 - val_loss: 1.1445 - val_acc: 0.5000\n",
            "Epoch 105/500\n",
            "93/93 [==============================] - 0s 195us/step - loss: 1.0639 - acc: 0.6129 - val_loss: 1.1417 - val_acc: 0.5000\n",
            "Epoch 106/500\n",
            "93/93 [==============================] - 0s 179us/step - loss: 1.0609 - acc: 0.6129 - val_loss: 1.1389 - val_acc: 0.5000\n",
            "Epoch 107/500\n",
            "93/93 [==============================] - 0s 182us/step - loss: 1.0578 - acc: 0.6129 - val_loss: 1.1362 - val_acc: 0.5000\n",
            "Epoch 108/500\n",
            "93/93 [==============================] - 0s 185us/step - loss: 1.0554 - acc: 0.6129 - val_loss: 1.1339 - val_acc: 0.5000\n",
            "Epoch 109/500\n",
            "93/93 [==============================] - 0s 166us/step - loss: 1.0524 - acc: 0.6237 - val_loss: 1.1313 - val_acc: 0.5000\n",
            "Epoch 110/500\n",
            "93/93 [==============================] - 0s 159us/step - loss: 1.0497 - acc: 0.6237 - val_loss: 1.1290 - val_acc: 0.5000\n",
            "Epoch 111/500\n",
            "93/93 [==============================] - 0s 168us/step - loss: 1.0469 - acc: 0.6237 - val_loss: 1.1267 - val_acc: 0.5000\n",
            "Epoch 112/500\n",
            "93/93 [==============================] - 0s 171us/step - loss: 1.0439 - acc: 0.6344 - val_loss: 1.1243 - val_acc: 0.5000\n",
            "Epoch 113/500\n",
            "93/93 [==============================] - 0s 160us/step - loss: 1.0408 - acc: 0.6344 - val_loss: 1.1217 - val_acc: 0.5000\n",
            "Epoch 114/500\n",
            "93/93 [==============================] - 0s 156us/step - loss: 1.0379 - acc: 0.6344 - val_loss: 1.1192 - val_acc: 0.5000\n",
            "Epoch 115/500\n",
            "93/93 [==============================] - 0s 164us/step - loss: 1.0353 - acc: 0.6344 - val_loss: 1.1165 - val_acc: 0.5000\n",
            "Epoch 116/500\n",
            "93/93 [==============================] - 0s 167us/step - loss: 1.0325 - acc: 0.6344 - val_loss: 1.1141 - val_acc: 0.5000\n",
            "Epoch 117/500\n",
            "93/93 [==============================] - 0s 167us/step - loss: 1.0299 - acc: 0.6452 - val_loss: 1.1113 - val_acc: 0.5000\n",
            "Epoch 118/500\n",
            "93/93 [==============================] - 0s 165us/step - loss: 1.0273 - acc: 0.6452 - val_loss: 1.1085 - val_acc: 0.5000\n",
            "Epoch 119/500\n",
            "93/93 [==============================] - 0s 170us/step - loss: 1.0243 - acc: 0.6452 - val_loss: 1.1060 - val_acc: 0.5000\n",
            "Epoch 120/500\n",
            "93/93 [==============================] - 0s 166us/step - loss: 1.0216 - acc: 0.6452 - val_loss: 1.1032 - val_acc: 0.5000\n",
            "Epoch 121/500\n",
            "93/93 [==============================] - 0s 166us/step - loss: 1.0186 - acc: 0.6452 - val_loss: 1.1005 - val_acc: 0.5000\n",
            "Epoch 122/500\n",
            "93/93 [==============================] - 0s 173us/step - loss: 1.0160 - acc: 0.6452 - val_loss: 1.0983 - val_acc: 0.5000\n",
            "Epoch 123/500\n",
            "93/93 [==============================] - 0s 174us/step - loss: 1.0134 - acc: 0.6452 - val_loss: 1.0960 - val_acc: 0.5000\n",
            "Epoch 124/500\n",
            "93/93 [==============================] - 0s 165us/step - loss: 1.0111 - acc: 0.6452 - val_loss: 1.0938 - val_acc: 0.5000\n",
            "Epoch 125/500\n",
            "93/93 [==============================] - 0s 166us/step - loss: 1.0088 - acc: 0.6452 - val_loss: 1.0918 - val_acc: 0.5000\n",
            "Epoch 126/500\n",
            "93/93 [==============================] - 0s 118us/step - loss: 1.0059 - acc: 0.6452 - val_loss: 1.0894 - val_acc: 0.5000\n",
            "Epoch 127/500\n",
            "93/93 [==============================] - 0s 298us/step - loss: 1.0033 - acc: 0.6452 - val_loss: 1.0870 - val_acc: 0.5000\n",
            "Epoch 128/500\n",
            "93/93 [==============================] - 0s 148us/step - loss: 1.0006 - acc: 0.6559 - val_loss: 1.0848 - val_acc: 0.5000\n",
            "Epoch 129/500\n",
            "93/93 [==============================] - 0s 119us/step - loss: 0.9980 - acc: 0.6559 - val_loss: 1.0830 - val_acc: 0.4583\n",
            "Epoch 130/500\n",
            "93/93 [==============================] - 0s 134us/step - loss: 0.9958 - acc: 0.6559 - val_loss: 1.0812 - val_acc: 0.4583\n",
            "Epoch 131/500\n",
            "93/93 [==============================] - 0s 142us/step - loss: 0.9931 - acc: 0.6559 - val_loss: 1.0791 - val_acc: 0.4583\n",
            "Epoch 132/500\n",
            "93/93 [==============================] - 0s 145us/step - loss: 0.9906 - acc: 0.6559 - val_loss: 1.0770 - val_acc: 0.4583\n",
            "Epoch 133/500\n",
            "93/93 [==============================] - 0s 191us/step - loss: 0.9882 - acc: 0.6559 - val_loss: 1.0747 - val_acc: 0.4583\n",
            "Epoch 134/500\n",
            "93/93 [==============================] - 0s 176us/step - loss: 0.9856 - acc: 0.6559 - val_loss: 1.0725 - val_acc: 0.4583\n",
            "Epoch 135/500\n",
            "93/93 [==============================] - 0s 177us/step - loss: 0.9832 - acc: 0.6559 - val_loss: 1.0704 - val_acc: 0.4583\n",
            "Epoch 136/500\n",
            "93/93 [==============================] - 0s 183us/step - loss: 0.9810 - acc: 0.6667 - val_loss: 1.0683 - val_acc: 0.4583\n",
            "Epoch 137/500\n",
            "93/93 [==============================] - 0s 182us/step - loss: 0.9784 - acc: 0.6559 - val_loss: 1.0661 - val_acc: 0.4583\n",
            "Epoch 138/500\n",
            "93/93 [==============================] - 0s 132us/step - loss: 0.9761 - acc: 0.6667 - val_loss: 1.0637 - val_acc: 0.4583\n",
            "Epoch 139/500\n",
            "93/93 [==============================] - 0s 151us/step - loss: 0.9737 - acc: 0.6667 - val_loss: 1.0620 - val_acc: 0.4583\n",
            "Epoch 140/500\n",
            "93/93 [==============================] - 0s 142us/step - loss: 0.9712 - acc: 0.6667 - val_loss: 1.0598 - val_acc: 0.4583\n",
            "Epoch 141/500\n",
            "93/93 [==============================] - 0s 178us/step - loss: 0.9688 - acc: 0.6667 - val_loss: 1.0580 - val_acc: 0.4583\n",
            "Epoch 142/500\n",
            "93/93 [==============================] - 0s 138us/step - loss: 0.9666 - acc: 0.6667 - val_loss: 1.0559 - val_acc: 0.4583\n",
            "Epoch 143/500\n",
            "93/93 [==============================] - 0s 156us/step - loss: 0.9639 - acc: 0.6774 - val_loss: 1.0540 - val_acc: 0.4583\n",
            "Epoch 144/500\n",
            "93/93 [==============================] - 0s 156us/step - loss: 0.9621 - acc: 0.6774 - val_loss: 1.0524 - val_acc: 0.5000\n",
            "Epoch 145/500\n",
            "93/93 [==============================] - 0s 156us/step - loss: 0.9594 - acc: 0.6774 - val_loss: 1.0507 - val_acc: 0.5000\n",
            "Epoch 146/500\n",
            "93/93 [==============================] - 0s 120us/step - loss: 0.9572 - acc: 0.6667 - val_loss: 1.0487 - val_acc: 0.5000\n",
            "Epoch 147/500\n",
            "93/93 [==============================] - 0s 156us/step - loss: 0.9550 - acc: 0.6667 - val_loss: 1.0469 - val_acc: 0.5000\n",
            "Epoch 148/500\n",
            "93/93 [==============================] - 0s 222us/step - loss: 0.9525 - acc: 0.6667 - val_loss: 1.0448 - val_acc: 0.5417\n",
            "Epoch 149/500\n",
            "93/93 [==============================] - 0s 185us/step - loss: 0.9505 - acc: 0.6667 - val_loss: 1.0431 - val_acc: 0.5417\n",
            "Epoch 150/500\n",
            "93/93 [==============================] - 0s 145us/step - loss: 0.9482 - acc: 0.6667 - val_loss: 1.0414 - val_acc: 0.5417\n",
            "Epoch 151/500\n",
            "93/93 [==============================] - 0s 173us/step - loss: 0.9461 - acc: 0.6667 - val_loss: 1.0401 - val_acc: 0.5417\n",
            "Epoch 152/500\n",
            "93/93 [==============================] - 0s 152us/step - loss: 0.9441 - acc: 0.6667 - val_loss: 1.0378 - val_acc: 0.5417\n",
            "Epoch 153/500\n",
            "93/93 [==============================] - 0s 176us/step - loss: 0.9416 - acc: 0.6667 - val_loss: 1.0361 - val_acc: 0.5417\n",
            "Epoch 154/500\n",
            "93/93 [==============================] - 0s 165us/step - loss: 0.9399 - acc: 0.6667 - val_loss: 1.0342 - val_acc: 0.5417\n",
            "Epoch 155/500\n",
            "93/93 [==============================] - 0s 161us/step - loss: 0.9379 - acc: 0.6667 - val_loss: 1.0325 - val_acc: 0.5417\n",
            "Epoch 156/500\n",
            "93/93 [==============================] - 0s 126us/step - loss: 0.9357 - acc: 0.6667 - val_loss: 1.0315 - val_acc: 0.5417\n",
            "Epoch 157/500\n",
            "93/93 [==============================] - 0s 126us/step - loss: 0.9335 - acc: 0.6667 - val_loss: 1.0294 - val_acc: 0.5417\n",
            "Epoch 158/500\n",
            "93/93 [==============================] - 0s 147us/step - loss: 0.9315 - acc: 0.6667 - val_loss: 1.0277 - val_acc: 0.5417\n",
            "Epoch 159/500\n",
            "93/93 [==============================] - 0s 98us/step - loss: 0.9294 - acc: 0.6667 - val_loss: 1.0264 - val_acc: 0.5417\n",
            "Epoch 160/500\n",
            "93/93 [==============================] - 0s 223us/step - loss: 0.9273 - acc: 0.6667 - val_loss: 1.0249 - val_acc: 0.5417\n",
            "Epoch 161/500\n",
            "93/93 [==============================] - 0s 171us/step - loss: 0.9254 - acc: 0.6667 - val_loss: 1.0235 - val_acc: 0.5833\n",
            "Epoch 162/500\n",
            "93/93 [==============================] - 0s 150us/step - loss: 0.9232 - acc: 0.6667 - val_loss: 1.0221 - val_acc: 0.5833\n",
            "Epoch 163/500\n",
            "93/93 [==============================] - 0s 149us/step - loss: 0.9212 - acc: 0.6667 - val_loss: 1.0200 - val_acc: 0.5833\n",
            "Epoch 164/500\n",
            "93/93 [==============================] - 0s 146us/step - loss: 0.9191 - acc: 0.6667 - val_loss: 1.0186 - val_acc: 0.5833\n",
            "Epoch 165/500\n",
            "93/93 [==============================] - 0s 103us/step - loss: 0.9177 - acc: 0.6667 - val_loss: 1.0172 - val_acc: 0.6250\n",
            "Epoch 166/500\n",
            "93/93 [==============================] - 0s 132us/step - loss: 0.9152 - acc: 0.6667 - val_loss: 1.0154 - val_acc: 0.6250\n",
            "Epoch 167/500\n",
            "93/93 [==============================] - 0s 142us/step - loss: 0.9137 - acc: 0.6667 - val_loss: 1.0146 - val_acc: 0.6250\n",
            "Epoch 168/500\n",
            "93/93 [==============================] - 0s 160us/step - loss: 0.9119 - acc: 0.6667 - val_loss: 1.0129 - val_acc: 0.6250\n",
            "Epoch 169/500\n",
            "93/93 [==============================] - 0s 138us/step - loss: 0.9095 - acc: 0.6667 - val_loss: 1.0115 - val_acc: 0.6250\n",
            "Epoch 170/500\n",
            "93/93 [==============================] - 0s 207us/step - loss: 0.9076 - acc: 0.6667 - val_loss: 1.0104 - val_acc: 0.6250\n",
            "Epoch 171/500\n",
            "93/93 [==============================] - 0s 214us/step - loss: 0.9057 - acc: 0.6667 - val_loss: 1.0090 - val_acc: 0.6250\n",
            "Epoch 172/500\n",
            "93/93 [==============================] - 0s 197us/step - loss: 0.9048 - acc: 0.6667 - val_loss: 1.0085 - val_acc: 0.6250\n",
            "Epoch 173/500\n",
            "93/93 [==============================] - 0s 197us/step - loss: 0.9023 - acc: 0.6667 - val_loss: 1.0070 - val_acc: 0.6250\n",
            "Epoch 174/500\n",
            "93/93 [==============================] - 0s 211us/step - loss: 0.9003 - acc: 0.6667 - val_loss: 1.0054 - val_acc: 0.6250\n",
            "Epoch 175/500\n",
            "93/93 [==============================] - 0s 196us/step - loss: 0.8994 - acc: 0.6667 - val_loss: 1.0048 - val_acc: 0.6250\n",
            "Epoch 176/500\n",
            "93/93 [==============================] - 0s 165us/step - loss: 0.8974 - acc: 0.6774 - val_loss: 1.0025 - val_acc: 0.6250\n",
            "Epoch 177/500\n",
            "93/93 [==============================] - 0s 96us/step - loss: 0.8949 - acc: 0.6667 - val_loss: 1.0010 - val_acc: 0.6250\n",
            "Epoch 178/500\n",
            "93/93 [==============================] - 0s 116us/step - loss: 0.8933 - acc: 0.6667 - val_loss: 0.9996 - val_acc: 0.6250\n",
            "Epoch 179/500\n",
            "93/93 [==============================] - 0s 109us/step - loss: 0.8927 - acc: 0.6667 - val_loss: 0.9977 - val_acc: 0.6250\n",
            "Epoch 180/500\n",
            "93/93 [==============================] - 0s 153us/step - loss: 0.8901 - acc: 0.6667 - val_loss: 0.9975 - val_acc: 0.6250\n",
            "Epoch 181/500\n",
            "93/93 [==============================] - 0s 135us/step - loss: 0.8882 - acc: 0.6667 - val_loss: 0.9966 - val_acc: 0.6250\n",
            "Epoch 182/500\n",
            "93/93 [==============================] - 0s 130us/step - loss: 0.8865 - acc: 0.6774 - val_loss: 0.9954 - val_acc: 0.6250\n",
            "Epoch 183/500\n",
            "93/93 [==============================] - 0s 127us/step - loss: 0.8846 - acc: 0.6882 - val_loss: 0.9942 - val_acc: 0.6250\n",
            "Epoch 184/500\n",
            "93/93 [==============================] - 0s 145us/step - loss: 0.8833 - acc: 0.6989 - val_loss: 0.9938 - val_acc: 0.6250\n",
            "Epoch 185/500\n",
            "93/93 [==============================] - 0s 129us/step - loss: 0.8812 - acc: 0.7097 - val_loss: 0.9921 - val_acc: 0.6250\n",
            "Epoch 186/500\n",
            "93/93 [==============================] - 0s 124us/step - loss: 0.8798 - acc: 0.7097 - val_loss: 0.9909 - val_acc: 0.6250\n",
            "Epoch 187/500\n",
            "93/93 [==============================] - 0s 129us/step - loss: 0.8778 - acc: 0.7097 - val_loss: 0.9896 - val_acc: 0.6250\n",
            "Epoch 188/500\n",
            "93/93 [==============================] - 0s 137us/step - loss: 0.8764 - acc: 0.7097 - val_loss: 0.9881 - val_acc: 0.6250\n",
            "Epoch 189/500\n",
            "93/93 [==============================] - 0s 114us/step - loss: 0.8746 - acc: 0.6989 - val_loss: 0.9870 - val_acc: 0.6250\n",
            "Epoch 190/500\n",
            "93/93 [==============================] - 0s 130us/step - loss: 0.8730 - acc: 0.7097 - val_loss: 0.9858 - val_acc: 0.6250\n",
            "Epoch 191/500\n",
            "93/93 [==============================] - 0s 142us/step - loss: 0.8710 - acc: 0.6774 - val_loss: 0.9851 - val_acc: 0.6250\n",
            "Epoch 192/500\n",
            "93/93 [==============================] - 0s 128us/step - loss: 0.8696 - acc: 0.6989 - val_loss: 0.9843 - val_acc: 0.6250\n",
            "Epoch 193/500\n",
            "93/93 [==============================] - 0s 122us/step - loss: 0.8679 - acc: 0.6989 - val_loss: 0.9840 - val_acc: 0.6250\n",
            "Epoch 194/500\n",
            "93/93 [==============================] - 0s 130us/step - loss: 0.8667 - acc: 0.6989 - val_loss: 0.9837 - val_acc: 0.6250\n",
            "Epoch 195/500\n",
            "93/93 [==============================] - 0s 122us/step - loss: 0.8646 - acc: 0.7097 - val_loss: 0.9829 - val_acc: 0.6250\n",
            "Epoch 196/500\n",
            "93/93 [==============================] - 0s 122us/step - loss: 0.8631 - acc: 0.7097 - val_loss: 0.9821 - val_acc: 0.6250\n",
            "Epoch 197/500\n",
            "93/93 [==============================] - 0s 120us/step - loss: 0.8618 - acc: 0.6989 - val_loss: 0.9808 - val_acc: 0.6250\n",
            "Epoch 198/500\n",
            "93/93 [==============================] - 0s 144us/step - loss: 0.8607 - acc: 0.6989 - val_loss: 0.9802 - val_acc: 0.6250\n",
            "Epoch 199/500\n",
            "93/93 [==============================] - 0s 135us/step - loss: 0.8589 - acc: 0.6989 - val_loss: 0.9787 - val_acc: 0.6250\n",
            "Epoch 200/500\n",
            "93/93 [==============================] - 0s 129us/step - loss: 0.8569 - acc: 0.6989 - val_loss: 0.9775 - val_acc: 0.6250\n",
            "Epoch 201/500\n",
            "93/93 [==============================] - 0s 138us/step - loss: 0.8561 - acc: 0.6989 - val_loss: 0.9761 - val_acc: 0.6250\n",
            "Epoch 202/500\n",
            "93/93 [==============================] - 0s 132us/step - loss: 0.8542 - acc: 0.6774 - val_loss: 0.9758 - val_acc: 0.6250\n",
            "Epoch 203/500\n",
            "93/93 [==============================] - 0s 139us/step - loss: 0.8528 - acc: 0.6882 - val_loss: 0.9759 - val_acc: 0.6250\n",
            "Epoch 204/500\n",
            "93/93 [==============================] - 0s 137us/step - loss: 0.8511 - acc: 0.6989 - val_loss: 0.9756 - val_acc: 0.6250\n",
            "Epoch 205/500\n",
            "93/93 [==============================] - 0s 124us/step - loss: 0.8495 - acc: 0.6989 - val_loss: 0.9747 - val_acc: 0.6250\n",
            "Epoch 206/500\n",
            "93/93 [==============================] - 0s 130us/step - loss: 0.8480 - acc: 0.6989 - val_loss: 0.9738 - val_acc: 0.6250\n",
            "Epoch 207/500\n",
            "93/93 [==============================] - 0s 141us/step - loss: 0.8465 - acc: 0.6989 - val_loss: 0.9734 - val_acc: 0.6250\n",
            "Epoch 208/500\n",
            "93/93 [==============================] - 0s 132us/step - loss: 0.8451 - acc: 0.6989 - val_loss: 0.9725 - val_acc: 0.6250\n",
            "Epoch 209/500\n",
            "93/93 [==============================] - 0s 121us/step - loss: 0.8442 - acc: 0.6989 - val_loss: 0.9726 - val_acc: 0.6250\n",
            "Epoch 210/500\n",
            "93/93 [==============================] - 0s 125us/step - loss: 0.8420 - acc: 0.6989 - val_loss: 0.9715 - val_acc: 0.6250\n",
            "Epoch 211/500\n",
            "93/93 [==============================] - 0s 169us/step - loss: 0.8405 - acc: 0.6989 - val_loss: 0.9708 - val_acc: 0.6250\n",
            "Epoch 212/500\n",
            "93/93 [==============================] - 0s 132us/step - loss: 0.8396 - acc: 0.6989 - val_loss: 0.9700 - val_acc: 0.6250\n",
            "Epoch 213/500\n",
            "93/93 [==============================] - 0s 130us/step - loss: 0.8377 - acc: 0.6989 - val_loss: 0.9694 - val_acc: 0.6250\n",
            "Epoch 214/500\n",
            "93/93 [==============================] - 0s 131us/step - loss: 0.8363 - acc: 0.6989 - val_loss: 0.9679 - val_acc: 0.6250\n",
            "Epoch 215/500\n",
            "93/93 [==============================] - 0s 118us/step - loss: 0.8349 - acc: 0.6989 - val_loss: 0.9672 - val_acc: 0.6250\n",
            "Epoch 216/500\n",
            "93/93 [==============================] - 0s 128us/step - loss: 0.8333 - acc: 0.6989 - val_loss: 0.9667 - val_acc: 0.6250\n",
            "Epoch 217/500\n",
            "93/93 [==============================] - 0s 135us/step - loss: 0.8320 - acc: 0.6989 - val_loss: 0.9664 - val_acc: 0.6250\n",
            "Epoch 218/500\n",
            "93/93 [==============================] - 0s 143us/step - loss: 0.8306 - acc: 0.6989 - val_loss: 0.9652 - val_acc: 0.6250\n",
            "Epoch 219/500\n",
            "93/93 [==============================] - 0s 144us/step - loss: 0.8291 - acc: 0.6989 - val_loss: 0.9651 - val_acc: 0.6250\n",
            "Epoch 220/500\n",
            "93/93 [==============================] - 0s 139us/step - loss: 0.8280 - acc: 0.6989 - val_loss: 0.9646 - val_acc: 0.6250\n",
            "Epoch 221/500\n",
            "93/93 [==============================] - 0s 134us/step - loss: 0.8269 - acc: 0.6989 - val_loss: 0.9642 - val_acc: 0.6250\n",
            "Epoch 222/500\n",
            "93/93 [==============================] - 0s 137us/step - loss: 0.8255 - acc: 0.6989 - val_loss: 0.9633 - val_acc: 0.6250\n",
            "Epoch 223/500\n",
            "93/93 [==============================] - 0s 125us/step - loss: 0.8238 - acc: 0.6989 - val_loss: 0.9627 - val_acc: 0.6250\n",
            "Epoch 224/500\n",
            "93/93 [==============================] - 0s 130us/step - loss: 0.8233 - acc: 0.6989 - val_loss: 0.9624 - val_acc: 0.6250\n",
            "Epoch 225/500\n",
            "93/93 [==============================] - 0s 141us/step - loss: 0.8216 - acc: 0.6989 - val_loss: 0.9622 - val_acc: 0.6250\n",
            "Epoch 226/500\n",
            "93/93 [==============================] - 0s 124us/step - loss: 0.8203 - acc: 0.6989 - val_loss: 0.9614 - val_acc: 0.6250\n",
            "Epoch 227/500\n",
            "93/93 [==============================] - 0s 132us/step - loss: 0.8191 - acc: 0.6989 - val_loss: 0.9601 - val_acc: 0.6250\n",
            "Epoch 228/500\n",
            "93/93 [==============================] - 0s 132us/step - loss: 0.8179 - acc: 0.6989 - val_loss: 0.9591 - val_acc: 0.6250\n",
            "Epoch 229/500\n",
            "93/93 [==============================] - 0s 122us/step - loss: 0.8167 - acc: 0.7097 - val_loss: 0.9596 - val_acc: 0.6250\n",
            "Epoch 230/500\n",
            "93/93 [==============================] - 0s 128us/step - loss: 0.8147 - acc: 0.7097 - val_loss: 0.9592 - val_acc: 0.6250\n",
            "Epoch 231/500\n",
            "93/93 [==============================] - 0s 129us/step - loss: 0.8138 - acc: 0.7097 - val_loss: 0.9579 - val_acc: 0.6250\n",
            "Epoch 232/500\n",
            "93/93 [==============================] - 0s 140us/step - loss: 0.8122 - acc: 0.7097 - val_loss: 0.9573 - val_acc: 0.6250\n",
            "Epoch 233/500\n",
            "93/93 [==============================] - 0s 143us/step - loss: 0.8110 - acc: 0.7097 - val_loss: 0.9567 - val_acc: 0.6250\n",
            "Epoch 234/500\n",
            "93/93 [==============================] - 0s 134us/step - loss: 0.8097 - acc: 0.7097 - val_loss: 0.9561 - val_acc: 0.6250\n",
            "Epoch 235/500\n",
            "93/93 [==============================] - 0s 163us/step - loss: 0.8084 - acc: 0.7097 - val_loss: 0.9561 - val_acc: 0.6250\n",
            "Epoch 236/500\n",
            "93/93 [==============================] - 0s 192us/step - loss: 0.8074 - acc: 0.7097 - val_loss: 0.9556 - val_acc: 0.6250\n",
            "Epoch 237/500\n",
            "93/93 [==============================] - 0s 172us/step - loss: 0.8068 - acc: 0.7097 - val_loss: 0.9540 - val_acc: 0.6250\n",
            "Epoch 238/500\n",
            "93/93 [==============================] - 0s 162us/step - loss: 0.8048 - acc: 0.7097 - val_loss: 0.9533 - val_acc: 0.6250\n",
            "Epoch 239/500\n",
            "93/93 [==============================] - 0s 189us/step - loss: 0.8049 - acc: 0.7097 - val_loss: 0.9530 - val_acc: 0.6250\n",
            "Epoch 240/500\n",
            "93/93 [==============================] - 0s 141us/step - loss: 0.8022 - acc: 0.7204 - val_loss: 0.9526 - val_acc: 0.6250\n",
            "Epoch 241/500\n",
            "93/93 [==============================] - 0s 130us/step - loss: 0.8015 - acc: 0.7204 - val_loss: 0.9519 - val_acc: 0.6250\n",
            "Epoch 242/500\n",
            "93/93 [==============================] - 0s 138us/step - loss: 0.8004 - acc: 0.7097 - val_loss: 0.9519 - val_acc: 0.6667\n",
            "Epoch 243/500\n",
            "93/93 [==============================] - 0s 151us/step - loss: 0.7987 - acc: 0.7097 - val_loss: 0.9518 - val_acc: 0.6667\n",
            "Epoch 244/500\n",
            "93/93 [==============================] - 0s 147us/step - loss: 0.7980 - acc: 0.7097 - val_loss: 0.9524 - val_acc: 0.6667\n",
            "Epoch 245/500\n",
            "93/93 [==============================] - 0s 132us/step - loss: 0.7962 - acc: 0.7097 - val_loss: 0.9518 - val_acc: 0.6667\n",
            "Epoch 246/500\n",
            "93/93 [==============================] - 0s 108us/step - loss: 0.7957 - acc: 0.6989 - val_loss: 0.9521 - val_acc: 0.6250\n",
            "Epoch 247/500\n",
            "93/93 [==============================] - 0s 111us/step - loss: 0.7940 - acc: 0.7097 - val_loss: 0.9508 - val_acc: 0.6667\n",
            "Epoch 248/500\n",
            "93/93 [==============================] - 0s 137us/step - loss: 0.7929 - acc: 0.7097 - val_loss: 0.9496 - val_acc: 0.6667\n",
            "Epoch 249/500\n",
            "93/93 [==============================] - 0s 131us/step - loss: 0.7918 - acc: 0.6989 - val_loss: 0.9493 - val_acc: 0.6667\n",
            "Epoch 250/500\n",
            "93/93 [==============================] - 0s 161us/step - loss: 0.7921 - acc: 0.7097 - val_loss: 0.9476 - val_acc: 0.6667\n",
            "Epoch 251/500\n",
            "93/93 [==============================] - 0s 150us/step - loss: 0.7897 - acc: 0.7097 - val_loss: 0.9480 - val_acc: 0.6667\n",
            "Epoch 252/500\n",
            "93/93 [==============================] - 0s 149us/step - loss: 0.7879 - acc: 0.7097 - val_loss: 0.9472 - val_acc: 0.6667\n",
            "Epoch 253/500\n",
            "93/93 [==============================] - 0s 145us/step - loss: 0.7878 - acc: 0.6989 - val_loss: 0.9470 - val_acc: 0.6667\n",
            "Epoch 254/500\n",
            "93/93 [==============================] - 0s 136us/step - loss: 0.7861 - acc: 0.7097 - val_loss: 0.9461 - val_acc: 0.6667\n",
            "Epoch 255/500\n",
            "93/93 [==============================] - 0s 125us/step - loss: 0.7847 - acc: 0.7097 - val_loss: 0.9458 - val_acc: 0.6667\n",
            "Epoch 256/500\n",
            "93/93 [==============================] - 0s 151us/step - loss: 0.7847 - acc: 0.7097 - val_loss: 0.9464 - val_acc: 0.6667\n",
            "Epoch 257/500\n",
            "93/93 [==============================] - 0s 138us/step - loss: 0.7824 - acc: 0.6989 - val_loss: 0.9461 - val_acc: 0.6667\n",
            "Epoch 258/500\n",
            "93/93 [==============================] - 0s 132us/step - loss: 0.7814 - acc: 0.7097 - val_loss: 0.9459 - val_acc: 0.6667\n",
            "Epoch 259/500\n",
            "93/93 [==============================] - 0s 138us/step - loss: 0.7803 - acc: 0.7097 - val_loss: 0.9452 - val_acc: 0.6667\n",
            "Epoch 260/500\n",
            "93/93 [==============================] - 0s 128us/step - loss: 0.7793 - acc: 0.7097 - val_loss: 0.9445 - val_acc: 0.6667\n",
            "Epoch 261/500\n",
            "93/93 [==============================] - 0s 130us/step - loss: 0.7778 - acc: 0.7097 - val_loss: 0.9442 - val_acc: 0.6667\n",
            "Epoch 262/500\n",
            "93/93 [==============================] - 0s 148us/step - loss: 0.7774 - acc: 0.7097 - val_loss: 0.9442 - val_acc: 0.6667\n",
            "Epoch 263/500\n",
            "93/93 [==============================] - 0s 139us/step - loss: 0.7762 - acc: 0.7097 - val_loss: 0.9435 - val_acc: 0.6667\n",
            "Epoch 264/500\n",
            "93/93 [==============================] - 0s 170us/step - loss: 0.7750 - acc: 0.6989 - val_loss: 0.9432 - val_acc: 0.6667\n",
            "Epoch 265/500\n",
            "93/93 [==============================] - 0s 131us/step - loss: 0.7736 - acc: 0.6989 - val_loss: 0.9426 - val_acc: 0.6667\n",
            "Epoch 266/500\n",
            "93/93 [==============================] - 0s 120us/step - loss: 0.7725 - acc: 0.6989 - val_loss: 0.9423 - val_acc: 0.6667\n",
            "Epoch 267/500\n",
            "93/93 [==============================] - 0s 137us/step - loss: 0.7717 - acc: 0.6989 - val_loss: 0.9420 - val_acc: 0.6667\n",
            "Epoch 268/500\n",
            "93/93 [==============================] - 0s 154us/step - loss: 0.7705 - acc: 0.6989 - val_loss: 0.9410 - val_acc: 0.6667\n",
            "Epoch 269/500\n",
            "93/93 [==============================] - 0s 134us/step - loss: 0.7697 - acc: 0.6989 - val_loss: 0.9397 - val_acc: 0.6667\n",
            "Epoch 270/500\n",
            "93/93 [==============================] - 0s 151us/step - loss: 0.7689 - acc: 0.6989 - val_loss: 0.9396 - val_acc: 0.6667\n",
            "Epoch 271/500\n",
            "93/93 [==============================] - 0s 137us/step - loss: 0.7679 - acc: 0.6989 - val_loss: 0.9385 - val_acc: 0.6667\n",
            "Epoch 272/500\n",
            "93/93 [==============================] - 0s 143us/step - loss: 0.7661 - acc: 0.6989 - val_loss: 0.9384 - val_acc: 0.6667\n",
            "Epoch 273/500\n",
            "93/93 [==============================] - 0s 164us/step - loss: 0.7653 - acc: 0.6989 - val_loss: 0.9395 - val_acc: 0.6667\n",
            "Epoch 274/500\n",
            "93/93 [==============================] - 0s 163us/step - loss: 0.7643 - acc: 0.6989 - val_loss: 0.9391 - val_acc: 0.6667\n",
            "Epoch 275/500\n",
            "93/93 [==============================] - 0s 155us/step - loss: 0.7627 - acc: 0.6989 - val_loss: 0.9395 - val_acc: 0.6667\n",
            "Epoch 276/500\n",
            "93/93 [==============================] - 0s 171us/step - loss: 0.7624 - acc: 0.6989 - val_loss: 0.9399 - val_acc: 0.6667\n",
            "Epoch 277/500\n",
            "93/93 [==============================] - 0s 177us/step - loss: 0.7609 - acc: 0.6989 - val_loss: 0.9397 - val_acc: 0.6667\n",
            "Epoch 278/500\n",
            "93/93 [==============================] - 0s 182us/step - loss: 0.7598 - acc: 0.6989 - val_loss: 0.9395 - val_acc: 0.6667\n",
            "Epoch 279/500\n",
            "93/93 [==============================] - 0s 176us/step - loss: 0.7591 - acc: 0.6989 - val_loss: 0.9399 - val_acc: 0.6667\n",
            "Epoch 280/500\n",
            "93/93 [==============================] - 0s 167us/step - loss: 0.7580 - acc: 0.6989 - val_loss: 0.9404 - val_acc: 0.6667\n",
            "Epoch 281/500\n",
            "93/93 [==============================] - 0s 158us/step - loss: 0.7566 - acc: 0.6989 - val_loss: 0.9398 - val_acc: 0.6667\n",
            "Epoch 282/500\n",
            "93/93 [==============================] - 0s 156us/step - loss: 0.7557 - acc: 0.6989 - val_loss: 0.9403 - val_acc: 0.6667\n",
            "Epoch 283/500\n",
            "93/93 [==============================] - 0s 162us/step - loss: 0.7552 - acc: 0.6989 - val_loss: 0.9404 - val_acc: 0.6667\n",
            "Epoch 284/500\n",
            "93/93 [==============================] - 0s 151us/step - loss: 0.7538 - acc: 0.6989 - val_loss: 0.9403 - val_acc: 0.6667\n",
            "Epoch 285/500\n",
            "93/93 [==============================] - 0s 145us/step - loss: 0.7526 - acc: 0.6989 - val_loss: 0.9400 - val_acc: 0.6667\n",
            "Epoch 286/500\n",
            "93/93 [==============================] - 0s 87us/step - loss: 0.7517 - acc: 0.6989 - val_loss: 0.9389 - val_acc: 0.6667\n",
            "Epoch 287/500\n",
            "93/93 [==============================] - 0s 78us/step - loss: 0.7508 - acc: 0.6989 - val_loss: 0.9393 - val_acc: 0.6667\n",
            "Epoch 288/500\n",
            "93/93 [==============================] - 0s 80us/step - loss: 0.7500 - acc: 0.6989 - val_loss: 0.9397 - val_acc: 0.6667\n",
            "Epoch 289/500\n",
            "93/93 [==============================] - 0s 99us/step - loss: 0.7494 - acc: 0.6989 - val_loss: 0.9390 - val_acc: 0.6667\n",
            "Epoch 290/500\n",
            "93/93 [==============================] - 0s 122us/step - loss: 0.7479 - acc: 0.6989 - val_loss: 0.9392 - val_acc: 0.6667\n",
            "Epoch 291/500\n",
            "93/93 [==============================] - 0s 140us/step - loss: 0.7471 - acc: 0.6989 - val_loss: 0.9391 - val_acc: 0.6667\n",
            "Epoch 292/500\n",
            "93/93 [==============================] - 0s 133us/step - loss: 0.7464 - acc: 0.6989 - val_loss: 0.9400 - val_acc: 0.6667\n",
            "Epoch 293/500\n",
            "93/93 [==============================] - 0s 139us/step - loss: 0.7452 - acc: 0.6989 - val_loss: 0.9392 - val_acc: 0.6667\n",
            "Epoch 294/500\n",
            "93/93 [==============================] - 0s 151us/step - loss: 0.7449 - acc: 0.6989 - val_loss: 0.9378 - val_acc: 0.6667\n",
            "Epoch 295/500\n",
            "93/93 [==============================] - 0s 128us/step - loss: 0.7438 - acc: 0.6989 - val_loss: 0.9381 - val_acc: 0.6667\n",
            "Epoch 296/500\n",
            "93/93 [==============================] - 0s 142us/step - loss: 0.7434 - acc: 0.6989 - val_loss: 0.9384 - val_acc: 0.6667\n",
            "Epoch 297/500\n",
            "93/93 [==============================] - 0s 147us/step - loss: 0.7416 - acc: 0.6989 - val_loss: 0.9387 - val_acc: 0.6667\n",
            "Epoch 298/500\n",
            "93/93 [==============================] - 0s 198us/step - loss: 0.7405 - acc: 0.7097 - val_loss: 0.9390 - val_acc: 0.6667\n",
            "Epoch 299/500\n",
            "93/93 [==============================] - 0s 168us/step - loss: 0.7398 - acc: 0.7097 - val_loss: 0.9400 - val_acc: 0.6667\n",
            "Epoch 300/500\n",
            "93/93 [==============================] - 0s 173us/step - loss: 0.7387 - acc: 0.7097 - val_loss: 0.9404 - val_acc: 0.6667\n",
            "Epoch 301/500\n",
            "93/93 [==============================] - 0s 147us/step - loss: 0.7376 - acc: 0.7097 - val_loss: 0.9406 - val_acc: 0.6667\n",
            "Epoch 302/500\n",
            "93/93 [==============================] - 0s 160us/step - loss: 0.7368 - acc: 0.7097 - val_loss: 0.9403 - val_acc: 0.6667\n",
            "Epoch 303/500\n",
            "93/93 [==============================] - 0s 121us/step - loss: 0.7368 - acc: 0.6989 - val_loss: 0.9386 - val_acc: 0.6667\n",
            "Epoch 304/500\n",
            "93/93 [==============================] - 0s 144us/step - loss: 0.7347 - acc: 0.7097 - val_loss: 0.9383 - val_acc: 0.6667\n",
            "Epoch 305/500\n",
            "93/93 [==============================] - 0s 152us/step - loss: 0.7335 - acc: 0.7097 - val_loss: 0.9379 - val_acc: 0.6667\n",
            "Epoch 306/500\n",
            "93/93 [==============================] - 0s 191us/step - loss: 0.7339 - acc: 0.7097 - val_loss: 0.9362 - val_acc: 0.6667\n",
            "Epoch 307/500\n",
            "93/93 [==============================] - 0s 160us/step - loss: 0.7318 - acc: 0.7097 - val_loss: 0.9366 - val_acc: 0.6667\n",
            "Epoch 308/500\n",
            "93/93 [==============================] - 0s 152us/step - loss: 0.7316 - acc: 0.7097 - val_loss: 0.9369 - val_acc: 0.6667\n",
            "Epoch 309/500\n",
            "93/93 [==============================] - 0s 151us/step - loss: 0.7313 - acc: 0.7097 - val_loss: 0.9356 - val_acc: 0.6667\n",
            "Epoch 310/500\n",
            "93/93 [==============================] - 0s 161us/step - loss: 0.7297 - acc: 0.7312 - val_loss: 0.9370 - val_acc: 0.6667\n",
            "Epoch 311/500\n",
            "93/93 [==============================] - 0s 170us/step - loss: 0.7284 - acc: 0.7097 - val_loss: 0.9379 - val_acc: 0.6667\n",
            "Epoch 312/500\n",
            "93/93 [==============================] - 0s 149us/step - loss: 0.7279 - acc: 0.7097 - val_loss: 0.9372 - val_acc: 0.6667\n",
            "Epoch 313/500\n",
            "93/93 [==============================] - 0s 263us/step - loss: 0.7265 - acc: 0.7204 - val_loss: 0.9378 - val_acc: 0.6667\n",
            "Epoch 314/500\n",
            "93/93 [==============================] - 0s 148us/step - loss: 0.7265 - acc: 0.7204 - val_loss: 0.9384 - val_acc: 0.6667\n",
            "Epoch 315/500\n",
            "93/93 [==============================] - 0s 145us/step - loss: 0.7244 - acc: 0.7097 - val_loss: 0.9381 - val_acc: 0.6667\n",
            "Epoch 316/500\n",
            "93/93 [==============================] - 0s 143us/step - loss: 0.7240 - acc: 0.7419 - val_loss: 0.9389 - val_acc: 0.6667\n",
            "Epoch 317/500\n",
            "93/93 [==============================] - 0s 143us/step - loss: 0.7230 - acc: 0.7204 - val_loss: 0.9390 - val_acc: 0.6667\n",
            "Epoch 318/500\n",
            "93/93 [==============================] - 0s 157us/step - loss: 0.7219 - acc: 0.7204 - val_loss: 0.9384 - val_acc: 0.6667\n",
            "Epoch 319/500\n",
            "93/93 [==============================] - 0s 151us/step - loss: 0.7223 - acc: 0.7204 - val_loss: 0.9368 - val_acc: 0.6667\n",
            "Epoch 320/500\n",
            "93/93 [==============================] - 0s 153us/step - loss: 0.7206 - acc: 0.7312 - val_loss: 0.9371 - val_acc: 0.6667\n",
            "Epoch 321/500\n",
            "93/93 [==============================] - 0s 149us/step - loss: 0.7195 - acc: 0.7204 - val_loss: 0.9381 - val_acc: 0.6667\n",
            "Epoch 322/500\n",
            "93/93 [==============================] - 0s 136us/step - loss: 0.7183 - acc: 0.7204 - val_loss: 0.9383 - val_acc: 0.6667\n",
            "Epoch 323/500\n",
            "93/93 [==============================] - 0s 159us/step - loss: 0.7174 - acc: 0.7204 - val_loss: 0.9378 - val_acc: 0.6667\n",
            "Epoch 324/500\n",
            "93/93 [==============================] - 0s 171us/step - loss: 0.7176 - acc: 0.7204 - val_loss: 0.9393 - val_acc: 0.6667\n",
            "Epoch 325/500\n",
            "93/93 [==============================] - 0s 140us/step - loss: 0.7165 - acc: 0.7204 - val_loss: 0.9395 - val_acc: 0.6667\n",
            "Epoch 326/500\n",
            "93/93 [==============================] - 0s 142us/step - loss: 0.7153 - acc: 0.7204 - val_loss: 0.9401 - val_acc: 0.6667\n",
            "Epoch 327/500\n",
            "93/93 [==============================] - 0s 127us/step - loss: 0.7143 - acc: 0.7204 - val_loss: 0.9406 - val_acc: 0.6667\n",
            "Epoch 328/500\n",
            "93/93 [==============================] - 0s 134us/step - loss: 0.7143 - acc: 0.7204 - val_loss: 0.9422 - val_acc: 0.6667\n",
            "Epoch 329/500\n",
            "93/93 [==============================] - 0s 137us/step - loss: 0.7133 - acc: 0.7204 - val_loss: 0.9439 - val_acc: 0.6667\n",
            "Epoch 330/500\n",
            "93/93 [==============================] - 0s 133us/step - loss: 0.7115 - acc: 0.7204 - val_loss: 0.9437 - val_acc: 0.6667\n",
            "Epoch 331/500\n",
            "93/93 [==============================] - 0s 164us/step - loss: 0.7118 - acc: 0.7204 - val_loss: 0.9424 - val_acc: 0.6667\n",
            "Epoch 332/500\n",
            "93/93 [==============================] - 0s 130us/step - loss: 0.7103 - acc: 0.7204 - val_loss: 0.9435 - val_acc: 0.6667\n",
            "Epoch 333/500\n",
            "93/93 [==============================] - 0s 105us/step - loss: 0.7093 - acc: 0.7312 - val_loss: 0.9441 - val_acc: 0.6667\n",
            "Epoch 334/500\n",
            "93/93 [==============================] - 0s 150us/step - loss: 0.7083 - acc: 0.7312 - val_loss: 0.9441 - val_acc: 0.6667\n",
            "Epoch 335/500\n",
            "93/93 [==============================] - 0s 152us/step - loss: 0.7079 - acc: 0.7312 - val_loss: 0.9422 - val_acc: 0.6667\n",
            "Epoch 336/500\n",
            "93/93 [==============================] - 0s 130us/step - loss: 0.7066 - acc: 0.7312 - val_loss: 0.9416 - val_acc: 0.6667\n",
            "Epoch 337/500\n",
            "93/93 [==============================] - 0s 130us/step - loss: 0.7059 - acc: 0.7419 - val_loss: 0.9425 - val_acc: 0.6667\n",
            "Epoch 338/500\n",
            "93/93 [==============================] - 0s 152us/step - loss: 0.7047 - acc: 0.7527 - val_loss: 0.9422 - val_acc: 0.6667\n",
            "Epoch 339/500\n",
            "93/93 [==============================] - 0s 159us/step - loss: 0.7058 - acc: 0.7419 - val_loss: 0.9443 - val_acc: 0.6667\n",
            "Epoch 340/500\n",
            "93/93 [==============================] - 0s 124us/step - loss: 0.7047 - acc: 0.7204 - val_loss: 0.9452 - val_acc: 0.6667\n",
            "Epoch 341/500\n",
            "93/93 [==============================] - 0s 135us/step - loss: 0.7033 - acc: 0.7312 - val_loss: 0.9436 - val_acc: 0.6667\n",
            "Epoch 342/500\n",
            "93/93 [==============================] - 0s 155us/step - loss: 0.7014 - acc: 0.7527 - val_loss: 0.9431 - val_acc: 0.6667\n",
            "Epoch 343/500\n",
            "93/93 [==============================] - 0s 158us/step - loss: 0.7011 - acc: 0.7527 - val_loss: 0.9418 - val_acc: 0.6667\n",
            "Epoch 344/500\n",
            "93/93 [==============================] - 0s 176us/step - loss: 0.7000 - acc: 0.7419 - val_loss: 0.9430 - val_acc: 0.6667\n",
            "Epoch 345/500\n",
            "93/93 [==============================] - 0s 169us/step - loss: 0.6994 - acc: 0.7419 - val_loss: 0.9423 - val_acc: 0.6667\n",
            "Epoch 346/500\n",
            "93/93 [==============================] - 0s 176us/step - loss: 0.6978 - acc: 0.7419 - val_loss: 0.9422 - val_acc: 0.6667\n",
            "Epoch 347/500\n",
            "93/93 [==============================] - 0s 146us/step - loss: 0.6977 - acc: 0.7419 - val_loss: 0.9418 - val_acc: 0.6667\n",
            "Epoch 348/500\n",
            "93/93 [==============================] - 0s 152us/step - loss: 0.6966 - acc: 0.7419 - val_loss: 0.9438 - val_acc: 0.6667\n",
            "Epoch 349/500\n",
            "93/93 [==============================] - 0s 130us/step - loss: 0.6962 - acc: 0.7419 - val_loss: 0.9452 - val_acc: 0.6667\n",
            "Epoch 350/500\n",
            "93/93 [==============================] - 0s 148us/step - loss: 0.6948 - acc: 0.7419 - val_loss: 0.9451 - val_acc: 0.6667\n",
            "Epoch 351/500\n",
            "93/93 [==============================] - 0s 165us/step - loss: 0.6939 - acc: 0.7419 - val_loss: 0.9450 - val_acc: 0.6667\n",
            "Epoch 352/500\n",
            "93/93 [==============================] - 0s 141us/step - loss: 0.6937 - acc: 0.7419 - val_loss: 0.9442 - val_acc: 0.6667\n",
            "Epoch 353/500\n",
            "93/93 [==============================] - 0s 133us/step - loss: 0.6920 - acc: 0.7527 - val_loss: 0.9444 - val_acc: 0.6667\n",
            "Epoch 354/500\n",
            "93/93 [==============================] - 0s 151us/step - loss: 0.6913 - acc: 0.7419 - val_loss: 0.9451 - val_acc: 0.6667\n",
            "Epoch 355/500\n",
            "93/93 [==============================] - 0s 140us/step - loss: 0.6905 - acc: 0.7527 - val_loss: 0.9454 - val_acc: 0.6667\n",
            "Epoch 356/500\n",
            "93/93 [==============================] - 0s 143us/step - loss: 0.6897 - acc: 0.7527 - val_loss: 0.9457 - val_acc: 0.6667\n",
            "Epoch 357/500\n",
            "93/93 [==============================] - 0s 120us/step - loss: 0.6888 - acc: 0.7527 - val_loss: 0.9454 - val_acc: 0.6667\n",
            "Epoch 358/500\n",
            "93/93 [==============================] - 0s 157us/step - loss: 0.6886 - acc: 0.7527 - val_loss: 0.9466 - val_acc: 0.6667\n",
            "Epoch 359/500\n",
            "93/93 [==============================] - 0s 160us/step - loss: 0.6880 - acc: 0.7634 - val_loss: 0.9450 - val_acc: 0.6250\n",
            "Epoch 360/500\n",
            "93/93 [==============================] - 0s 142us/step - loss: 0.6875 - acc: 0.7527 - val_loss: 0.9468 - val_acc: 0.6667\n",
            "Epoch 361/500\n",
            "93/93 [==============================] - 0s 144us/step - loss: 0.6882 - acc: 0.7527 - val_loss: 0.9487 - val_acc: 0.6667\n",
            "Epoch 362/500\n",
            "93/93 [==============================] - 0s 147us/step - loss: 0.6855 - acc: 0.7419 - val_loss: 0.9492 - val_acc: 0.6667\n",
            "Epoch 363/500\n",
            "93/93 [==============================] - 0s 157us/step - loss: 0.6853 - acc: 0.7527 - val_loss: 0.9502 - val_acc: 0.6667\n",
            "Epoch 364/500\n",
            "93/93 [==============================] - 0s 134us/step - loss: 0.6842 - acc: 0.7527 - val_loss: 0.9510 - val_acc: 0.6667\n",
            "Epoch 365/500\n",
            "93/93 [==============================] - 0s 148us/step - loss: 0.6826 - acc: 0.7634 - val_loss: 0.9505 - val_acc: 0.6667\n",
            "Epoch 366/500\n",
            "93/93 [==============================] - 0s 132us/step - loss: 0.6829 - acc: 0.7634 - val_loss: 0.9505 - val_acc: 0.6667\n",
            "Epoch 367/500\n",
            "93/93 [==============================] - 0s 145us/step - loss: 0.6816 - acc: 0.7527 - val_loss: 0.9504 - val_acc: 0.6667\n",
            "Epoch 368/500\n",
            "93/93 [==============================] - 0s 133us/step - loss: 0.6808 - acc: 0.7634 - val_loss: 0.9505 - val_acc: 0.6667\n",
            "Epoch 369/500\n",
            "93/93 [==============================] - 0s 129us/step - loss: 0.6796 - acc: 0.7634 - val_loss: 0.9496 - val_acc: 0.6667\n",
            "Epoch 370/500\n",
            "93/93 [==============================] - 0s 130us/step - loss: 0.6788 - acc: 0.7527 - val_loss: 0.9495 - val_acc: 0.6250\n",
            "Epoch 371/500\n",
            "93/93 [==============================] - 0s 134us/step - loss: 0.6782 - acc: 0.7634 - val_loss: 0.9491 - val_acc: 0.6250\n",
            "Epoch 372/500\n",
            "93/93 [==============================] - 0s 130us/step - loss: 0.6772 - acc: 0.7527 - val_loss: 0.9497 - val_acc: 0.6250\n",
            "Epoch 373/500\n",
            "93/93 [==============================] - 0s 124us/step - loss: 0.6767 - acc: 0.7527 - val_loss: 0.9499 - val_acc: 0.6250\n",
            "Epoch 374/500\n",
            "93/93 [==============================] - 0s 154us/step - loss: 0.6771 - acc: 0.7527 - val_loss: 0.9527 - val_acc: 0.6667\n",
            "Epoch 375/500\n",
            "93/93 [==============================] - 0s 140us/step - loss: 0.6750 - acc: 0.7634 - val_loss: 0.9536 - val_acc: 0.6667\n",
            "Epoch 376/500\n",
            "93/93 [==============================] - 0s 147us/step - loss: 0.6759 - acc: 0.7527 - val_loss: 0.9546 - val_acc: 0.6667\n",
            "Epoch 377/500\n",
            "93/93 [==============================] - 0s 150us/step - loss: 0.6738 - acc: 0.7634 - val_loss: 0.9550 - val_acc: 0.6667\n",
            "Epoch 378/500\n",
            "93/93 [==============================] - 0s 164us/step - loss: 0.6730 - acc: 0.7634 - val_loss: 0.9555 - val_acc: 0.6667\n",
            "Epoch 379/500\n",
            "93/93 [==============================] - 0s 140us/step - loss: 0.6732 - acc: 0.7634 - val_loss: 0.9570 - val_acc: 0.6667\n",
            "Epoch 380/500\n",
            "93/93 [==============================] - 0s 124us/step - loss: 0.6713 - acc: 0.7634 - val_loss: 0.9571 - val_acc: 0.6667\n",
            "Epoch 381/500\n",
            "93/93 [==============================] - 0s 135us/step - loss: 0.6708 - acc: 0.7634 - val_loss: 0.9561 - val_acc: 0.6250\n",
            "Epoch 382/500\n",
            "93/93 [==============================] - 0s 140us/step - loss: 0.6706 - acc: 0.7634 - val_loss: 0.9542 - val_acc: 0.6250\n",
            "Epoch 383/500\n",
            "93/93 [==============================] - 0s 129us/step - loss: 0.6689 - acc: 0.7527 - val_loss: 0.9539 - val_acc: 0.6250\n",
            "Epoch 384/500\n",
            "93/93 [==============================] - 0s 132us/step - loss: 0.6686 - acc: 0.7527 - val_loss: 0.9540 - val_acc: 0.6250\n",
            "Epoch 385/500\n",
            "93/93 [==============================] - 0s 130us/step - loss: 0.6672 - acc: 0.7634 - val_loss: 0.9552 - val_acc: 0.6250\n",
            "Epoch 386/500\n",
            "93/93 [==============================] - 0s 127us/step - loss: 0.6671 - acc: 0.7634 - val_loss: 0.9561 - val_acc: 0.6250\n",
            "Epoch 387/500\n",
            "93/93 [==============================] - 0s 150us/step - loss: 0.6662 - acc: 0.7634 - val_loss: 0.9561 - val_acc: 0.6250\n",
            "Epoch 388/500\n",
            "93/93 [==============================] - 0s 129us/step - loss: 0.6651 - acc: 0.7634 - val_loss: 0.9571 - val_acc: 0.6250\n",
            "Epoch 389/500\n",
            "93/93 [==============================] - 0s 139us/step - loss: 0.6643 - acc: 0.7527 - val_loss: 0.9578 - val_acc: 0.6250\n",
            "Epoch 390/500\n",
            "93/93 [==============================] - 0s 132us/step - loss: 0.6636 - acc: 0.7634 - val_loss: 0.9574 - val_acc: 0.6250\n",
            "Epoch 391/500\n",
            "93/93 [==============================] - 0s 130us/step - loss: 0.6626 - acc: 0.7634 - val_loss: 0.9569 - val_acc: 0.6250\n",
            "Epoch 392/500\n",
            "93/93 [==============================] - 0s 140us/step - loss: 0.6632 - acc: 0.7634 - val_loss: 0.9559 - val_acc: 0.6250\n",
            "Epoch 393/500\n",
            "93/93 [==============================] - 0s 150us/step - loss: 0.6613 - acc: 0.7634 - val_loss: 0.9564 - val_acc: 0.6250\n",
            "Epoch 394/500\n",
            "93/93 [==============================] - 0s 142us/step - loss: 0.6609 - acc: 0.7634 - val_loss: 0.9568 - val_acc: 0.6250\n",
            "Epoch 395/500\n",
            "93/93 [==============================] - 0s 123us/step - loss: 0.6598 - acc: 0.7634 - val_loss: 0.9563 - val_acc: 0.6250\n",
            "Epoch 396/500\n",
            "93/93 [==============================] - 0s 141us/step - loss: 0.6597 - acc: 0.7634 - val_loss: 0.9579 - val_acc: 0.6250\n",
            "Epoch 397/500\n",
            "93/93 [==============================] - 0s 155us/step - loss: 0.6584 - acc: 0.7742 - val_loss: 0.9572 - val_acc: 0.6250\n",
            "Epoch 398/500\n",
            "93/93 [==============================] - 0s 172us/step - loss: 0.6579 - acc: 0.7634 - val_loss: 0.9576 - val_acc: 0.6250\n",
            "Epoch 399/500\n",
            "93/93 [==============================] - 0s 182us/step - loss: 0.6571 - acc: 0.7634 - val_loss: 0.9568 - val_acc: 0.6250\n",
            "Epoch 400/500\n",
            "93/93 [==============================] - 0s 131us/step - loss: 0.6569 - acc: 0.7634 - val_loss: 0.9573 - val_acc: 0.6250\n",
            "Epoch 401/500\n",
            "93/93 [==============================] - 0s 153us/step - loss: 0.6571 - acc: 0.7634 - val_loss: 0.9582 - val_acc: 0.6250\n",
            "Epoch 402/500\n",
            "93/93 [==============================] - 0s 150us/step - loss: 0.6545 - acc: 0.7634 - val_loss: 0.9594 - val_acc: 0.6250\n",
            "Epoch 403/500\n",
            "93/93 [==============================] - 0s 131us/step - loss: 0.6541 - acc: 0.7634 - val_loss: 0.9590 - val_acc: 0.6250\n",
            "Epoch 404/500\n",
            "93/93 [==============================] - 0s 142us/step - loss: 0.6535 - acc: 0.7634 - val_loss: 0.9597 - val_acc: 0.6250\n",
            "Epoch 405/500\n",
            "93/93 [==============================] - 0s 151us/step - loss: 0.6527 - acc: 0.7634 - val_loss: 0.9592 - val_acc: 0.6250\n",
            "Epoch 406/500\n",
            "93/93 [==============================] - 0s 173us/step - loss: 0.6519 - acc: 0.7742 - val_loss: 0.9590 - val_acc: 0.6250\n",
            "Epoch 407/500\n",
            "93/93 [==============================] - 0s 136us/step - loss: 0.6515 - acc: 0.7634 - val_loss: 0.9597 - val_acc: 0.6250\n",
            "Epoch 408/500\n",
            "93/93 [==============================] - 0s 134us/step - loss: 0.6510 - acc: 0.7634 - val_loss: 0.9604 - val_acc: 0.6250\n",
            "Epoch 409/500\n",
            "93/93 [==============================] - 0s 148us/step - loss: 0.6495 - acc: 0.7634 - val_loss: 0.9616 - val_acc: 0.6250\n",
            "Epoch 410/500\n",
            "93/93 [==============================] - 0s 167us/step - loss: 0.6495 - acc: 0.7634 - val_loss: 0.9607 - val_acc: 0.6250\n",
            "Epoch 411/500\n",
            "93/93 [==============================] - 0s 181us/step - loss: 0.6479 - acc: 0.7634 - val_loss: 0.9610 - val_acc: 0.6250\n",
            "Epoch 412/500\n",
            "93/93 [==============================] - 0s 156us/step - loss: 0.6481 - acc: 0.7634 - val_loss: 0.9632 - val_acc: 0.6250\n",
            "Epoch 413/500\n",
            "93/93 [==============================] - 0s 159us/step - loss: 0.6473 - acc: 0.7742 - val_loss: 0.9633 - val_acc: 0.6250\n",
            "Epoch 414/500\n",
            "93/93 [==============================] - 0s 151us/step - loss: 0.6467 - acc: 0.7742 - val_loss: 0.9655 - val_acc: 0.6250\n",
            "Epoch 415/500\n",
            "93/93 [==============================] - 0s 159us/step - loss: 0.6454 - acc: 0.7742 - val_loss: 0.9657 - val_acc: 0.6250\n",
            "Epoch 416/500\n",
            "93/93 [==============================] - 0s 140us/step - loss: 0.6444 - acc: 0.7742 - val_loss: 0.9667 - val_acc: 0.6250\n",
            "Epoch 417/500\n",
            "93/93 [==============================] - 0s 140us/step - loss: 0.6437 - acc: 0.7742 - val_loss: 0.9678 - val_acc: 0.6250\n",
            "Epoch 418/500\n",
            "93/93 [==============================] - 0s 255us/step - loss: 0.6439 - acc: 0.7742 - val_loss: 0.9666 - val_acc: 0.6250\n",
            "Epoch 419/500\n",
            "93/93 [==============================] - 0s 235us/step - loss: 0.6428 - acc: 0.7634 - val_loss: 0.9653 - val_acc: 0.6250\n",
            "Epoch 420/500\n",
            "93/93 [==============================] - 0s 240us/step - loss: 0.6418 - acc: 0.7634 - val_loss: 0.9653 - val_acc: 0.6250\n",
            "Epoch 421/500\n",
            "93/93 [==============================] - 0s 283us/step - loss: 0.6415 - acc: 0.7634 - val_loss: 0.9654 - val_acc: 0.6250\n",
            "Epoch 422/500\n",
            "93/93 [==============================] - 0s 226us/step - loss: 0.6407 - acc: 0.7527 - val_loss: 0.9672 - val_acc: 0.6250\n",
            "Epoch 423/500\n",
            "93/93 [==============================] - 0s 168us/step - loss: 0.6402 - acc: 0.7634 - val_loss: 0.9668 - val_acc: 0.6250\n",
            "Epoch 424/500\n",
            "93/93 [==============================] - 0s 154us/step - loss: 0.6394 - acc: 0.7634 - val_loss: 0.9670 - val_acc: 0.6250\n",
            "Epoch 425/500\n",
            "93/93 [==============================] - 0s 152us/step - loss: 0.6381 - acc: 0.7527 - val_loss: 0.9672 - val_acc: 0.6250\n",
            "Epoch 426/500\n",
            "93/93 [==============================] - 0s 134us/step - loss: 0.6379 - acc: 0.7634 - val_loss: 0.9671 - val_acc: 0.6250\n",
            "Epoch 427/500\n",
            "93/93 [==============================] - 0s 168us/step - loss: 0.6386 - acc: 0.7634 - val_loss: 0.9687 - val_acc: 0.6250\n",
            "Epoch 428/500\n",
            "93/93 [==============================] - 0s 139us/step - loss: 0.6366 - acc: 0.7742 - val_loss: 0.9690 - val_acc: 0.6250\n",
            "Epoch 429/500\n",
            "93/93 [==============================] - 0s 112us/step - loss: 0.6373 - acc: 0.7634 - val_loss: 0.9668 - val_acc: 0.6250\n",
            "Epoch 430/500\n",
            "93/93 [==============================] - 0s 133us/step - loss: 0.6350 - acc: 0.7527 - val_loss: 0.9677 - val_acc: 0.6250\n",
            "Epoch 431/500\n",
            "93/93 [==============================] - 0s 135us/step - loss: 0.6348 - acc: 0.7527 - val_loss: 0.9690 - val_acc: 0.6250\n",
            "Epoch 432/500\n",
            "93/93 [==============================] - 0s 120us/step - loss: 0.6340 - acc: 0.7634 - val_loss: 0.9700 - val_acc: 0.6250\n",
            "Epoch 433/500\n",
            "93/93 [==============================] - 0s 142us/step - loss: 0.6334 - acc: 0.7527 - val_loss: 0.9693 - val_acc: 0.6250\n",
            "Epoch 434/500\n",
            "93/93 [==============================] - 0s 145us/step - loss: 0.6337 - acc: 0.7527 - val_loss: 0.9723 - val_acc: 0.6250\n",
            "Epoch 435/500\n",
            "93/93 [==============================] - 0s 172us/step - loss: 0.6315 - acc: 0.7634 - val_loss: 0.9728 - val_acc: 0.6250\n",
            "Epoch 436/500\n",
            "93/93 [==============================] - 0s 164us/step - loss: 0.6323 - acc: 0.7634 - val_loss: 0.9713 - val_acc: 0.6250\n",
            "Epoch 437/500\n",
            "93/93 [==============================] - 0s 168us/step - loss: 0.6308 - acc: 0.7527 - val_loss: 0.9741 - val_acc: 0.6250\n",
            "Epoch 438/500\n",
            "93/93 [==============================] - 0s 150us/step - loss: 0.6292 - acc: 0.7634 - val_loss: 0.9740 - val_acc: 0.6250\n",
            "Epoch 439/500\n",
            "93/93 [==============================] - 0s 159us/step - loss: 0.6300 - acc: 0.7634 - val_loss: 0.9731 - val_acc: 0.6250\n",
            "Epoch 440/500\n",
            "93/93 [==============================] - 0s 137us/step - loss: 0.6280 - acc: 0.7634 - val_loss: 0.9736 - val_acc: 0.6250\n",
            "Epoch 441/500\n",
            "93/93 [==============================] - 0s 138us/step - loss: 0.6268 - acc: 0.7527 - val_loss: 0.9740 - val_acc: 0.6250\n",
            "Epoch 442/500\n",
            "93/93 [==============================] - 0s 162us/step - loss: 0.6263 - acc: 0.7634 - val_loss: 0.9743 - val_acc: 0.6250\n",
            "Epoch 443/500\n",
            "93/93 [==============================] - 0s 144us/step - loss: 0.6276 - acc: 0.7742 - val_loss: 0.9725 - val_acc: 0.6250\n",
            "Epoch 444/500\n",
            "93/93 [==============================] - 0s 141us/step - loss: 0.6252 - acc: 0.7527 - val_loss: 0.9732 - val_acc: 0.6250\n",
            "Epoch 445/500\n",
            "93/93 [==============================] - 0s 168us/step - loss: 0.6263 - acc: 0.7527 - val_loss: 0.9774 - val_acc: 0.6250\n",
            "Epoch 446/500\n",
            "93/93 [==============================] - 0s 139us/step - loss: 0.6244 - acc: 0.7742 - val_loss: 0.9764 - val_acc: 0.6250\n",
            "Epoch 447/500\n",
            "93/93 [==============================] - 0s 154us/step - loss: 0.6227 - acc: 0.7742 - val_loss: 0.9772 - val_acc: 0.6250\n",
            "Epoch 448/500\n",
            "93/93 [==============================] - 0s 155us/step - loss: 0.6232 - acc: 0.7742 - val_loss: 0.9768 - val_acc: 0.6250\n",
            "Epoch 449/500\n",
            "93/93 [==============================] - 0s 175us/step - loss: 0.6223 - acc: 0.7527 - val_loss: 0.9773 - val_acc: 0.6250\n",
            "Epoch 450/500\n",
            "93/93 [==============================] - 0s 165us/step - loss: 0.6227 - acc: 0.7742 - val_loss: 0.9771 - val_acc: 0.6250\n",
            "Epoch 451/500\n",
            "93/93 [==============================] - 0s 160us/step - loss: 0.6208 - acc: 0.7742 - val_loss: 0.9778 - val_acc: 0.6250\n",
            "Epoch 452/500\n",
            "93/93 [==============================] - 0s 149us/step - loss: 0.6202 - acc: 0.7742 - val_loss: 0.9784 - val_acc: 0.6250\n",
            "Epoch 453/500\n",
            "93/93 [==============================] - 0s 150us/step - loss: 0.6189 - acc: 0.7742 - val_loss: 0.9792 - val_acc: 0.6250\n",
            "Epoch 454/500\n",
            "93/93 [==============================] - 0s 154us/step - loss: 0.6195 - acc: 0.7742 - val_loss: 0.9782 - val_acc: 0.6250\n",
            "Epoch 455/500\n",
            "93/93 [==============================] - 0s 149us/step - loss: 0.6180 - acc: 0.7634 - val_loss: 0.9792 - val_acc: 0.6250\n",
            "Epoch 456/500\n",
            "93/93 [==============================] - 0s 157us/step - loss: 0.6176 - acc: 0.7527 - val_loss: 0.9786 - val_acc: 0.6250\n",
            "Epoch 457/500\n",
            "93/93 [==============================] - 0s 141us/step - loss: 0.6163 - acc: 0.7527 - val_loss: 0.9791 - val_acc: 0.6250\n",
            "Epoch 458/500\n",
            "93/93 [==============================] - 0s 158us/step - loss: 0.6161 - acc: 0.7527 - val_loss: 0.9808 - val_acc: 0.6250\n",
            "Epoch 459/500\n",
            "93/93 [==============================] - 0s 144us/step - loss: 0.6150 - acc: 0.7634 - val_loss: 0.9820 - val_acc: 0.6250\n",
            "Epoch 460/500\n",
            "93/93 [==============================] - 0s 171us/step - loss: 0.6145 - acc: 0.7742 - val_loss: 0.9836 - val_acc: 0.6250\n",
            "Epoch 461/500\n",
            "93/93 [==============================] - 0s 146us/step - loss: 0.6135 - acc: 0.7742 - val_loss: 0.9842 - val_acc: 0.6250\n",
            "Epoch 462/500\n",
            "93/93 [==============================] - 0s 138us/step - loss: 0.6146 - acc: 0.7742 - val_loss: 0.9840 - val_acc: 0.6250\n",
            "Epoch 463/500\n",
            "93/93 [==============================] - 0s 189us/step - loss: 0.6139 - acc: 0.7742 - val_loss: 0.9854 - val_acc: 0.6250\n",
            "Epoch 464/500\n",
            "93/93 [==============================] - 0s 193us/step - loss: 0.6118 - acc: 0.7742 - val_loss: 0.9847 - val_acc: 0.6250\n",
            "Epoch 465/500\n",
            "93/93 [==============================] - 0s 146us/step - loss: 0.6109 - acc: 0.7742 - val_loss: 0.9857 - val_acc: 0.6250\n",
            "Epoch 466/500\n",
            "93/93 [==============================] - 0s 164us/step - loss: 0.6103 - acc: 0.7742 - val_loss: 0.9875 - val_acc: 0.6250\n",
            "Epoch 467/500\n",
            "93/93 [==============================] - 0s 154us/step - loss: 0.6103 - acc: 0.7742 - val_loss: 0.9885 - val_acc: 0.6250\n",
            "Epoch 468/500\n",
            "93/93 [==============================] - 0s 146us/step - loss: 0.6093 - acc: 0.7849 - val_loss: 0.9878 - val_acc: 0.6250\n",
            "Epoch 469/500\n",
            "93/93 [==============================] - 0s 154us/step - loss: 0.6082 - acc: 0.7742 - val_loss: 0.9881 - val_acc: 0.6250\n",
            "Epoch 470/500\n",
            "93/93 [==============================] - 0s 146us/step - loss: 0.6078 - acc: 0.7849 - val_loss: 0.9904 - val_acc: 0.6250\n",
            "Epoch 471/500\n",
            "93/93 [==============================] - 0s 147us/step - loss: 0.6086 - acc: 0.7849 - val_loss: 0.9914 - val_acc: 0.6250\n",
            "Epoch 472/500\n",
            "93/93 [==============================] - 0s 147us/step - loss: 0.6070 - acc: 0.7742 - val_loss: 0.9900 - val_acc: 0.6250\n",
            "Epoch 473/500\n",
            "93/93 [==============================] - 0s 155us/step - loss: 0.6059 - acc: 0.7849 - val_loss: 0.9905 - val_acc: 0.6250\n",
            "Epoch 474/500\n",
            "93/93 [==============================] - 0s 142us/step - loss: 0.6051 - acc: 0.7849 - val_loss: 0.9906 - val_acc: 0.6250\n",
            "Epoch 475/500\n",
            "93/93 [==============================] - 0s 140us/step - loss: 0.6050 - acc: 0.7742 - val_loss: 0.9889 - val_acc: 0.6250\n",
            "Epoch 476/500\n",
            "93/93 [==============================] - 0s 140us/step - loss: 0.6037 - acc: 0.7527 - val_loss: 0.9895 - val_acc: 0.6250\n",
            "Epoch 477/500\n",
            "93/93 [==============================] - 0s 170us/step - loss: 0.6029 - acc: 0.7527 - val_loss: 0.9905 - val_acc: 0.6250\n",
            "Epoch 478/500\n",
            "93/93 [==============================] - 0s 166us/step - loss: 0.6050 - acc: 0.7527 - val_loss: 0.9914 - val_acc: 0.6667\n",
            "Epoch 479/500\n",
            "93/93 [==============================] - 0s 185us/step - loss: 0.6017 - acc: 0.7742 - val_loss: 0.9911 - val_acc: 0.6667\n",
            "Epoch 480/500\n",
            "93/93 [==============================] - 0s 196us/step - loss: 0.6014 - acc: 0.7634 - val_loss: 0.9925 - val_acc: 0.6250\n",
            "Epoch 481/500\n",
            "93/93 [==============================] - 0s 187us/step - loss: 0.6002 - acc: 0.7742 - val_loss: 0.9933 - val_acc: 0.6250\n",
            "Epoch 482/500\n",
            "93/93 [==============================] - 0s 156us/step - loss: 0.6010 - acc: 0.7742 - val_loss: 0.9920 - val_acc: 0.6667\n",
            "Epoch 483/500\n",
            "93/93 [==============================] - 0s 146us/step - loss: 0.5989 - acc: 0.7742 - val_loss: 0.9934 - val_acc: 0.6250\n",
            "Epoch 484/500\n",
            "93/93 [==============================] - 0s 168us/step - loss: 0.5981 - acc: 0.7742 - val_loss: 0.9927 - val_acc: 0.6250\n",
            "Epoch 485/500\n",
            "93/93 [==============================] - 0s 140us/step - loss: 0.5983 - acc: 0.7634 - val_loss: 0.9926 - val_acc: 0.6250\n",
            "Epoch 486/500\n",
            "93/93 [==============================] - 0s 164us/step - loss: 0.5979 - acc: 0.7527 - val_loss: 0.9920 - val_acc: 0.6250\n",
            "Epoch 487/500\n",
            "93/93 [==============================] - 0s 166us/step - loss: 0.5969 - acc: 0.7527 - val_loss: 0.9928 - val_acc: 0.6250\n",
            "Epoch 488/500\n",
            "93/93 [==============================] - 0s 115us/step - loss: 0.5959 - acc: 0.7634 - val_loss: 0.9930 - val_acc: 0.6250\n",
            "Epoch 489/500\n",
            "93/93 [==============================] - 0s 156us/step - loss: 0.5971 - acc: 0.7527 - val_loss: 0.9969 - val_acc: 0.6250\n",
            "Epoch 490/500\n",
            "93/93 [==============================] - 0s 131us/step - loss: 0.5940 - acc: 0.7742 - val_loss: 0.9976 - val_acc: 0.6250\n",
            "Epoch 491/500\n",
            "93/93 [==============================] - 0s 161us/step - loss: 0.5934 - acc: 0.7849 - val_loss: 0.9974 - val_acc: 0.6250\n",
            "Epoch 492/500\n",
            "93/93 [==============================] - 0s 143us/step - loss: 0.5942 - acc: 0.7634 - val_loss: 0.9999 - val_acc: 0.6250\n",
            "Epoch 493/500\n",
            "93/93 [==============================] - 0s 142us/step - loss: 0.5929 - acc: 0.7849 - val_loss: 0.9989 - val_acc: 0.6250\n",
            "Epoch 494/500\n",
            "93/93 [==============================] - 0s 138us/step - loss: 0.5914 - acc: 0.7742 - val_loss: 0.9995 - val_acc: 0.6250\n",
            "Epoch 495/500\n",
            "93/93 [==============================] - 0s 147us/step - loss: 0.5908 - acc: 0.7742 - val_loss: 0.9980 - val_acc: 0.6250\n",
            "Epoch 496/500\n",
            "93/93 [==============================] - 0s 144us/step - loss: 0.5906 - acc: 0.7634 - val_loss: 0.9989 - val_acc: 0.6250\n",
            "Epoch 497/500\n",
            "93/93 [==============================] - 0s 151us/step - loss: 0.5897 - acc: 0.7634 - val_loss: 0.9987 - val_acc: 0.6250\n",
            "Epoch 498/500\n",
            "93/93 [==============================] - 0s 141us/step - loss: 0.5889 - acc: 0.7634 - val_loss: 0.9990 - val_acc: 0.6250\n",
            "Epoch 499/500\n",
            "93/93 [==============================] - 0s 149us/step - loss: 0.5890 - acc: 0.7634 - val_loss: 1.0019 - val_acc: 0.6250\n",
            "Epoch 500/500\n",
            "93/93 [==============================] - 0s 161us/step - loss: 0.5882 - acc: 0.7634 - val_loss: 1.0009 - val_acc: 0.6250\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe9d0103048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wN5eV_CvILvB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn_pred=nn.predict(prediction_input_preprocessor.transform(X_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDlYvRxmqlw5",
        "colab_type": "code",
        "outputId": "cb60bad2-b9df-4ccd-9ed7-e8ed7e1c644e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# Predicting vanilla neural network labels\n",
        "\n",
        "print(nn.predict_classes(prediction_input_preprocessor.transform(X_test)))\n",
        "\n",
        "prediction_index=nn.predict_classes(prediction_input_preprocessor.transform(X_test))\n",
        "\n",
        "labels=processed_y_train.columns\n",
        "\n",
        "def index_to_label(labels,index_n): \n",
        "    return labels[index_n]\n",
        "    \n",
        "index_to_label(labels,1)\n",
        "\n",
        "nn_predicted_labels=list(map(lambda x: labels[x], prediction_index))\n",
        "print(nn_predicted_labels)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 0 0 1 4 1 0 0 1 2 1 0 3 2 1 1 4 4 2 0 4 3 1 3 1 2 0 1 0 4 0 2 1 4 2 4 4\n",
            " 4 1]\n",
            "['High', 'Average', 'Average', 'High', 'Very Low', 'High', 'Average', 'Average', 'High', 'Low', 'High', 'Average', 'Very High', 'Low', 'High', 'High', 'Very Low', 'Very Low', 'Low', 'Average', 'Very Low', 'Very High', 'High', 'Very High', 'High', 'Low', 'Average', 'High', 'Average', 'Very Low', 'Average', 'Low', 'High', 'Very Low', 'Low', 'Very Low', 'Very Low', 'Very Low', 'High']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uk30IRXIQAQr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "outputId": "00a93111-ce46-4cf6-eac2-70f232d60148"
      },
      "source": [
        "# Model evaluation for vanilla neural network results\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import pandas as pd\n",
        "from math import sqrt\n",
        "\n",
        "def model_eval_metrics(y_true, y_pred,classification=\"TRUE\"):\n",
        "     if classification==\"TRUE\":\n",
        "        accuracy_eval = accuracy_score(y_true, y_pred)\n",
        "        f1_score_eval = f1_score(y_true, y_pred,average=\"macro\",zero_division=0)\n",
        "        precision_eval = precision_score(y_true, y_pred,average=\"macro\",zero_division=0)\n",
        "        recall_eval = recall_score(y_true, y_pred,average=\"macro\",zero_division=0)\n",
        "        mse_eval = 0\n",
        "        rmse_eval = 0\n",
        "        mae_eval = 0\n",
        "        r2_eval = 0\n",
        "        metricdata = {'accuracy': [accuracy_eval], 'f1_score': [f1_score_eval], 'precision': [precision_eval], 'recall': [recall_eval], 'mse': [mse_eval], 'rmse': [rmse_eval], 'mae': [mae_eval], 'r2': [r2_eval]}\n",
        "        finalmetricdata = pd.DataFrame.from_dict(metricdata)\n",
        "     else:\n",
        "        accuracy_eval = 0\n",
        "        f1_score_eval = 0\n",
        "        precision_eval = 0\n",
        "        recall_eval = 0\n",
        "        mse_eval = mean_squared_error(y_true, y_pred)\n",
        "        rmse_eval = sqrt(mean_squared_error(y_true, y_pred))\n",
        "        mae_eval = mean_absolute_error(y_true, y_pred)\n",
        "        r2_eval = r2_score(y_true, y_pred)\n",
        "        metricdata = {'accuracy': [accuracy_eval], 'f1_score': [f1_score_eval], 'precision': [precision_eval], 'recall': [recall_eval], 'mse': [mse_eval], 'rmse': [rmse_eval], 'mae': [mae_eval], 'r2': [r2_eval]}\n",
        "        finalmetricdata = pd.DataFrame.from_dict(metricdata)\n",
        "     return finalmetricdata\n",
        "\n",
        "model_eval_metrics( y_test,nn_predicted_labels,classification=\"TRUE\")\n",
        "nn_model_eval = model_eval_metrics( y_test,nn_predicted_labels,classification=\"TRUE\")\n",
        "nn_model_eval\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>f1_score</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>mse</th>\n",
              "      <th>rmse</th>\n",
              "      <th>mae</th>\n",
              "      <th>r2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.435897</td>\n",
              "      <td>0.439548</td>\n",
              "      <td>0.511111</td>\n",
              "      <td>0.452778</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   accuracy  f1_score  precision    recall  mse  rmse  mae  r2\n",
              "0  0.435897  0.439548   0.511111  0.452778    0     0    0   0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AA6zD8aVUSxM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ea99bba2-e6b9-47a3-8e0d-713073fc25ab"
      },
      "source": [
        "# NN with dropout regularization \n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout,BatchNormalization\n",
        "import keras\n",
        "from keras.optimizers import SGD\n",
        "nn_dropout = Sequential()\n",
        "nn_dropout.add(Dense(64, input_dim=20, activation='relu'))\n",
        "nn_dropout.add(Dropout(.3))\n",
        "nn_dropout.add(Dense(64, activation='relu'))\n",
        "nn_dropout.add(Dropout(.3))\n",
        "nn_dropout.add(Dense(64, activation='relu'))\n",
        "nn_dropout.add(Dropout(.3))\n",
        "nn_dropout.add(Dense(5, activation='softmax')) \n",
        "                                            \n",
        "nn_dropout.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "\n",
        "nn_dropout.fit(processed_X_train, processed_y_train, epochs = 500)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "117/117 [==============================] - 9s 81ms/step - loss: 1.6299 - acc: 0.2479\n",
            "Epoch 2/500\n",
            "117/117 [==============================] - 0s 353us/step - loss: 1.6269 - acc: 0.2137\n",
            "Epoch 3/500\n",
            "117/117 [==============================] - 0s 357us/step - loss: 1.5973 - acc: 0.2479\n",
            "Epoch 4/500\n",
            "117/117 [==============================] - 0s 416us/step - loss: 1.5759 - acc: 0.2821\n",
            "Epoch 5/500\n",
            "117/117 [==============================] - 0s 378us/step - loss: 1.5907 - acc: 0.2479\n",
            "Epoch 6/500\n",
            "117/117 [==============================] - 0s 383us/step - loss: 1.5863 - acc: 0.1966\n",
            "Epoch 7/500\n",
            "117/117 [==============================] - 0s 414us/step - loss: 1.6026 - acc: 0.2735\n",
            "Epoch 8/500\n",
            "117/117 [==============================] - 0s 375us/step - loss: 1.5672 - acc: 0.2735\n",
            "Epoch 9/500\n",
            "117/117 [==============================] - 0s 413us/step - loss: 1.5462 - acc: 0.2906\n",
            "Epoch 10/500\n",
            "117/117 [==============================] - 0s 390us/step - loss: 1.5656 - acc: 0.2735\n",
            "Epoch 11/500\n",
            "117/117 [==============================] - 0s 378us/step - loss: 1.5430 - acc: 0.3077\n",
            "Epoch 12/500\n",
            "117/117 [==============================] - 0s 384us/step - loss: 1.5351 - acc: 0.3077\n",
            "Epoch 13/500\n",
            "117/117 [==============================] - 0s 414us/step - loss: 1.4962 - acc: 0.3077\n",
            "Epoch 14/500\n",
            "117/117 [==============================] - 0s 469us/step - loss: 1.5406 - acc: 0.3333\n",
            "Epoch 15/500\n",
            "117/117 [==============================] - 0s 426us/step - loss: 1.5134 - acc: 0.3419\n",
            "Epoch 16/500\n",
            "117/117 [==============================] - 0s 341us/step - loss: 1.5323 - acc: 0.2735\n",
            "Epoch 17/500\n",
            "117/117 [==============================] - 0s 353us/step - loss: 1.5429 - acc: 0.2991\n",
            "Epoch 18/500\n",
            "117/117 [==============================] - 0s 358us/step - loss: 1.5259 - acc: 0.3761\n",
            "Epoch 19/500\n",
            "117/117 [==============================] - 0s 364us/step - loss: 1.4920 - acc: 0.3590\n",
            "Epoch 20/500\n",
            "117/117 [==============================] - 0s 433us/step - loss: 1.4947 - acc: 0.3419\n",
            "Epoch 21/500\n",
            "117/117 [==============================] - 0s 344us/step - loss: 1.5176 - acc: 0.3333\n",
            "Epoch 22/500\n",
            "117/117 [==============================] - 0s 331us/step - loss: 1.5181 - acc: 0.3675\n",
            "Epoch 23/500\n",
            "117/117 [==============================] - 0s 316us/step - loss: 1.4963 - acc: 0.3590\n",
            "Epoch 24/500\n",
            "117/117 [==============================] - 0s 366us/step - loss: 1.4760 - acc: 0.4017\n",
            "Epoch 25/500\n",
            "117/117 [==============================] - 0s 409us/step - loss: 1.4852 - acc: 0.3932\n",
            "Epoch 26/500\n",
            "117/117 [==============================] - 0s 427us/step - loss: 1.4624 - acc: 0.3761\n",
            "Epoch 27/500\n",
            "117/117 [==============================] - 0s 393us/step - loss: 1.4511 - acc: 0.4103\n",
            "Epoch 28/500\n",
            "117/117 [==============================] - 0s 394us/step - loss: 1.4550 - acc: 0.4103\n",
            "Epoch 29/500\n",
            "117/117 [==============================] - 0s 389us/step - loss: 1.4512 - acc: 0.3932\n",
            "Epoch 30/500\n",
            "117/117 [==============================] - 0s 399us/step - loss: 1.4619 - acc: 0.3932\n",
            "Epoch 31/500\n",
            "117/117 [==============================] - 0s 353us/step - loss: 1.4593 - acc: 0.3675\n",
            "Epoch 32/500\n",
            "117/117 [==============================] - 0s 353us/step - loss: 1.4327 - acc: 0.4274\n",
            "Epoch 33/500\n",
            "117/117 [==============================] - 0s 400us/step - loss: 1.4684 - acc: 0.4103\n",
            "Epoch 34/500\n",
            "117/117 [==============================] - 0s 366us/step - loss: 1.4420 - acc: 0.4017\n",
            "Epoch 35/500\n",
            "117/117 [==============================] - 0s 395us/step - loss: 1.3990 - acc: 0.4530\n",
            "Epoch 36/500\n",
            "117/117 [==============================] - 0s 373us/step - loss: 1.4147 - acc: 0.4615\n",
            "Epoch 37/500\n",
            "117/117 [==============================] - 0s 435us/step - loss: 1.3642 - acc: 0.4615\n",
            "Epoch 38/500\n",
            "117/117 [==============================] - 0s 400us/step - loss: 1.4557 - acc: 0.4103\n",
            "Epoch 39/500\n",
            "117/117 [==============================] - 0s 384us/step - loss: 1.4040 - acc: 0.4274\n",
            "Epoch 40/500\n",
            "117/117 [==============================] - 0s 386us/step - loss: 1.3788 - acc: 0.4017\n",
            "Epoch 41/500\n",
            "117/117 [==============================] - 0s 385us/step - loss: 1.3730 - acc: 0.4872\n",
            "Epoch 42/500\n",
            "117/117 [==============================] - 0s 383us/step - loss: 1.4198 - acc: 0.3846\n",
            "Epoch 43/500\n",
            "117/117 [==============================] - 0s 373us/step - loss: 1.3982 - acc: 0.4444\n",
            "Epoch 44/500\n",
            "117/117 [==============================] - 0s 377us/step - loss: 1.3879 - acc: 0.3846\n",
            "Epoch 45/500\n",
            "117/117 [==============================] - 0s 403us/step - loss: 1.4015 - acc: 0.4786\n",
            "Epoch 46/500\n",
            "117/117 [==============================] - 0s 428us/step - loss: 1.3933 - acc: 0.3846\n",
            "Epoch 47/500\n",
            "117/117 [==============================] - 0s 397us/step - loss: 1.3454 - acc: 0.5128\n",
            "Epoch 48/500\n",
            "117/117 [==============================] - 0s 364us/step - loss: 1.3450 - acc: 0.4786\n",
            "Epoch 49/500\n",
            "117/117 [==============================] - 0s 367us/step - loss: 1.3592 - acc: 0.4530\n",
            "Epoch 50/500\n",
            "117/117 [==============================] - 0s 422us/step - loss: 1.3435 - acc: 0.5214\n",
            "Epoch 51/500\n",
            "117/117 [==============================] - 0s 383us/step - loss: 1.3429 - acc: 0.4530\n",
            "Epoch 52/500\n",
            "117/117 [==============================] - 0s 357us/step - loss: 1.3206 - acc: 0.4786\n",
            "Epoch 53/500\n",
            "117/117 [==============================] - 0s 400us/step - loss: 1.3260 - acc: 0.4872\n",
            "Epoch 54/500\n",
            "117/117 [==============================] - 0s 389us/step - loss: 1.3153 - acc: 0.4786\n",
            "Epoch 55/500\n",
            "117/117 [==============================] - 0s 378us/step - loss: 1.3261 - acc: 0.4615\n",
            "Epoch 56/500\n",
            "117/117 [==============================] - 0s 366us/step - loss: 1.2873 - acc: 0.5043\n",
            "Epoch 57/500\n",
            "117/117 [==============================] - 0s 331us/step - loss: 1.3163 - acc: 0.4444\n",
            "Epoch 58/500\n",
            "117/117 [==============================] - 0s 365us/step - loss: 1.2957 - acc: 0.4701\n",
            "Epoch 59/500\n",
            "117/117 [==============================] - 0s 343us/step - loss: 1.2760 - acc: 0.4786\n",
            "Epoch 60/500\n",
            "117/117 [==============================] - 0s 386us/step - loss: 1.2864 - acc: 0.4957\n",
            "Epoch 61/500\n",
            "117/117 [==============================] - 0s 367us/step - loss: 1.2222 - acc: 0.5385\n",
            "Epoch 62/500\n",
            "117/117 [==============================] - 0s 355us/step - loss: 1.2869 - acc: 0.4530\n",
            "Epoch 63/500\n",
            "117/117 [==============================] - 0s 360us/step - loss: 1.2774 - acc: 0.4615\n",
            "Epoch 64/500\n",
            "117/117 [==============================] - 0s 354us/step - loss: 1.2979 - acc: 0.4615\n",
            "Epoch 65/500\n",
            "117/117 [==============================] - 0s 373us/step - loss: 1.3267 - acc: 0.4615\n",
            "Epoch 66/500\n",
            "117/117 [==============================] - 0s 357us/step - loss: 1.2327 - acc: 0.4786\n",
            "Epoch 67/500\n",
            "117/117 [==============================] - 0s 379us/step - loss: 1.2688 - acc: 0.4786\n",
            "Epoch 68/500\n",
            "117/117 [==============================] - 0s 345us/step - loss: 1.2900 - acc: 0.5043\n",
            "Epoch 69/500\n",
            "117/117 [==============================] - 0s 328us/step - loss: 1.3443 - acc: 0.4359\n",
            "Epoch 70/500\n",
            "117/117 [==============================] - 0s 332us/step - loss: 1.2677 - acc: 0.4530\n",
            "Epoch 71/500\n",
            "117/117 [==============================] - 0s 391us/step - loss: 1.2536 - acc: 0.4530\n",
            "Epoch 72/500\n",
            "117/117 [==============================] - 0s 373us/step - loss: 1.2648 - acc: 0.4530\n",
            "Epoch 73/500\n",
            "117/117 [==============================] - 0s 341us/step - loss: 1.2567 - acc: 0.4786\n",
            "Epoch 74/500\n",
            "117/117 [==============================] - 0s 347us/step - loss: 1.2541 - acc: 0.4872\n",
            "Epoch 75/500\n",
            "117/117 [==============================] - 0s 391us/step - loss: 1.2248 - acc: 0.4701\n",
            "Epoch 76/500\n",
            "117/117 [==============================] - 0s 370us/step - loss: 1.2190 - acc: 0.5043\n",
            "Epoch 77/500\n",
            "117/117 [==============================] - 0s 319us/step - loss: 1.2046 - acc: 0.5043\n",
            "Epoch 78/500\n",
            "117/117 [==============================] - 0s 363us/step - loss: 1.1776 - acc: 0.5299\n",
            "Epoch 79/500\n",
            "117/117 [==============================] - 0s 394us/step - loss: 1.2369 - acc: 0.5043\n",
            "Epoch 80/500\n",
            "117/117 [==============================] - 0s 422us/step - loss: 1.1728 - acc: 0.5470\n",
            "Epoch 81/500\n",
            "117/117 [==============================] - 0s 405us/step - loss: 1.2362 - acc: 0.5043\n",
            "Epoch 82/500\n",
            "117/117 [==============================] - 0s 379us/step - loss: 1.2220 - acc: 0.4786\n",
            "Epoch 83/500\n",
            "117/117 [==============================] - 0s 406us/step - loss: 1.2358 - acc: 0.5299\n",
            "Epoch 84/500\n",
            "117/117 [==============================] - 0s 390us/step - loss: 1.2240 - acc: 0.4274\n",
            "Epoch 85/500\n",
            "117/117 [==============================] - 0s 414us/step - loss: 1.1673 - acc: 0.5043\n",
            "Epoch 86/500\n",
            "117/117 [==============================] - 0s 395us/step - loss: 1.1962 - acc: 0.4872\n",
            "Epoch 87/500\n",
            "117/117 [==============================] - 0s 402us/step - loss: 1.2420 - acc: 0.4786\n",
            "Epoch 88/500\n",
            "117/117 [==============================] - 0s 384us/step - loss: 1.1540 - acc: 0.5214\n",
            "Epoch 89/500\n",
            "117/117 [==============================] - 0s 408us/step - loss: 1.2193 - acc: 0.4872\n",
            "Epoch 90/500\n",
            "117/117 [==============================] - 0s 389us/step - loss: 1.2283 - acc: 0.5385\n",
            "Epoch 91/500\n",
            "117/117 [==============================] - 0s 378us/step - loss: 1.1578 - acc: 0.4957\n",
            "Epoch 92/500\n",
            "117/117 [==============================] - 0s 364us/step - loss: 1.1549 - acc: 0.5470\n",
            "Epoch 93/500\n",
            "117/117 [==============================] - 0s 382us/step - loss: 1.2009 - acc: 0.4957\n",
            "Epoch 94/500\n",
            "117/117 [==============================] - 0s 352us/step - loss: 1.1804 - acc: 0.5128\n",
            "Epoch 95/500\n",
            "117/117 [==============================] - 0s 411us/step - loss: 1.1402 - acc: 0.4872\n",
            "Epoch 96/500\n",
            "117/117 [==============================] - 0s 368us/step - loss: 1.1922 - acc: 0.5385\n",
            "Epoch 97/500\n",
            "117/117 [==============================] - 0s 337us/step - loss: 1.1891 - acc: 0.4957\n",
            "Epoch 98/500\n",
            "117/117 [==============================] - 0s 406us/step - loss: 1.1482 - acc: 0.5214\n",
            "Epoch 99/500\n",
            "117/117 [==============================] - 0s 443us/step - loss: 1.2068 - acc: 0.5043\n",
            "Epoch 100/500\n",
            "117/117 [==============================] - 0s 373us/step - loss: 1.0984 - acc: 0.5385\n",
            "Epoch 101/500\n",
            "117/117 [==============================] - 0s 403us/step - loss: 1.1220 - acc: 0.5385\n",
            "Epoch 102/500\n",
            "117/117 [==============================] - 0s 456us/step - loss: 1.1574 - acc: 0.5299\n",
            "Epoch 103/500\n",
            "117/117 [==============================] - 0s 345us/step - loss: 1.1251 - acc: 0.5043\n",
            "Epoch 104/500\n",
            "117/117 [==============================] - 0s 400us/step - loss: 1.1597 - acc: 0.5128\n",
            "Epoch 105/500\n",
            "117/117 [==============================] - 0s 407us/step - loss: 1.0862 - acc: 0.5641\n",
            "Epoch 106/500\n",
            "117/117 [==============================] - 0s 376us/step - loss: 1.0879 - acc: 0.5299\n",
            "Epoch 107/500\n",
            "117/117 [==============================] - 0s 362us/step - loss: 1.1404 - acc: 0.5470\n",
            "Epoch 108/500\n",
            "117/117 [==============================] - 0s 396us/step - loss: 1.1145 - acc: 0.5641\n",
            "Epoch 109/500\n",
            "117/117 [==============================] - 0s 361us/step - loss: 1.1384 - acc: 0.5214\n",
            "Epoch 110/500\n",
            "117/117 [==============================] - 0s 366us/step - loss: 1.0838 - acc: 0.5641\n",
            "Epoch 111/500\n",
            "117/117 [==============================] - 0s 423us/step - loss: 1.1193 - acc: 0.4786\n",
            "Epoch 112/500\n",
            "117/117 [==============================] - 0s 453us/step - loss: 1.1195 - acc: 0.5128\n",
            "Epoch 113/500\n",
            "117/117 [==============================] - 0s 380us/step - loss: 1.1399 - acc: 0.5214\n",
            "Epoch 114/500\n",
            "117/117 [==============================] - 0s 391us/step - loss: 1.0518 - acc: 0.5470\n",
            "Epoch 115/500\n",
            "117/117 [==============================] - 0s 366us/step - loss: 1.1082 - acc: 0.5299\n",
            "Epoch 116/500\n",
            "117/117 [==============================] - 0s 389us/step - loss: 1.1468 - acc: 0.5385\n",
            "Epoch 117/500\n",
            "117/117 [==============================] - 0s 394us/step - loss: 1.0913 - acc: 0.5214\n",
            "Epoch 118/500\n",
            "117/117 [==============================] - 0s 413us/step - loss: 1.1414 - acc: 0.5128\n",
            "Epoch 119/500\n",
            "117/117 [==============================] - 0s 356us/step - loss: 1.0872 - acc: 0.5299\n",
            "Epoch 120/500\n",
            "117/117 [==============================] - 0s 415us/step - loss: 1.1463 - acc: 0.5128\n",
            "Epoch 121/500\n",
            "117/117 [==============================] - 0s 333us/step - loss: 1.0798 - acc: 0.5299\n",
            "Epoch 122/500\n",
            "117/117 [==============================] - 0s 363us/step - loss: 1.0675 - acc: 0.5556\n",
            "Epoch 123/500\n",
            "117/117 [==============================] - 0s 383us/step - loss: 1.0693 - acc: 0.5385\n",
            "Epoch 124/500\n",
            "117/117 [==============================] - 0s 426us/step - loss: 1.0542 - acc: 0.5897\n",
            "Epoch 125/500\n",
            "117/117 [==============================] - 0s 296us/step - loss: 1.1156 - acc: 0.5214\n",
            "Epoch 126/500\n",
            "117/117 [==============================] - 0s 418us/step - loss: 1.0733 - acc: 0.5812\n",
            "Epoch 127/500\n",
            "117/117 [==============================] - 0s 401us/step - loss: 1.0714 - acc: 0.5556\n",
            "Epoch 128/500\n",
            "117/117 [==============================] - 0s 413us/step - loss: 1.1531 - acc: 0.5128\n",
            "Epoch 129/500\n",
            "117/117 [==============================] - 0s 376us/step - loss: 1.1089 - acc: 0.5556\n",
            "Epoch 130/500\n",
            "117/117 [==============================] - 0s 380us/step - loss: 1.0755 - acc: 0.5470\n",
            "Epoch 131/500\n",
            "117/117 [==============================] - 0s 363us/step - loss: 1.0315 - acc: 0.5556\n",
            "Epoch 132/500\n",
            "117/117 [==============================] - 0s 389us/step - loss: 1.1175 - acc: 0.5299\n",
            "Epoch 133/500\n",
            "117/117 [==============================] - 0s 356us/step - loss: 1.0617 - acc: 0.4957\n",
            "Epoch 134/500\n",
            "117/117 [==============================] - 0s 405us/step - loss: 1.1344 - acc: 0.5556\n",
            "Epoch 135/500\n",
            "117/117 [==============================] - 0s 386us/step - loss: 1.0867 - acc: 0.5641\n",
            "Epoch 136/500\n",
            "117/117 [==============================] - 0s 385us/step - loss: 1.0555 - acc: 0.5214\n",
            "Epoch 137/500\n",
            "117/117 [==============================] - 0s 379us/step - loss: 1.1091 - acc: 0.5043\n",
            "Epoch 138/500\n",
            "117/117 [==============================] - 0s 366us/step - loss: 1.0734 - acc: 0.5641\n",
            "Epoch 139/500\n",
            "117/117 [==============================] - 0s 375us/step - loss: 1.0770 - acc: 0.5812\n",
            "Epoch 140/500\n",
            "117/117 [==============================] - 0s 411us/step - loss: 1.0428 - acc: 0.5470\n",
            "Epoch 141/500\n",
            "117/117 [==============================] - 0s 412us/step - loss: 1.1042 - acc: 0.4957\n",
            "Epoch 142/500\n",
            "117/117 [==============================] - 0s 404us/step - loss: 1.0611 - acc: 0.5299\n",
            "Epoch 143/500\n",
            "117/117 [==============================] - 0s 445us/step - loss: 1.0538 - acc: 0.5983\n",
            "Epoch 144/500\n",
            "117/117 [==============================] - 0s 385us/step - loss: 1.0359 - acc: 0.5299\n",
            "Epoch 145/500\n",
            "117/117 [==============================] - 0s 398us/step - loss: 1.1168 - acc: 0.5299\n",
            "Epoch 146/500\n",
            "117/117 [==============================] - 0s 423us/step - loss: 1.0637 - acc: 0.5556\n",
            "Epoch 147/500\n",
            "117/117 [==============================] - 0s 378us/step - loss: 1.0716 - acc: 0.5043\n",
            "Epoch 148/500\n",
            "117/117 [==============================] - 0s 443us/step - loss: 1.0783 - acc: 0.5470\n",
            "Epoch 149/500\n",
            "117/117 [==============================] - 0s 372us/step - loss: 0.9893 - acc: 0.5897\n",
            "Epoch 150/500\n",
            "117/117 [==============================] - 0s 376us/step - loss: 1.0729 - acc: 0.5385\n",
            "Epoch 151/500\n",
            "117/117 [==============================] - 0s 386us/step - loss: 1.0048 - acc: 0.5641\n",
            "Epoch 152/500\n",
            "117/117 [==============================] - 0s 381us/step - loss: 0.9938 - acc: 0.6068\n",
            "Epoch 153/500\n",
            "117/117 [==============================] - 0s 403us/step - loss: 1.0386 - acc: 0.5726\n",
            "Epoch 154/500\n",
            "117/117 [==============================] - 0s 463us/step - loss: 1.0070 - acc: 0.5897\n",
            "Epoch 155/500\n",
            "117/117 [==============================] - 0s 346us/step - loss: 1.0523 - acc: 0.5641\n",
            "Epoch 156/500\n",
            "117/117 [==============================] - 0s 351us/step - loss: 1.0108 - acc: 0.5641\n",
            "Epoch 157/500\n",
            "117/117 [==============================] - 0s 357us/step - loss: 1.0632 - acc: 0.6068\n",
            "Epoch 158/500\n",
            "117/117 [==============================] - 0s 331us/step - loss: 1.0265 - acc: 0.5641\n",
            "Epoch 159/500\n",
            "117/117 [==============================] - 0s 321us/step - loss: 1.0187 - acc: 0.5385\n",
            "Epoch 160/500\n",
            "117/117 [==============================] - 0s 414us/step - loss: 1.0705 - acc: 0.5812\n",
            "Epoch 161/500\n",
            "117/117 [==============================] - 0s 479us/step - loss: 0.9874 - acc: 0.5726\n",
            "Epoch 162/500\n",
            "117/117 [==============================] - 0s 427us/step - loss: 1.0396 - acc: 0.5812\n",
            "Epoch 163/500\n",
            "117/117 [==============================] - 0s 482us/step - loss: 1.0232 - acc: 0.5385\n",
            "Epoch 164/500\n",
            "117/117 [==============================] - 0s 354us/step - loss: 0.9811 - acc: 0.5812\n",
            "Epoch 165/500\n",
            "117/117 [==============================] - 0s 363us/step - loss: 1.0526 - acc: 0.5897\n",
            "Epoch 166/500\n",
            "117/117 [==============================] - 0s 381us/step - loss: 0.9807 - acc: 0.5812\n",
            "Epoch 167/500\n",
            "117/117 [==============================] - 0s 385us/step - loss: 1.0850 - acc: 0.5385\n",
            "Epoch 168/500\n",
            "117/117 [==============================] - 0s 402us/step - loss: 1.0518 - acc: 0.5556\n",
            "Epoch 169/500\n",
            "117/117 [==============================] - 0s 481us/step - loss: 1.0254 - acc: 0.5641\n",
            "Epoch 170/500\n",
            "117/117 [==============================] - 0s 376us/step - loss: 0.9484 - acc: 0.6068\n",
            "Epoch 171/500\n",
            "117/117 [==============================] - 0s 395us/step - loss: 0.9902 - acc: 0.5897\n",
            "Epoch 172/500\n",
            "117/117 [==============================] - 0s 413us/step - loss: 1.0154 - acc: 0.5726\n",
            "Epoch 173/500\n",
            "117/117 [==============================] - 0s 362us/step - loss: 0.9832 - acc: 0.5641\n",
            "Epoch 174/500\n",
            "117/117 [==============================] - 0s 387us/step - loss: 1.0129 - acc: 0.5641\n",
            "Epoch 175/500\n",
            "117/117 [==============================] - 0s 456us/step - loss: 1.0327 - acc: 0.5128\n",
            "Epoch 176/500\n",
            "117/117 [==============================] - 0s 390us/step - loss: 0.9662 - acc: 0.5983\n",
            "Epoch 177/500\n",
            "117/117 [==============================] - 0s 408us/step - loss: 1.0047 - acc: 0.5556\n",
            "Epoch 178/500\n",
            "117/117 [==============================] - 0s 441us/step - loss: 1.0119 - acc: 0.5556\n",
            "Epoch 179/500\n",
            "117/117 [==============================] - 0s 509us/step - loss: 1.0414 - acc: 0.5812\n",
            "Epoch 180/500\n",
            "117/117 [==============================] - 0s 446us/step - loss: 1.0107 - acc: 0.6068\n",
            "Epoch 181/500\n",
            "117/117 [==============================] - 0s 402us/step - loss: 0.9981 - acc: 0.5812\n",
            "Epoch 182/500\n",
            "117/117 [==============================] - 0s 351us/step - loss: 1.0272 - acc: 0.5470\n",
            "Epoch 183/500\n",
            "117/117 [==============================] - 0s 376us/step - loss: 0.9381 - acc: 0.6239\n",
            "Epoch 184/500\n",
            "117/117 [==============================] - 0s 359us/step - loss: 1.0288 - acc: 0.5556\n",
            "Epoch 185/500\n",
            "117/117 [==============================] - 0s 329us/step - loss: 1.0287 - acc: 0.5983\n",
            "Epoch 186/500\n",
            "117/117 [==============================] - 0s 332us/step - loss: 0.9812 - acc: 0.5983\n",
            "Epoch 187/500\n",
            "117/117 [==============================] - 0s 395us/step - loss: 0.9465 - acc: 0.6154\n",
            "Epoch 188/500\n",
            "117/117 [==============================] - 0s 337us/step - loss: 0.9854 - acc: 0.5641\n",
            "Epoch 189/500\n",
            "117/117 [==============================] - 0s 522us/step - loss: 1.0115 - acc: 0.5812\n",
            "Epoch 190/500\n",
            "117/117 [==============================] - 0s 451us/step - loss: 0.9782 - acc: 0.6068\n",
            "Epoch 191/500\n",
            "117/117 [==============================] - 0s 306us/step - loss: 0.9481 - acc: 0.5812\n",
            "Epoch 192/500\n",
            "117/117 [==============================] - 0s 365us/step - loss: 0.9967 - acc: 0.5556\n",
            "Epoch 193/500\n",
            "117/117 [==============================] - 0s 418us/step - loss: 0.9816 - acc: 0.6154\n",
            "Epoch 194/500\n",
            "117/117 [==============================] - 0s 408us/step - loss: 0.9849 - acc: 0.6068\n",
            "Epoch 195/500\n",
            "117/117 [==============================] - 0s 342us/step - loss: 0.9961 - acc: 0.5214\n",
            "Epoch 196/500\n",
            "117/117 [==============================] - 0s 436us/step - loss: 0.9969 - acc: 0.5641\n",
            "Epoch 197/500\n",
            "117/117 [==============================] - 0s 390us/step - loss: 0.9880 - acc: 0.5470\n",
            "Epoch 198/500\n",
            "117/117 [==============================] - 0s 434us/step - loss: 0.9845 - acc: 0.6068\n",
            "Epoch 199/500\n",
            "117/117 [==============================] - 0s 390us/step - loss: 0.9615 - acc: 0.5897\n",
            "Epoch 200/500\n",
            "117/117 [==============================] - 0s 336us/step - loss: 1.0167 - acc: 0.6068\n",
            "Epoch 201/500\n",
            "117/117 [==============================] - 0s 439us/step - loss: 0.9830 - acc: 0.5812\n",
            "Epoch 202/500\n",
            "117/117 [==============================] - 0s 406us/step - loss: 1.0413 - acc: 0.5641\n",
            "Epoch 203/500\n",
            "117/117 [==============================] - 0s 389us/step - loss: 0.9385 - acc: 0.6325\n",
            "Epoch 204/500\n",
            "117/117 [==============================] - 0s 411us/step - loss: 0.9963 - acc: 0.6068\n",
            "Epoch 205/500\n",
            "117/117 [==============================] - 0s 413us/step - loss: 0.9426 - acc: 0.5897\n",
            "Epoch 206/500\n",
            "117/117 [==============================] - 0s 373us/step - loss: 0.9650 - acc: 0.5812\n",
            "Epoch 207/500\n",
            "117/117 [==============================] - 0s 333us/step - loss: 1.0381 - acc: 0.5299\n",
            "Epoch 208/500\n",
            "117/117 [==============================] - 0s 324us/step - loss: 0.9915 - acc: 0.5983\n",
            "Epoch 209/500\n",
            "117/117 [==============================] - 0s 399us/step - loss: 0.9669 - acc: 0.6068\n",
            "Epoch 210/500\n",
            "117/117 [==============================] - 0s 351us/step - loss: 1.0109 - acc: 0.5726\n",
            "Epoch 211/500\n",
            "117/117 [==============================] - 0s 403us/step - loss: 1.0032 - acc: 0.5983\n",
            "Epoch 212/500\n",
            "117/117 [==============================] - 0s 384us/step - loss: 0.9471 - acc: 0.6239\n",
            "Epoch 213/500\n",
            "117/117 [==============================] - 0s 485us/step - loss: 0.9710 - acc: 0.6239\n",
            "Epoch 214/500\n",
            "117/117 [==============================] - 0s 426us/step - loss: 0.9687 - acc: 0.5983\n",
            "Epoch 215/500\n",
            "117/117 [==============================] - 0s 365us/step - loss: 0.9415 - acc: 0.6068\n",
            "Epoch 216/500\n",
            "117/117 [==============================] - 0s 409us/step - loss: 0.9979 - acc: 0.5983\n",
            "Epoch 217/500\n",
            "117/117 [==============================] - 0s 428us/step - loss: 0.9527 - acc: 0.5897\n",
            "Epoch 218/500\n",
            "117/117 [==============================] - 0s 356us/step - loss: 0.9688 - acc: 0.6068\n",
            "Epoch 219/500\n",
            "117/117 [==============================] - 0s 422us/step - loss: 0.9473 - acc: 0.6325\n",
            "Epoch 220/500\n",
            "117/117 [==============================] - 0s 495us/step - loss: 0.9440 - acc: 0.5897\n",
            "Epoch 221/500\n",
            "117/117 [==============================] - 0s 389us/step - loss: 1.0108 - acc: 0.5470\n",
            "Epoch 222/500\n",
            "117/117 [==============================] - 0s 338us/step - loss: 0.9068 - acc: 0.6581\n",
            "Epoch 223/500\n",
            "117/117 [==============================] - 0s 447us/step - loss: 0.9511 - acc: 0.5641\n",
            "Epoch 224/500\n",
            "117/117 [==============================] - 0s 457us/step - loss: 0.9429 - acc: 0.6325\n",
            "Epoch 225/500\n",
            "117/117 [==============================] - 0s 379us/step - loss: 0.9275 - acc: 0.6154\n",
            "Epoch 226/500\n",
            "117/117 [==============================] - 0s 397us/step - loss: 0.9790 - acc: 0.5385\n",
            "Epoch 227/500\n",
            "117/117 [==============================] - 0s 349us/step - loss: 0.9911 - acc: 0.5214\n",
            "Epoch 228/500\n",
            "117/117 [==============================] - 0s 371us/step - loss: 0.9568 - acc: 0.6581\n",
            "Epoch 229/500\n",
            "117/117 [==============================] - 0s 310us/step - loss: 0.9843 - acc: 0.6154\n",
            "Epoch 230/500\n",
            "117/117 [==============================] - 0s 323us/step - loss: 0.9226 - acc: 0.5897\n",
            "Epoch 231/500\n",
            "117/117 [==============================] - 0s 416us/step - loss: 0.9824 - acc: 0.6325\n",
            "Epoch 232/500\n",
            "117/117 [==============================] - 0s 333us/step - loss: 0.9124 - acc: 0.6068\n",
            "Epoch 233/500\n",
            "117/117 [==============================] - 0s 395us/step - loss: 0.9331 - acc: 0.5726\n",
            "Epoch 234/500\n",
            "117/117 [==============================] - 0s 351us/step - loss: 0.9135 - acc: 0.5897\n",
            "Epoch 235/500\n",
            "117/117 [==============================] - 0s 355us/step - loss: 0.9685 - acc: 0.5812\n",
            "Epoch 236/500\n",
            "117/117 [==============================] - 0s 316us/step - loss: 0.9203 - acc: 0.5983\n",
            "Epoch 237/500\n",
            "117/117 [==============================] - 0s 342us/step - loss: 0.9220 - acc: 0.5726\n",
            "Epoch 238/500\n",
            "117/117 [==============================] - 0s 390us/step - loss: 0.9382 - acc: 0.5897\n",
            "Epoch 239/500\n",
            "117/117 [==============================] - 0s 334us/step - loss: 1.0228 - acc: 0.5726\n",
            "Epoch 240/500\n",
            "117/117 [==============================] - 0s 391us/step - loss: 0.9218 - acc: 0.6068\n",
            "Epoch 241/500\n",
            "117/117 [==============================] - 0s 392us/step - loss: 0.9400 - acc: 0.5641\n",
            "Epoch 242/500\n",
            "117/117 [==============================] - 0s 339us/step - loss: 0.9118 - acc: 0.6410\n",
            "Epoch 243/500\n",
            "117/117 [==============================] - 0s 397us/step - loss: 0.9275 - acc: 0.6154\n",
            "Epoch 244/500\n",
            "117/117 [==============================] - 0s 397us/step - loss: 0.9095 - acc: 0.6325\n",
            "Epoch 245/500\n",
            "117/117 [==============================] - 0s 329us/step - loss: 0.9313 - acc: 0.6068\n",
            "Epoch 246/500\n",
            "117/117 [==============================] - 0s 322us/step - loss: 0.8974 - acc: 0.6581\n",
            "Epoch 247/500\n",
            "117/117 [==============================] - 0s 351us/step - loss: 0.8853 - acc: 0.6325\n",
            "Epoch 248/500\n",
            "117/117 [==============================] - 0s 277us/step - loss: 0.9131 - acc: 0.6068\n",
            "Epoch 249/500\n",
            "117/117 [==============================] - 0s 348us/step - loss: 0.9395 - acc: 0.6325\n",
            "Epoch 250/500\n",
            "117/117 [==============================] - 0s 335us/step - loss: 0.9185 - acc: 0.5556\n",
            "Epoch 251/500\n",
            "117/117 [==============================] - 0s 327us/step - loss: 0.8700 - acc: 0.6325\n",
            "Epoch 252/500\n",
            "117/117 [==============================] - 0s 381us/step - loss: 0.9089 - acc: 0.6410\n",
            "Epoch 253/500\n",
            "117/117 [==============================] - 0s 322us/step - loss: 0.9436 - acc: 0.6239\n",
            "Epoch 254/500\n",
            "117/117 [==============================] - 0s 350us/step - loss: 0.9127 - acc: 0.6496\n",
            "Epoch 255/500\n",
            "117/117 [==============================] - 0s 338us/step - loss: 0.8928 - acc: 0.6325\n",
            "Epoch 256/500\n",
            "117/117 [==============================] - 0s 360us/step - loss: 0.9813 - acc: 0.5556\n",
            "Epoch 257/500\n",
            "117/117 [==============================] - 0s 344us/step - loss: 0.9490 - acc: 0.6410\n",
            "Epoch 258/500\n",
            "117/117 [==============================] - 0s 375us/step - loss: 0.9423 - acc: 0.5812\n",
            "Epoch 259/500\n",
            "117/117 [==============================] - 0s 326us/step - loss: 0.9178 - acc: 0.6068\n",
            "Epoch 260/500\n",
            "117/117 [==============================] - 0s 364us/step - loss: 0.8957 - acc: 0.6239\n",
            "Epoch 261/500\n",
            "117/117 [==============================] - 0s 359us/step - loss: 0.8880 - acc: 0.6068\n",
            "Epoch 262/500\n",
            "117/117 [==============================] - 0s 340us/step - loss: 0.9564 - acc: 0.5385\n",
            "Epoch 263/500\n",
            "117/117 [==============================] - 0s 348us/step - loss: 0.9128 - acc: 0.6410\n",
            "Epoch 264/500\n",
            "117/117 [==============================] - 0s 340us/step - loss: 0.9119 - acc: 0.6068\n",
            "Epoch 265/500\n",
            "117/117 [==============================] - 0s 321us/step - loss: 0.9068 - acc: 0.6068\n",
            "Epoch 266/500\n",
            "117/117 [==============================] - 0s 379us/step - loss: 0.9164 - acc: 0.5983\n",
            "Epoch 267/500\n",
            "117/117 [==============================] - 0s 324us/step - loss: 0.9777 - acc: 0.5983\n",
            "Epoch 268/500\n",
            "117/117 [==============================] - 0s 333us/step - loss: 0.9226 - acc: 0.5641\n",
            "Epoch 269/500\n",
            "117/117 [==============================] - 0s 338us/step - loss: 0.8725 - acc: 0.6325\n",
            "Epoch 270/500\n",
            "117/117 [==============================] - 0s 337us/step - loss: 0.9511 - acc: 0.5983\n",
            "Epoch 271/500\n",
            "117/117 [==============================] - 0s 369us/step - loss: 0.9358 - acc: 0.6496\n",
            "Epoch 272/500\n",
            "117/117 [==============================] - 0s 293us/step - loss: 0.9276 - acc: 0.5983\n",
            "Epoch 273/500\n",
            "117/117 [==============================] - 0s 320us/step - loss: 0.9349 - acc: 0.5983\n",
            "Epoch 274/500\n",
            "117/117 [==============================] - 0s 290us/step - loss: 0.9186 - acc: 0.6154\n",
            "Epoch 275/500\n",
            "117/117 [==============================] - 0s 345us/step - loss: 0.9100 - acc: 0.6154\n",
            "Epoch 276/500\n",
            "117/117 [==============================] - 0s 433us/step - loss: 0.9325 - acc: 0.6410\n",
            "Epoch 277/500\n",
            "117/117 [==============================] - 0s 424us/step - loss: 0.8947 - acc: 0.6239\n",
            "Epoch 278/500\n",
            "117/117 [==============================] - 0s 334us/step - loss: 0.9672 - acc: 0.5726\n",
            "Epoch 279/500\n",
            "117/117 [==============================] - 0s 419us/step - loss: 0.8802 - acc: 0.6581\n",
            "Epoch 280/500\n",
            "117/117 [==============================] - 0s 375us/step - loss: 0.9445 - acc: 0.5897\n",
            "Epoch 281/500\n",
            "117/117 [==============================] - 0s 437us/step - loss: 0.9113 - acc: 0.6325\n",
            "Epoch 282/500\n",
            "117/117 [==============================] - 0s 387us/step - loss: 0.9042 - acc: 0.5897\n",
            "Epoch 283/500\n",
            "117/117 [==============================] - 0s 405us/step - loss: 0.9514 - acc: 0.6581\n",
            "Epoch 284/500\n",
            "117/117 [==============================] - 0s 386us/step - loss: 0.9433 - acc: 0.6239\n",
            "Epoch 285/500\n",
            "117/117 [==============================] - 0s 338us/step - loss: 0.8935 - acc: 0.6239\n",
            "Epoch 286/500\n",
            "117/117 [==============================] - 0s 415us/step - loss: 0.9570 - acc: 0.5556\n",
            "Epoch 287/500\n",
            "117/117 [==============================] - 0s 435us/step - loss: 0.9025 - acc: 0.6923\n",
            "Epoch 288/500\n",
            "117/117 [==============================] - 0s 445us/step - loss: 0.8804 - acc: 0.6325\n",
            "Epoch 289/500\n",
            "117/117 [==============================] - 0s 385us/step - loss: 0.9714 - acc: 0.6239\n",
            "Epoch 290/500\n",
            "117/117 [==============================] - 0s 387us/step - loss: 0.9097 - acc: 0.5726\n",
            "Epoch 291/500\n",
            "117/117 [==============================] - 0s 390us/step - loss: 0.9142 - acc: 0.6752\n",
            "Epoch 292/500\n",
            "117/117 [==============================] - 0s 326us/step - loss: 0.8806 - acc: 0.6239\n",
            "Epoch 293/500\n",
            "117/117 [==============================] - 0s 402us/step - loss: 0.9403 - acc: 0.6154\n",
            "Epoch 294/500\n",
            "117/117 [==============================] - 0s 418us/step - loss: 0.8513 - acc: 0.6239\n",
            "Epoch 295/500\n",
            "117/117 [==============================] - 0s 364us/step - loss: 0.9640 - acc: 0.5214\n",
            "Epoch 296/500\n",
            "117/117 [==============================] - 0s 374us/step - loss: 0.8744 - acc: 0.6154\n",
            "Epoch 297/500\n",
            "117/117 [==============================] - 0s 364us/step - loss: 0.8800 - acc: 0.5983\n",
            "Epoch 298/500\n",
            "117/117 [==============================] - 0s 352us/step - loss: 0.8774 - acc: 0.6239\n",
            "Epoch 299/500\n",
            "117/117 [==============================] - 0s 343us/step - loss: 0.8769 - acc: 0.6838\n",
            "Epoch 300/500\n",
            "117/117 [==============================] - 0s 341us/step - loss: 0.8793 - acc: 0.6581\n",
            "Epoch 301/500\n",
            "117/117 [==============================] - 0s 390us/step - loss: 0.8990 - acc: 0.6325\n",
            "Epoch 302/500\n",
            "117/117 [==============================] - 0s 434us/step - loss: 0.8846 - acc: 0.6667\n",
            "Epoch 303/500\n",
            "117/117 [==============================] - 0s 350us/step - loss: 0.8627 - acc: 0.6752\n",
            "Epoch 304/500\n",
            "117/117 [==============================] - 0s 419us/step - loss: 0.8389 - acc: 0.6667\n",
            "Epoch 305/500\n",
            "117/117 [==============================] - 0s 427us/step - loss: 0.9049 - acc: 0.6239\n",
            "Epoch 306/500\n",
            "117/117 [==============================] - 0s 368us/step - loss: 0.8691 - acc: 0.6325\n",
            "Epoch 307/500\n",
            "117/117 [==============================] - 0s 362us/step - loss: 0.8558 - acc: 0.6410\n",
            "Epoch 308/500\n",
            "117/117 [==============================] - 0s 426us/step - loss: 0.8826 - acc: 0.5726\n",
            "Epoch 309/500\n",
            "117/117 [==============================] - 0s 387us/step - loss: 0.8229 - acc: 0.6496\n",
            "Epoch 310/500\n",
            "117/117 [==============================] - 0s 487us/step - loss: 0.9066 - acc: 0.6239\n",
            "Epoch 311/500\n",
            "117/117 [==============================] - 0s 389us/step - loss: 0.8912 - acc: 0.5812\n",
            "Epoch 312/500\n",
            "117/117 [==============================] - 0s 449us/step - loss: 0.8263 - acc: 0.6496\n",
            "Epoch 313/500\n",
            "117/117 [==============================] - 0s 366us/step - loss: 0.7872 - acc: 0.6752\n",
            "Epoch 314/500\n",
            "117/117 [==============================] - 0s 390us/step - loss: 0.8873 - acc: 0.6667\n",
            "Epoch 315/500\n",
            "117/117 [==============================] - 0s 420us/step - loss: 0.9489 - acc: 0.5897\n",
            "Epoch 316/500\n",
            "117/117 [==============================] - 0s 379us/step - loss: 0.8695 - acc: 0.6068\n",
            "Epoch 317/500\n",
            "117/117 [==============================] - 0s 350us/step - loss: 0.9485 - acc: 0.5812\n",
            "Epoch 318/500\n",
            "117/117 [==============================] - 0s 438us/step - loss: 0.8512 - acc: 0.6838\n",
            "Epoch 319/500\n",
            "117/117 [==============================] - 0s 381us/step - loss: 0.9209 - acc: 0.6154\n",
            "Epoch 320/500\n",
            "117/117 [==============================] - 0s 402us/step - loss: 0.9020 - acc: 0.6410\n",
            "Epoch 321/500\n",
            "117/117 [==============================] - 0s 413us/step - loss: 0.8487 - acc: 0.6410\n",
            "Epoch 322/500\n",
            "117/117 [==============================] - 0s 395us/step - loss: 0.8873 - acc: 0.6496\n",
            "Epoch 323/500\n",
            "117/117 [==============================] - 0s 407us/step - loss: 0.8850 - acc: 0.6068\n",
            "Epoch 324/500\n",
            "117/117 [==============================] - 0s 369us/step - loss: 0.8747 - acc: 0.6325\n",
            "Epoch 325/500\n",
            "117/117 [==============================] - 0s 330us/step - loss: 0.8758 - acc: 0.6496\n",
            "Epoch 326/500\n",
            "117/117 [==============================] - 0s 334us/step - loss: 0.8999 - acc: 0.6496\n",
            "Epoch 327/500\n",
            "117/117 [==============================] - 0s 376us/step - loss: 0.8711 - acc: 0.6410\n",
            "Epoch 328/500\n",
            "117/117 [==============================] - 0s 425us/step - loss: 0.8478 - acc: 0.6581\n",
            "Epoch 329/500\n",
            "117/117 [==============================] - 0s 375us/step - loss: 0.8435 - acc: 0.6325\n",
            "Epoch 330/500\n",
            "117/117 [==============================] - 0s 393us/step - loss: 0.8406 - acc: 0.6068\n",
            "Epoch 331/500\n",
            "117/117 [==============================] - 0s 414us/step - loss: 0.8705 - acc: 0.6410\n",
            "Epoch 332/500\n",
            "117/117 [==============================] - 0s 429us/step - loss: 0.9017 - acc: 0.5897\n",
            "Epoch 333/500\n",
            "117/117 [==============================] - 0s 422us/step - loss: 0.9335 - acc: 0.5897\n",
            "Epoch 334/500\n",
            "117/117 [==============================] - 0s 406us/step - loss: 0.9254 - acc: 0.6325\n",
            "Epoch 335/500\n",
            "117/117 [==============================] - 0s 477us/step - loss: 0.8431 - acc: 0.6325\n",
            "Epoch 336/500\n",
            "117/117 [==============================] - 0s 378us/step - loss: 0.8171 - acc: 0.6325\n",
            "Epoch 337/500\n",
            "117/117 [==============================] - 0s 416us/step - loss: 0.8517 - acc: 0.6410\n",
            "Epoch 338/500\n",
            "117/117 [==============================] - 0s 371us/step - loss: 0.8431 - acc: 0.6667\n",
            "Epoch 339/500\n",
            "117/117 [==============================] - 0s 329us/step - loss: 0.8558 - acc: 0.6581\n",
            "Epoch 340/500\n",
            "117/117 [==============================] - 0s 362us/step - loss: 0.8458 - acc: 0.6581\n",
            "Epoch 341/500\n",
            "117/117 [==============================] - 0s 346us/step - loss: 0.8442 - acc: 0.6325\n",
            "Epoch 342/500\n",
            "117/117 [==============================] - 0s 349us/step - loss: 0.9252 - acc: 0.6154\n",
            "Epoch 343/500\n",
            "117/117 [==============================] - 0s 365us/step - loss: 0.8725 - acc: 0.6410\n",
            "Epoch 344/500\n",
            "117/117 [==============================] - 0s 357us/step - loss: 0.8126 - acc: 0.6239\n",
            "Epoch 345/500\n",
            "117/117 [==============================] - 0s 379us/step - loss: 0.8877 - acc: 0.6154\n",
            "Epoch 346/500\n",
            "117/117 [==============================] - 0s 334us/step - loss: 0.8508 - acc: 0.6410\n",
            "Epoch 347/500\n",
            "117/117 [==============================] - 0s 318us/step - loss: 0.8865 - acc: 0.6410\n",
            "Epoch 348/500\n",
            "117/117 [==============================] - 0s 371us/step - loss: 0.8557 - acc: 0.6496\n",
            "Epoch 349/500\n",
            "117/117 [==============================] - 0s 364us/step - loss: 0.9154 - acc: 0.5385\n",
            "Epoch 350/500\n",
            "117/117 [==============================] - 0s 336us/step - loss: 0.9128 - acc: 0.5812\n",
            "Epoch 351/500\n",
            "117/117 [==============================] - 0s 354us/step - loss: 0.8620 - acc: 0.6667\n",
            "Epoch 352/500\n",
            "117/117 [==============================] - 0s 325us/step - loss: 0.8549 - acc: 0.6581\n",
            "Epoch 353/500\n",
            "117/117 [==============================] - 0s 384us/step - loss: 0.8424 - acc: 0.6667\n",
            "Epoch 354/500\n",
            "117/117 [==============================] - 0s 354us/step - loss: 0.8266 - acc: 0.6325\n",
            "Epoch 355/500\n",
            "117/117 [==============================] - 0s 374us/step - loss: 0.8633 - acc: 0.6667\n",
            "Epoch 356/500\n",
            "117/117 [==============================] - 0s 413us/step - loss: 0.8389 - acc: 0.6496\n",
            "Epoch 357/500\n",
            "117/117 [==============================] - 0s 346us/step - loss: 0.8655 - acc: 0.6239\n",
            "Epoch 358/500\n",
            "117/117 [==============================] - 0s 351us/step - loss: 0.8011 - acc: 0.6752\n",
            "Epoch 359/500\n",
            "117/117 [==============================] - 0s 364us/step - loss: 0.8838 - acc: 0.6325\n",
            "Epoch 360/500\n",
            "117/117 [==============================] - 0s 358us/step - loss: 0.8429 - acc: 0.6667\n",
            "Epoch 361/500\n",
            "117/117 [==============================] - 0s 391us/step - loss: 0.8591 - acc: 0.6752\n",
            "Epoch 362/500\n",
            "117/117 [==============================] - 0s 396us/step - loss: 0.8568 - acc: 0.6410\n",
            "Epoch 363/500\n",
            "117/117 [==============================] - 0s 420us/step - loss: 0.8222 - acc: 0.6068\n",
            "Epoch 364/500\n",
            "117/117 [==============================] - 0s 426us/step - loss: 0.8603 - acc: 0.7009\n",
            "Epoch 365/500\n",
            "117/117 [==============================] - 0s 388us/step - loss: 0.8209 - acc: 0.6752\n",
            "Epoch 366/500\n",
            "117/117 [==============================] - 0s 437us/step - loss: 0.8808 - acc: 0.6410\n",
            "Epoch 367/500\n",
            "117/117 [==============================] - 0s 367us/step - loss: 0.8308 - acc: 0.6154\n",
            "Epoch 368/500\n",
            "117/117 [==============================] - 0s 381us/step - loss: 0.8574 - acc: 0.6410\n",
            "Epoch 369/500\n",
            "117/117 [==============================] - 0s 459us/step - loss: 0.8540 - acc: 0.6496\n",
            "Epoch 370/500\n",
            "117/117 [==============================] - 0s 372us/step - loss: 0.8155 - acc: 0.6667\n",
            "Epoch 371/500\n",
            "117/117 [==============================] - 0s 438us/step - loss: 0.8902 - acc: 0.6581\n",
            "Epoch 372/500\n",
            "117/117 [==============================] - 0s 400us/step - loss: 0.8686 - acc: 0.6068\n",
            "Epoch 373/500\n",
            "117/117 [==============================] - 0s 433us/step - loss: 0.8414 - acc: 0.6838\n",
            "Epoch 374/500\n",
            "117/117 [==============================] - 0s 344us/step - loss: 0.8609 - acc: 0.6239\n",
            "Epoch 375/500\n",
            "117/117 [==============================] - 0s 352us/step - loss: 0.7764 - acc: 0.6667\n",
            "Epoch 376/500\n",
            "117/117 [==============================] - 0s 441us/step - loss: 0.8471 - acc: 0.5812\n",
            "Epoch 377/500\n",
            "117/117 [==============================] - 0s 355us/step - loss: 0.8491 - acc: 0.6581\n",
            "Epoch 378/500\n",
            "117/117 [==============================] - 0s 351us/step - loss: 0.8594 - acc: 0.6923\n",
            "Epoch 379/500\n",
            "117/117 [==============================] - 0s 373us/step - loss: 0.8595 - acc: 0.6154\n",
            "Epoch 380/500\n",
            "117/117 [==============================] - 0s 331us/step - loss: 0.8542 - acc: 0.6154\n",
            "Epoch 381/500\n",
            "117/117 [==============================] - 0s 326us/step - loss: 0.8262 - acc: 0.6581\n",
            "Epoch 382/500\n",
            "117/117 [==============================] - 0s 324us/step - loss: 0.8678 - acc: 0.6496\n",
            "Epoch 383/500\n",
            "117/117 [==============================] - 0s 393us/step - loss: 0.8729 - acc: 0.5812\n",
            "Epoch 384/500\n",
            "117/117 [==============================] - 0s 379us/step - loss: 0.8140 - acc: 0.6923\n",
            "Epoch 385/500\n",
            "117/117 [==============================] - 0s 381us/step - loss: 0.8986 - acc: 0.6154\n",
            "Epoch 386/500\n",
            "117/117 [==============================] - 0s 377us/step - loss: 0.7628 - acc: 0.6667\n",
            "Epoch 387/500\n",
            "117/117 [==============================] - 0s 348us/step - loss: 0.8686 - acc: 0.6239\n",
            "Epoch 388/500\n",
            "117/117 [==============================] - 0s 382us/step - loss: 0.8085 - acc: 0.6667\n",
            "Epoch 389/500\n",
            "117/117 [==============================] - 0s 423us/step - loss: 0.7718 - acc: 0.7179\n",
            "Epoch 390/500\n",
            "117/117 [==============================] - 0s 403us/step - loss: 0.8161 - acc: 0.6667\n",
            "Epoch 391/500\n",
            "117/117 [==============================] - 0s 383us/step - loss: 0.8497 - acc: 0.6068\n",
            "Epoch 392/500\n",
            "117/117 [==============================] - 0s 386us/step - loss: 0.8451 - acc: 0.6667\n",
            "Epoch 393/500\n",
            "117/117 [==============================] - 0s 455us/step - loss: 0.8328 - acc: 0.6923\n",
            "Epoch 394/500\n",
            "117/117 [==============================] - 0s 424us/step - loss: 0.8958 - acc: 0.6410\n",
            "Epoch 395/500\n",
            "117/117 [==============================] - 0s 358us/step - loss: 0.8422 - acc: 0.6581\n",
            "Epoch 396/500\n",
            "117/117 [==============================] - 0s 422us/step - loss: 0.8260 - acc: 0.6325\n",
            "Epoch 397/500\n",
            "117/117 [==============================] - 0s 407us/step - loss: 0.8213 - acc: 0.6581\n",
            "Epoch 398/500\n",
            "117/117 [==============================] - 0s 413us/step - loss: 0.8532 - acc: 0.5897\n",
            "Epoch 399/500\n",
            "117/117 [==============================] - 0s 433us/step - loss: 0.8067 - acc: 0.6154\n",
            "Epoch 400/500\n",
            "117/117 [==============================] - 0s 347us/step - loss: 0.9273 - acc: 0.6325\n",
            "Epoch 401/500\n",
            "117/117 [==============================] - 0s 413us/step - loss: 0.7733 - acc: 0.6838\n",
            "Epoch 402/500\n",
            "117/117 [==============================] - 0s 401us/step - loss: 0.8378 - acc: 0.6410\n",
            "Epoch 403/500\n",
            "117/117 [==============================] - 0s 448us/step - loss: 0.8347 - acc: 0.7094\n",
            "Epoch 404/500\n",
            "117/117 [==============================] - 0s 388us/step - loss: 0.8127 - acc: 0.6667\n",
            "Epoch 405/500\n",
            "117/117 [==============================] - 0s 385us/step - loss: 0.8162 - acc: 0.7094\n",
            "Epoch 406/500\n",
            "117/117 [==============================] - 0s 427us/step - loss: 0.8138 - acc: 0.6752\n",
            "Epoch 407/500\n",
            "117/117 [==============================] - 0s 435us/step - loss: 0.7765 - acc: 0.7265\n",
            "Epoch 408/500\n",
            "117/117 [==============================] - 0s 400us/step - loss: 0.8562 - acc: 0.5726\n",
            "Epoch 409/500\n",
            "117/117 [==============================] - 0s 411us/step - loss: 0.8520 - acc: 0.6239\n",
            "Epoch 410/500\n",
            "117/117 [==============================] - 0s 406us/step - loss: 0.7929 - acc: 0.7094\n",
            "Epoch 411/500\n",
            "117/117 [==============================] - 0s 374us/step - loss: 0.8719 - acc: 0.6239\n",
            "Epoch 412/500\n",
            "117/117 [==============================] - 0s 398us/step - loss: 0.8041 - acc: 0.7009\n",
            "Epoch 413/500\n",
            "117/117 [==============================] - 0s 435us/step - loss: 0.8137 - acc: 0.6923\n",
            "Epoch 414/500\n",
            "117/117 [==============================] - 0s 383us/step - loss: 0.8344 - acc: 0.6068\n",
            "Epoch 415/500\n",
            "117/117 [==============================] - 0s 392us/step - loss: 0.8594 - acc: 0.5983\n",
            "Epoch 416/500\n",
            "117/117 [==============================] - 0s 369us/step - loss: 0.8266 - acc: 0.6154\n",
            "Epoch 417/500\n",
            "117/117 [==============================] - 0s 372us/step - loss: 0.7913 - acc: 0.6667\n",
            "Epoch 418/500\n",
            "117/117 [==============================] - 0s 356us/step - loss: 0.8165 - acc: 0.6068\n",
            "Epoch 419/500\n",
            "117/117 [==============================] - 0s 412us/step - loss: 0.9259 - acc: 0.5812\n",
            "Epoch 420/500\n",
            "117/117 [==============================] - 0s 336us/step - loss: 0.8475 - acc: 0.6752\n",
            "Epoch 421/500\n",
            "117/117 [==============================] - 0s 398us/step - loss: 0.8980 - acc: 0.6581\n",
            "Epoch 422/500\n",
            "117/117 [==============================] - 0s 383us/step - loss: 0.8357 - acc: 0.6325\n",
            "Epoch 423/500\n",
            "117/117 [==============================] - 0s 413us/step - loss: 0.8365 - acc: 0.6239\n",
            "Epoch 424/500\n",
            "117/117 [==============================] - 0s 459us/step - loss: 0.8057 - acc: 0.6752\n",
            "Epoch 425/500\n",
            "117/117 [==============================] - 0s 381us/step - loss: 0.7912 - acc: 0.6581\n",
            "Epoch 426/500\n",
            "117/117 [==============================] - 0s 417us/step - loss: 0.8017 - acc: 0.6667\n",
            "Epoch 427/500\n",
            "117/117 [==============================] - 0s 418us/step - loss: 0.7938 - acc: 0.7094\n",
            "Epoch 428/500\n",
            "117/117 [==============================] - 0s 393us/step - loss: 0.8247 - acc: 0.6923\n",
            "Epoch 429/500\n",
            "117/117 [==============================] - 0s 478us/step - loss: 0.7798 - acc: 0.7009\n",
            "Epoch 430/500\n",
            "117/117 [==============================] - 0s 507us/step - loss: 0.8208 - acc: 0.6410\n",
            "Epoch 431/500\n",
            "117/117 [==============================] - 0s 351us/step - loss: 0.7769 - acc: 0.7094\n",
            "Epoch 432/500\n",
            "117/117 [==============================] - 0s 390us/step - loss: 0.8273 - acc: 0.6838\n",
            "Epoch 433/500\n",
            "117/117 [==============================] - 0s 396us/step - loss: 0.8674 - acc: 0.6325\n",
            "Epoch 434/500\n",
            "117/117 [==============================] - 0s 412us/step - loss: 0.7478 - acc: 0.7521\n",
            "Epoch 435/500\n",
            "117/117 [==============================] - 0s 384us/step - loss: 0.7791 - acc: 0.7009\n",
            "Epoch 436/500\n",
            "117/117 [==============================] - 0s 398us/step - loss: 0.8525 - acc: 0.6068\n",
            "Epoch 437/500\n",
            "117/117 [==============================] - 0s 461us/step - loss: 0.7702 - acc: 0.6923\n",
            "Epoch 438/500\n",
            "117/117 [==============================] - 0s 457us/step - loss: 0.8418 - acc: 0.6838\n",
            "Epoch 439/500\n",
            "117/117 [==============================] - 0s 393us/step - loss: 0.7861 - acc: 0.6581\n",
            "Epoch 440/500\n",
            "117/117 [==============================] - 0s 394us/step - loss: 0.8289 - acc: 0.6410\n",
            "Epoch 441/500\n",
            "117/117 [==============================] - 0s 484us/step - loss: 0.8433 - acc: 0.6325\n",
            "Epoch 442/500\n",
            "117/117 [==============================] - 0s 452us/step - loss: 0.7700 - acc: 0.6923\n",
            "Epoch 443/500\n",
            "117/117 [==============================] - 0s 465us/step - loss: 0.8026 - acc: 0.6239\n",
            "Epoch 444/500\n",
            "117/117 [==============================] - 0s 376us/step - loss: 0.8448 - acc: 0.6410\n",
            "Epoch 445/500\n",
            "117/117 [==============================] - 0s 395us/step - loss: 0.7063 - acc: 0.7692\n",
            "Epoch 446/500\n",
            "117/117 [==============================] - 0s 398us/step - loss: 0.8134 - acc: 0.6752\n",
            "Epoch 447/500\n",
            "117/117 [==============================] - 0s 466us/step - loss: 0.8226 - acc: 0.6410\n",
            "Epoch 448/500\n",
            "117/117 [==============================] - 0s 390us/step - loss: 0.8314 - acc: 0.6154\n",
            "Epoch 449/500\n",
            "117/117 [==============================] - 0s 446us/step - loss: 0.7992 - acc: 0.6581\n",
            "Epoch 450/500\n",
            "117/117 [==============================] - 0s 464us/step - loss: 0.7609 - acc: 0.6496\n",
            "Epoch 451/500\n",
            "117/117 [==============================] - 0s 354us/step - loss: 0.8088 - acc: 0.6838\n",
            "Epoch 452/500\n",
            "117/117 [==============================] - 0s 386us/step - loss: 0.8039 - acc: 0.6752\n",
            "Epoch 453/500\n",
            "117/117 [==============================] - 0s 376us/step - loss: 0.7912 - acc: 0.6496\n",
            "Epoch 454/500\n",
            "117/117 [==============================] - 0s 383us/step - loss: 0.8408 - acc: 0.6068\n",
            "Epoch 455/500\n",
            "117/117 [==============================] - 0s 427us/step - loss: 0.8002 - acc: 0.7009\n",
            "Epoch 456/500\n",
            "117/117 [==============================] - 0s 442us/step - loss: 0.7814 - acc: 0.6667\n",
            "Epoch 457/500\n",
            "117/117 [==============================] - 0s 415us/step - loss: 0.8254 - acc: 0.6752\n",
            "Epoch 458/500\n",
            "117/117 [==============================] - 0s 440us/step - loss: 0.8395 - acc: 0.6667\n",
            "Epoch 459/500\n",
            "117/117 [==============================] - 0s 448us/step - loss: 0.7693 - acc: 0.7350\n",
            "Epoch 460/500\n",
            "117/117 [==============================] - 0s 418us/step - loss: 0.7833 - acc: 0.6667\n",
            "Epoch 461/500\n",
            "117/117 [==============================] - 0s 345us/step - loss: 0.8025 - acc: 0.6838\n",
            "Epoch 462/500\n",
            "117/117 [==============================] - 0s 370us/step - loss: 0.8122 - acc: 0.6581\n",
            "Epoch 463/500\n",
            "117/117 [==============================] - 0s 462us/step - loss: 0.7585 - acc: 0.7009\n",
            "Epoch 464/500\n",
            "117/117 [==============================] - 0s 403us/step - loss: 0.8130 - acc: 0.6068\n",
            "Epoch 465/500\n",
            "117/117 [==============================] - 0s 369us/step - loss: 0.7679 - acc: 0.6923\n",
            "Epoch 466/500\n",
            "117/117 [==============================] - 0s 397us/step - loss: 0.8087 - acc: 0.6496\n",
            "Epoch 467/500\n",
            "117/117 [==============================] - 0s 415us/step - loss: 0.7568 - acc: 0.6581\n",
            "Epoch 468/500\n",
            "117/117 [==============================] - 0s 397us/step - loss: 0.7545 - acc: 0.6752\n",
            "Epoch 469/500\n",
            "117/117 [==============================] - 0s 413us/step - loss: 0.7700 - acc: 0.6838\n",
            "Epoch 470/500\n",
            "117/117 [==============================] - 0s 394us/step - loss: 0.7603 - acc: 0.6838\n",
            "Epoch 471/500\n",
            "117/117 [==============================] - 0s 377us/step - loss: 0.8108 - acc: 0.5812\n",
            "Epoch 472/500\n",
            "117/117 [==============================] - 0s 382us/step - loss: 0.8450 - acc: 0.6667\n",
            "Epoch 473/500\n",
            "117/117 [==============================] - 0s 378us/step - loss: 0.8300 - acc: 0.6154\n",
            "Epoch 474/500\n",
            "117/117 [==============================] - 0s 443us/step - loss: 0.7705 - acc: 0.6581\n",
            "Epoch 475/500\n",
            "117/117 [==============================] - 0s 439us/step - loss: 0.7646 - acc: 0.6923\n",
            "Epoch 476/500\n",
            "117/117 [==============================] - 0s 503us/step - loss: 0.7786 - acc: 0.6410\n",
            "Epoch 477/500\n",
            "117/117 [==============================] - 0s 376us/step - loss: 0.7914 - acc: 0.6838\n",
            "Epoch 478/500\n",
            "117/117 [==============================] - 0s 416us/step - loss: 0.7818 - acc: 0.6838\n",
            "Epoch 479/500\n",
            "117/117 [==============================] - 0s 364us/step - loss: 0.7908 - acc: 0.7009\n",
            "Epoch 480/500\n",
            "117/117 [==============================] - 0s 400us/step - loss: 0.7526 - acc: 0.6838\n",
            "Epoch 481/500\n",
            "117/117 [==============================] - 0s 366us/step - loss: 0.8060 - acc: 0.6496\n",
            "Epoch 482/500\n",
            "117/117 [==============================] - 0s 403us/step - loss: 0.8071 - acc: 0.6325\n",
            "Epoch 483/500\n",
            "117/117 [==============================] - 0s 356us/step - loss: 0.8081 - acc: 0.6496\n",
            "Epoch 484/500\n",
            "117/117 [==============================] - 0s 333us/step - loss: 0.7804 - acc: 0.6581\n",
            "Epoch 485/500\n",
            "117/117 [==============================] - 0s 366us/step - loss: 0.7807 - acc: 0.7265\n",
            "Epoch 486/500\n",
            "117/117 [==============================] - 0s 322us/step - loss: 0.7664 - acc: 0.6752\n",
            "Epoch 487/500\n",
            "117/117 [==============================] - 0s 326us/step - loss: 0.8372 - acc: 0.6325\n",
            "Epoch 488/500\n",
            "117/117 [==============================] - 0s 427us/step - loss: 0.7936 - acc: 0.6667\n",
            "Epoch 489/500\n",
            "117/117 [==============================] - 0s 345us/step - loss: 0.7326 - acc: 0.7607\n",
            "Epoch 490/500\n",
            "117/117 [==============================] - 0s 339us/step - loss: 0.7637 - acc: 0.6496\n",
            "Epoch 491/500\n",
            "117/117 [==============================] - 0s 356us/step - loss: 0.8048 - acc: 0.6496\n",
            "Epoch 492/500\n",
            "117/117 [==============================] - 0s 337us/step - loss: 0.7217 - acc: 0.7009\n",
            "Epoch 493/500\n",
            "117/117 [==============================] - 0s 367us/step - loss: 0.7701 - acc: 0.6838\n",
            "Epoch 494/500\n",
            "117/117 [==============================] - 0s 346us/step - loss: 0.7444 - acc: 0.7179\n",
            "Epoch 495/500\n",
            "117/117 [==============================] - 0s 331us/step - loss: 0.7584 - acc: 0.6581\n",
            "Epoch 496/500\n",
            "117/117 [==============================] - 0s 357us/step - loss: 0.7322 - acc: 0.7094\n",
            "Epoch 497/500\n",
            "117/117 [==============================] - 0s 378us/step - loss: 0.6991 - acc: 0.6752\n",
            "Epoch 498/500\n",
            "117/117 [==============================] - 0s 357us/step - loss: 0.7731 - acc: 0.7094\n",
            "Epoch 499/500\n",
            "117/117 [==============================] - 0s 393us/step - loss: 0.7841 - acc: 0.7094\n",
            "Epoch 500/500\n",
            "117/117 [==============================] - 0s 329us/step - loss: 0.7920 - acc: 0.6838\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe9b98b8320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGb6gMil314f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "aebe38dc-fd85-4d3e-9e54-ee1b8f0426b7"
      },
      "source": [
        "# Predicting vanilla neural network with dropout labels\n",
        "\n",
        "print(nn_dropout.predict_classes(prediction_input_preprocessor.transform(X_test)))\n",
        "\n",
        "prediction_index_nn_dropout=nn_dropout.predict_classes(prediction_input_preprocessor.transform(X_test))\n",
        "\n",
        "labels=processed_y_train.columns\n",
        "\n",
        "def index_to_label(labels,index_n): \n",
        "    return labels[index_n]\n",
        "    \n",
        "index_to_label(labels,1)\n",
        "\n",
        "nn_dropout_predicted_labels=list(map(lambda x: labels[x], prediction_index_nn_dropout))\n",
        "print(nn_dropout_predicted_labels)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 0 0 1 4 1 2 0 1 2 1 0 3 2 1 1 4 4 4 0 4 3 1 3 1 2 0 1 1 4 0 2 1 4 2 4 4\n",
            " 4 1]\n",
            "['High', 'Average', 'Average', 'High', 'Very Low', 'High', 'Low', 'Average', 'High', 'Low', 'High', 'Average', 'Very High', 'Low', 'High', 'High', 'Very Low', 'Very Low', 'Very Low', 'Average', 'Very Low', 'Very High', 'High', 'Very High', 'High', 'Low', 'Average', 'High', 'High', 'Very Low', 'Average', 'Low', 'High', 'Very Low', 'Low', 'Very Low', 'Very Low', 'Very Low', 'High']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEQrSKDg3n0w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "outputId": "ddce0e6d-e88a-4b15-b107-967fdf999dca"
      },
      "source": [
        "# Vanilla neural network with dropout regularization evaluation\n",
        "\n",
        "model_eval_metrics(y_test,nn_dropout_predicted_labels,classification=\"TRUE\")\n",
        "nn_dropout_model_eval = model_eval_metrics( y_test,nn_dropout_predicted_labels,classification=\"TRUE\")\n",
        "nn_dropout_model_eval"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>f1_score</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>mse</th>\n",
              "      <th>rmse</th>\n",
              "      <th>mae</th>\n",
              "      <th>r2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.512821</td>\n",
              "      <td>0.516576</td>\n",
              "      <td>0.59978</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   accuracy  f1_score  precision    recall  mse  rmse  mae  r2\n",
              "0  0.512821  0.516576    0.59978  0.533333    0     0    0   0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-meU93oZ2P7t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2037b2d0-7610-4f73-cf4f-c5677a9a76ff"
      },
      "source": [
        "# NN with L2 regularization\n",
        "from keras.regularizers import l2\n",
        "\n",
        "nn_penalized = Sequential()\n",
        "nn_penalized.add(Dense(64, input_dim=20, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
        "nn_penalized.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
        "nn_penalized.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
        "nn_penalized.add(Dense(5, activation='softmax')) \n",
        "                                            \n",
        "nn_penalized.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "\n",
        "nn_penalized.fit(processed_X_train, processed_y_train, epochs = 500)  "
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "117/117 [==============================] - 10s 81ms/step - loss: 3.1681 - acc: 0.3077\n",
            "Epoch 2/500\n",
            "117/117 [==============================] - 0s 312us/step - loss: 3.1551 - acc: 0.2991\n",
            "Epoch 3/500\n",
            "117/117 [==============================] - 0s 303us/step - loss: 3.1424 - acc: 0.2991\n",
            "Epoch 4/500\n",
            "117/117 [==============================] - 0s 306us/step - loss: 3.1296 - acc: 0.3077\n",
            "Epoch 5/500\n",
            "117/117 [==============================] - 0s 310us/step - loss: 3.1172 - acc: 0.3248\n",
            "Epoch 6/500\n",
            "117/117 [==============================] - 0s 314us/step - loss: 3.1053 - acc: 0.3419\n",
            "Epoch 7/500\n",
            "117/117 [==============================] - 0s 341us/step - loss: 3.0940 - acc: 0.3419\n",
            "Epoch 8/500\n",
            "117/117 [==============================] - 0s 353us/step - loss: 3.0828 - acc: 0.3590\n",
            "Epoch 9/500\n",
            "117/117 [==============================] - 0s 310us/step - loss: 3.0710 - acc: 0.3675\n",
            "Epoch 10/500\n",
            "117/117 [==============================] - 0s 351us/step - loss: 3.0599 - acc: 0.3675\n",
            "Epoch 11/500\n",
            "117/117 [==============================] - 0s 327us/step - loss: 3.0489 - acc: 0.3761\n",
            "Epoch 12/500\n",
            "117/117 [==============================] - 0s 352us/step - loss: 3.0382 - acc: 0.3761\n",
            "Epoch 13/500\n",
            "117/117 [==============================] - 0s 390us/step - loss: 3.0267 - acc: 0.3932\n",
            "Epoch 14/500\n",
            "117/117 [==============================] - 0s 339us/step - loss: 3.0162 - acc: 0.3846\n",
            "Epoch 15/500\n",
            "117/117 [==============================] - 0s 309us/step - loss: 3.0050 - acc: 0.3761\n",
            "Epoch 16/500\n",
            "117/117 [==============================] - 0s 332us/step - loss: 2.9942 - acc: 0.3761\n",
            "Epoch 17/500\n",
            "117/117 [==============================] - 0s 394us/step - loss: 2.9834 - acc: 0.3932\n",
            "Epoch 18/500\n",
            "117/117 [==============================] - 0s 381us/step - loss: 2.9728 - acc: 0.4103\n",
            "Epoch 19/500\n",
            "117/117 [==============================] - 0s 371us/step - loss: 2.9621 - acc: 0.4188\n",
            "Epoch 20/500\n",
            "117/117 [==============================] - 0s 333us/step - loss: 2.9524 - acc: 0.4188\n",
            "Epoch 21/500\n",
            "117/117 [==============================] - 0s 319us/step - loss: 2.9412 - acc: 0.4359\n",
            "Epoch 22/500\n",
            "117/117 [==============================] - 0s 324us/step - loss: 2.9310 - acc: 0.4274\n",
            "Epoch 23/500\n",
            "117/117 [==============================] - 0s 323us/step - loss: 2.9210 - acc: 0.4359\n",
            "Epoch 24/500\n",
            "117/117 [==============================] - 0s 343us/step - loss: 2.9108 - acc: 0.4359\n",
            "Epoch 25/500\n",
            "117/117 [==============================] - 0s 296us/step - loss: 2.9007 - acc: 0.4359\n",
            "Epoch 26/500\n",
            "117/117 [==============================] - 0s 355us/step - loss: 2.8908 - acc: 0.4444\n",
            "Epoch 27/500\n",
            "117/117 [==============================] - 0s 324us/step - loss: 2.8811 - acc: 0.4701\n",
            "Epoch 28/500\n",
            "117/117 [==============================] - 0s 317us/step - loss: 2.8709 - acc: 0.4615\n",
            "Epoch 29/500\n",
            "117/117 [==============================] - 0s 315us/step - loss: 2.8615 - acc: 0.4701\n",
            "Epoch 30/500\n",
            "117/117 [==============================] - 0s 351us/step - loss: 2.8519 - acc: 0.4786\n",
            "Epoch 31/500\n",
            "117/117 [==============================] - 0s 326us/step - loss: 2.8426 - acc: 0.4786\n",
            "Epoch 32/500\n",
            "117/117 [==============================] - 0s 335us/step - loss: 2.8334 - acc: 0.4701\n",
            "Epoch 33/500\n",
            "117/117 [==============================] - 0s 314us/step - loss: 2.8240 - acc: 0.4957\n",
            "Epoch 34/500\n",
            "117/117 [==============================] - 0s 333us/step - loss: 2.8146 - acc: 0.4957\n",
            "Epoch 35/500\n",
            "117/117 [==============================] - 0s 340us/step - loss: 2.8056 - acc: 0.4872\n",
            "Epoch 36/500\n",
            "117/117 [==============================] - 0s 355us/step - loss: 2.7964 - acc: 0.4872\n",
            "Epoch 37/500\n",
            "117/117 [==============================] - 0s 322us/step - loss: 2.7875 - acc: 0.4786\n",
            "Epoch 38/500\n",
            "117/117 [==============================] - 0s 325us/step - loss: 2.7787 - acc: 0.4786\n",
            "Epoch 39/500\n",
            "117/117 [==============================] - 0s 309us/step - loss: 2.7695 - acc: 0.4872\n",
            "Epoch 40/500\n",
            "117/117 [==============================] - 0s 359us/step - loss: 2.7612 - acc: 0.4872\n",
            "Epoch 41/500\n",
            "117/117 [==============================] - 0s 440us/step - loss: 2.7527 - acc: 0.4786\n",
            "Epoch 42/500\n",
            "117/117 [==============================] - 0s 333us/step - loss: 2.7447 - acc: 0.4872\n",
            "Epoch 43/500\n",
            "117/117 [==============================] - 0s 366us/step - loss: 2.7358 - acc: 0.4786\n",
            "Epoch 44/500\n",
            "117/117 [==============================] - 0s 331us/step - loss: 2.7276 - acc: 0.5043\n",
            "Epoch 45/500\n",
            "117/117 [==============================] - 0s 342us/step - loss: 2.7194 - acc: 0.5043\n",
            "Epoch 46/500\n",
            "117/117 [==============================] - 0s 359us/step - loss: 2.7112 - acc: 0.5043\n",
            "Epoch 47/500\n",
            "117/117 [==============================] - 0s 319us/step - loss: 2.7035 - acc: 0.5128\n",
            "Epoch 48/500\n",
            "117/117 [==============================] - 0s 317us/step - loss: 2.6955 - acc: 0.5128\n",
            "Epoch 49/500\n",
            "117/117 [==============================] - 0s 351us/step - loss: 2.6880 - acc: 0.5214\n",
            "Epoch 50/500\n",
            "117/117 [==============================] - 0s 352us/step - loss: 2.6805 - acc: 0.5214\n",
            "Epoch 51/500\n",
            "117/117 [==============================] - 0s 311us/step - loss: 2.6727 - acc: 0.5299\n",
            "Epoch 52/500\n",
            "117/117 [==============================] - 0s 288us/step - loss: 2.6653 - acc: 0.5385\n",
            "Epoch 53/500\n",
            "117/117 [==============================] - 0s 408us/step - loss: 2.6579 - acc: 0.5470\n",
            "Epoch 54/500\n",
            "117/117 [==============================] - 0s 334us/step - loss: 2.6511 - acc: 0.5470\n",
            "Epoch 55/500\n",
            "117/117 [==============================] - 0s 299us/step - loss: 2.6436 - acc: 0.5556\n",
            "Epoch 56/500\n",
            "117/117 [==============================] - 0s 348us/step - loss: 2.6364 - acc: 0.5556\n",
            "Epoch 57/500\n",
            "117/117 [==============================] - 0s 295us/step - loss: 2.6295 - acc: 0.5470\n",
            "Epoch 58/500\n",
            "117/117 [==============================] - 0s 328us/step - loss: 2.6226 - acc: 0.5470\n",
            "Epoch 59/500\n",
            "117/117 [==============================] - 0s 401us/step - loss: 2.6158 - acc: 0.5556\n",
            "Epoch 60/500\n",
            "117/117 [==============================] - 0s 333us/step - loss: 2.6091 - acc: 0.5556\n",
            "Epoch 61/500\n",
            "117/117 [==============================] - 0s 364us/step - loss: 2.6028 - acc: 0.5812\n",
            "Epoch 62/500\n",
            "117/117 [==============================] - 0s 291us/step - loss: 2.5961 - acc: 0.5812\n",
            "Epoch 63/500\n",
            "117/117 [==============================] - 0s 323us/step - loss: 2.5897 - acc: 0.5812\n",
            "Epoch 64/500\n",
            "117/117 [==============================] - 0s 399us/step - loss: 2.5835 - acc: 0.5726\n",
            "Epoch 65/500\n",
            "117/117 [==============================] - 0s 360us/step - loss: 2.5770 - acc: 0.5726\n",
            "Epoch 66/500\n",
            "117/117 [==============================] - 0s 383us/step - loss: 2.5708 - acc: 0.5812\n",
            "Epoch 67/500\n",
            "117/117 [==============================] - 0s 384us/step - loss: 2.5645 - acc: 0.5812\n",
            "Epoch 68/500\n",
            "117/117 [==============================] - 0s 350us/step - loss: 2.5589 - acc: 0.5897\n",
            "Epoch 69/500\n",
            "117/117 [==============================] - 0s 350us/step - loss: 2.5525 - acc: 0.5812\n",
            "Epoch 70/500\n",
            "117/117 [==============================] - 0s 357us/step - loss: 2.5460 - acc: 0.5897\n",
            "Epoch 71/500\n",
            "117/117 [==============================] - 0s 325us/step - loss: 2.5403 - acc: 0.5897\n",
            "Epoch 72/500\n",
            "117/117 [==============================] - 0s 364us/step - loss: 2.5339 - acc: 0.5897\n",
            "Epoch 73/500\n",
            "117/117 [==============================] - 0s 321us/step - loss: 2.5282 - acc: 0.5983\n",
            "Epoch 74/500\n",
            "117/117 [==============================] - 0s 307us/step - loss: 2.5220 - acc: 0.5983\n",
            "Epoch 75/500\n",
            "117/117 [==============================] - 0s 305us/step - loss: 2.5167 - acc: 0.6068\n",
            "Epoch 76/500\n",
            "117/117 [==============================] - 0s 350us/step - loss: 2.5110 - acc: 0.6068\n",
            "Epoch 77/500\n",
            "117/117 [==============================] - 0s 295us/step - loss: 2.5047 - acc: 0.6154\n",
            "Epoch 78/500\n",
            "117/117 [==============================] - 0s 411us/step - loss: 2.4994 - acc: 0.6154\n",
            "Epoch 79/500\n",
            "117/117 [==============================] - 0s 328us/step - loss: 2.4938 - acc: 0.6068\n",
            "Epoch 80/500\n",
            "117/117 [==============================] - 0s 377us/step - loss: 2.4880 - acc: 0.6068\n",
            "Epoch 81/500\n",
            "117/117 [==============================] - 0s 387us/step - loss: 2.4824 - acc: 0.6239\n",
            "Epoch 82/500\n",
            "117/117 [==============================] - 0s 355us/step - loss: 2.4769 - acc: 0.5983\n",
            "Epoch 83/500\n",
            "117/117 [==============================] - 0s 357us/step - loss: 2.4721 - acc: 0.6154\n",
            "Epoch 84/500\n",
            "117/117 [==============================] - 0s 367us/step - loss: 2.4664 - acc: 0.6068\n",
            "Epoch 85/500\n",
            "117/117 [==============================] - 0s 401us/step - loss: 2.4612 - acc: 0.5983\n",
            "Epoch 86/500\n",
            "117/117 [==============================] - 0s 360us/step - loss: 2.4558 - acc: 0.6068\n",
            "Epoch 87/500\n",
            "117/117 [==============================] - 0s 384us/step - loss: 2.4508 - acc: 0.5983\n",
            "Epoch 88/500\n",
            "117/117 [==============================] - 0s 404us/step - loss: 2.4458 - acc: 0.6068\n",
            "Epoch 89/500\n",
            "117/117 [==============================] - 0s 479us/step - loss: 2.4404 - acc: 0.6154\n",
            "Epoch 90/500\n",
            "117/117 [==============================] - 0s 366us/step - loss: 2.4362 - acc: 0.6154\n",
            "Epoch 91/500\n",
            "117/117 [==============================] - 0s 433us/step - loss: 2.4306 - acc: 0.6325\n",
            "Epoch 92/500\n",
            "117/117 [==============================] - 0s 423us/step - loss: 2.4256 - acc: 0.6239\n",
            "Epoch 93/500\n",
            "117/117 [==============================] - 0s 411us/step - loss: 2.4209 - acc: 0.6239\n",
            "Epoch 94/500\n",
            "117/117 [==============================] - 0s 379us/step - loss: 2.4156 - acc: 0.6239\n",
            "Epoch 95/500\n",
            "117/117 [==============================] - 0s 386us/step - loss: 2.4108 - acc: 0.6410\n",
            "Epoch 96/500\n",
            "117/117 [==============================] - 0s 399us/step - loss: 2.4058 - acc: 0.6325\n",
            "Epoch 97/500\n",
            "117/117 [==============================] - 0s 450us/step - loss: 2.4010 - acc: 0.6410\n",
            "Epoch 98/500\n",
            "117/117 [==============================] - 0s 429us/step - loss: 2.3969 - acc: 0.6410\n",
            "Epoch 99/500\n",
            "117/117 [==============================] - 0s 364us/step - loss: 2.3913 - acc: 0.6496\n",
            "Epoch 100/500\n",
            "117/117 [==============================] - 0s 386us/step - loss: 2.3867 - acc: 0.6496\n",
            "Epoch 101/500\n",
            "117/117 [==============================] - 0s 358us/step - loss: 2.3820 - acc: 0.6581\n",
            "Epoch 102/500\n",
            "117/117 [==============================] - 0s 340us/step - loss: 2.3777 - acc: 0.6581\n",
            "Epoch 103/500\n",
            "117/117 [==============================] - 0s 389us/step - loss: 2.3727 - acc: 0.6581\n",
            "Epoch 104/500\n",
            "117/117 [==============================] - 0s 360us/step - loss: 2.3687 - acc: 0.6581\n",
            "Epoch 105/500\n",
            "117/117 [==============================] - 0s 371us/step - loss: 2.3636 - acc: 0.6667\n",
            "Epoch 106/500\n",
            "117/117 [==============================] - 0s 452us/step - loss: 2.3592 - acc: 0.6581\n",
            "Epoch 107/500\n",
            "117/117 [==============================] - 0s 411us/step - loss: 2.3553 - acc: 0.6581\n",
            "Epoch 108/500\n",
            "117/117 [==============================] - 0s 473us/step - loss: 2.3505 - acc: 0.6667\n",
            "Epoch 109/500\n",
            "117/117 [==============================] - 0s 380us/step - loss: 2.3459 - acc: 0.6667\n",
            "Epoch 110/500\n",
            "117/117 [==============================] - 0s 365us/step - loss: 2.3419 - acc: 0.6752\n",
            "Epoch 111/500\n",
            "117/117 [==============================] - 0s 423us/step - loss: 2.3366 - acc: 0.6838\n",
            "Epoch 112/500\n",
            "117/117 [==============================] - 0s 371us/step - loss: 2.3325 - acc: 0.6838\n",
            "Epoch 113/500\n",
            "117/117 [==============================] - 0s 350us/step - loss: 2.3284 - acc: 0.6923\n",
            "Epoch 114/500\n",
            "117/117 [==============================] - 0s 387us/step - loss: 2.3237 - acc: 0.7009\n",
            "Epoch 115/500\n",
            "117/117 [==============================] - 0s 355us/step - loss: 2.3191 - acc: 0.6923\n",
            "Epoch 116/500\n",
            "117/117 [==============================] - 0s 342us/step - loss: 2.3154 - acc: 0.7009\n",
            "Epoch 117/500\n",
            "117/117 [==============================] - 0s 370us/step - loss: 2.3112 - acc: 0.7009\n",
            "Epoch 118/500\n",
            "117/117 [==============================] - 0s 327us/step - loss: 2.3068 - acc: 0.7009\n",
            "Epoch 119/500\n",
            "117/117 [==============================] - 0s 348us/step - loss: 2.3021 - acc: 0.7009\n",
            "Epoch 120/500\n",
            "117/117 [==============================] - 0s 337us/step - loss: 2.2979 - acc: 0.7009\n",
            "Epoch 121/500\n",
            "117/117 [==============================] - 0s 327us/step - loss: 2.2936 - acc: 0.7009\n",
            "Epoch 122/500\n",
            "117/117 [==============================] - 0s 340us/step - loss: 2.2898 - acc: 0.7009\n",
            "Epoch 123/500\n",
            "117/117 [==============================] - 0s 387us/step - loss: 2.2856 - acc: 0.7009\n",
            "Epoch 124/500\n",
            "117/117 [==============================] - 0s 389us/step - loss: 2.2810 - acc: 0.7009\n",
            "Epoch 125/500\n",
            "117/117 [==============================] - 0s 355us/step - loss: 2.2770 - acc: 0.7009\n",
            "Epoch 126/500\n",
            "117/117 [==============================] - 0s 373us/step - loss: 2.2729 - acc: 0.7009\n",
            "Epoch 127/500\n",
            "117/117 [==============================] - 0s 337us/step - loss: 2.2693 - acc: 0.7009\n",
            "Epoch 128/500\n",
            "117/117 [==============================] - 0s 340us/step - loss: 2.2656 - acc: 0.7009\n",
            "Epoch 129/500\n",
            "117/117 [==============================] - 0s 400us/step - loss: 2.2615 - acc: 0.7009\n",
            "Epoch 130/500\n",
            "117/117 [==============================] - 0s 430us/step - loss: 2.2569 - acc: 0.7009\n",
            "Epoch 131/500\n",
            "117/117 [==============================] - 0s 445us/step - loss: 2.2538 - acc: 0.7094\n",
            "Epoch 132/500\n",
            "117/117 [==============================] - 0s 451us/step - loss: 2.2496 - acc: 0.7094\n",
            "Epoch 133/500\n",
            "117/117 [==============================] - 0s 385us/step - loss: 2.2457 - acc: 0.7094\n",
            "Epoch 134/500\n",
            "117/117 [==============================] - 0s 390us/step - loss: 2.2421 - acc: 0.7094\n",
            "Epoch 135/500\n",
            "117/117 [==============================] - 0s 416us/step - loss: 2.2375 - acc: 0.7094\n",
            "Epoch 136/500\n",
            "117/117 [==============================] - 0s 424us/step - loss: 2.2340 - acc: 0.7094\n",
            "Epoch 137/500\n",
            "117/117 [==============================] - 0s 397us/step - loss: 2.2300 - acc: 0.7094\n",
            "Epoch 138/500\n",
            "117/117 [==============================] - 0s 391us/step - loss: 2.2270 - acc: 0.7094\n",
            "Epoch 139/500\n",
            "117/117 [==============================] - 0s 340us/step - loss: 2.2229 - acc: 0.7094\n",
            "Epoch 140/500\n",
            "117/117 [==============================] - 0s 388us/step - loss: 2.2186 - acc: 0.7094\n",
            "Epoch 141/500\n",
            "117/117 [==============================] - 0s 388us/step - loss: 2.2154 - acc: 0.7094\n",
            "Epoch 142/500\n",
            "117/117 [==============================] - 0s 437us/step - loss: 2.2123 - acc: 0.7094\n",
            "Epoch 143/500\n",
            "117/117 [==============================] - 0s 359us/step - loss: 2.2083 - acc: 0.7094\n",
            "Epoch 144/500\n",
            "117/117 [==============================] - 0s 382us/step - loss: 2.2054 - acc: 0.7094\n",
            "Epoch 145/500\n",
            "117/117 [==============================] - 0s 391us/step - loss: 2.2007 - acc: 0.7094\n",
            "Epoch 146/500\n",
            "117/117 [==============================] - 0s 379us/step - loss: 2.1977 - acc: 0.7094\n",
            "Epoch 147/500\n",
            "117/117 [==============================] - 0s 429us/step - loss: 2.1948 - acc: 0.7094\n",
            "Epoch 148/500\n",
            "117/117 [==============================] - 0s 508us/step - loss: 2.1903 - acc: 0.7094\n",
            "Epoch 149/500\n",
            "117/117 [==============================] - 0s 433us/step - loss: 2.1870 - acc: 0.7094\n",
            "Epoch 150/500\n",
            "117/117 [==============================] - 0s 381us/step - loss: 2.1836 - acc: 0.7179\n",
            "Epoch 151/500\n",
            "117/117 [==============================] - 0s 431us/step - loss: 2.1804 - acc: 0.7179\n",
            "Epoch 152/500\n",
            "117/117 [==============================] - 0s 400us/step - loss: 2.1762 - acc: 0.7179\n",
            "Epoch 153/500\n",
            "117/117 [==============================] - 0s 354us/step - loss: 2.1733 - acc: 0.7179\n",
            "Epoch 154/500\n",
            "117/117 [==============================] - 0s 400us/step - loss: 2.1694 - acc: 0.7179\n",
            "Epoch 155/500\n",
            "117/117 [==============================] - 0s 372us/step - loss: 2.1668 - acc: 0.7265\n",
            "Epoch 156/500\n",
            "117/117 [==============================] - 0s 375us/step - loss: 2.1629 - acc: 0.7094\n",
            "Epoch 157/500\n",
            "117/117 [==============================] - 0s 362us/step - loss: 2.1591 - acc: 0.7179\n",
            "Epoch 158/500\n",
            "117/117 [==============================] - 0s 442us/step - loss: 2.1562 - acc: 0.7265\n",
            "Epoch 159/500\n",
            "117/117 [==============================] - 0s 463us/step - loss: 2.1530 - acc: 0.7179\n",
            "Epoch 160/500\n",
            "117/117 [==============================] - 0s 416us/step - loss: 2.1489 - acc: 0.7265\n",
            "Epoch 161/500\n",
            "117/117 [==============================] - 0s 382us/step - loss: 2.1468 - acc: 0.7265\n",
            "Epoch 162/500\n",
            "117/117 [==============================] - 0s 367us/step - loss: 2.1424 - acc: 0.7179\n",
            "Epoch 163/500\n",
            "117/117 [==============================] - 0s 418us/step - loss: 2.1391 - acc: 0.7179\n",
            "Epoch 164/500\n",
            "117/117 [==============================] - 0s 474us/step - loss: 2.1377 - acc: 0.7179\n",
            "Epoch 165/500\n",
            "117/117 [==============================] - 0s 380us/step - loss: 2.1334 - acc: 0.7179\n",
            "Epoch 166/500\n",
            "117/117 [==============================] - 0s 456us/step - loss: 2.1294 - acc: 0.7265\n",
            "Epoch 167/500\n",
            "117/117 [==============================] - 0s 474us/step - loss: 2.1263 - acc: 0.7179\n",
            "Epoch 168/500\n",
            "117/117 [==============================] - 0s 401us/step - loss: 2.1231 - acc: 0.7265\n",
            "Epoch 169/500\n",
            "117/117 [==============================] - 0s 482us/step - loss: 2.1209 - acc: 0.7179\n",
            "Epoch 170/500\n",
            "117/117 [==============================] - 0s 378us/step - loss: 2.1168 - acc: 0.7265\n",
            "Epoch 171/500\n",
            "117/117 [==============================] - 0s 411us/step - loss: 2.1139 - acc: 0.7265\n",
            "Epoch 172/500\n",
            "117/117 [==============================] - 0s 401us/step - loss: 2.1110 - acc: 0.7265\n",
            "Epoch 173/500\n",
            "117/117 [==============================] - 0s 433us/step - loss: 2.1081 - acc: 0.7179\n",
            "Epoch 174/500\n",
            "117/117 [==============================] - 0s 485us/step - loss: 2.1046 - acc: 0.7436\n",
            "Epoch 175/500\n",
            "117/117 [==============================] - 0s 370us/step - loss: 2.1011 - acc: 0.7436\n",
            "Epoch 176/500\n",
            "117/117 [==============================] - 0s 418us/step - loss: 2.0983 - acc: 0.7350\n",
            "Epoch 177/500\n",
            "117/117 [==============================] - 0s 406us/step - loss: 2.0952 - acc: 0.7350\n",
            "Epoch 178/500\n",
            "117/117 [==============================] - 0s 412us/step - loss: 2.0920 - acc: 0.7436\n",
            "Epoch 179/500\n",
            "117/117 [==============================] - 0s 407us/step - loss: 2.0887 - acc: 0.7436\n",
            "Epoch 180/500\n",
            "117/117 [==============================] - 0s 430us/step - loss: 2.0853 - acc: 0.7436\n",
            "Epoch 181/500\n",
            "117/117 [==============================] - 0s 369us/step - loss: 2.0823 - acc: 0.7436\n",
            "Epoch 182/500\n",
            "117/117 [==============================] - 0s 465us/step - loss: 2.0799 - acc: 0.7436\n",
            "Epoch 183/500\n",
            "117/117 [==============================] - 0s 465us/step - loss: 2.0764 - acc: 0.7436\n",
            "Epoch 184/500\n",
            "117/117 [==============================] - 0s 372us/step - loss: 2.0734 - acc: 0.7521\n",
            "Epoch 185/500\n",
            "117/117 [==============================] - 0s 456us/step - loss: 2.0709 - acc: 0.7521\n",
            "Epoch 186/500\n",
            "117/117 [==============================] - 0s 414us/step - loss: 2.0676 - acc: 0.7436\n",
            "Epoch 187/500\n",
            "117/117 [==============================] - 0s 356us/step - loss: 2.0644 - acc: 0.7436\n",
            "Epoch 188/500\n",
            "117/117 [==============================] - 0s 391us/step - loss: 2.0614 - acc: 0.7521\n",
            "Epoch 189/500\n",
            "117/117 [==============================] - 0s 442us/step - loss: 2.0583 - acc: 0.7521\n",
            "Epoch 190/500\n",
            "117/117 [==============================] - 0s 435us/step - loss: 2.0566 - acc: 0.7436\n",
            "Epoch 191/500\n",
            "117/117 [==============================] - 0s 451us/step - loss: 2.0527 - acc: 0.7521\n",
            "Epoch 192/500\n",
            "117/117 [==============================] - 0s 602us/step - loss: 2.0499 - acc: 0.7521\n",
            "Epoch 193/500\n",
            "117/117 [==============================] - 0s 364us/step - loss: 2.0469 - acc: 0.7521\n",
            "Epoch 194/500\n",
            "117/117 [==============================] - 0s 377us/step - loss: 2.0443 - acc: 0.7436\n",
            "Epoch 195/500\n",
            "117/117 [==============================] - 0s 377us/step - loss: 2.0420 - acc: 0.7436\n",
            "Epoch 196/500\n",
            "117/117 [==============================] - 0s 416us/step - loss: 2.0391 - acc: 0.7521\n",
            "Epoch 197/500\n",
            "117/117 [==============================] - 0s 386us/step - loss: 2.0363 - acc: 0.7521\n",
            "Epoch 198/500\n",
            "117/117 [==============================] - 0s 327us/step - loss: 2.0327 - acc: 0.7436\n",
            "Epoch 199/500\n",
            "117/117 [==============================] - 0s 372us/step - loss: 2.0303 - acc: 0.7436\n",
            "Epoch 200/500\n",
            "117/117 [==============================] - 0s 384us/step - loss: 2.0272 - acc: 0.7436\n",
            "Epoch 201/500\n",
            "117/117 [==============================] - 0s 323us/step - loss: 2.0252 - acc: 0.7436\n",
            "Epoch 202/500\n",
            "117/117 [==============================] - 0s 346us/step - loss: 2.0222 - acc: 0.7521\n",
            "Epoch 203/500\n",
            "117/117 [==============================] - 0s 327us/step - loss: 2.0193 - acc: 0.7521\n",
            "Epoch 204/500\n",
            "117/117 [==============================] - 0s 298us/step - loss: 2.0157 - acc: 0.7436\n",
            "Epoch 205/500\n",
            "117/117 [==============================] - 0s 363us/step - loss: 2.0139 - acc: 0.7521\n",
            "Epoch 206/500\n",
            "117/117 [==============================] - 0s 336us/step - loss: 2.0103 - acc: 0.7436\n",
            "Epoch 207/500\n",
            "117/117 [==============================] - 0s 331us/step - loss: 2.0073 - acc: 0.7521\n",
            "Epoch 208/500\n",
            "117/117 [==============================] - 0s 366us/step - loss: 2.0051 - acc: 0.7521\n",
            "Epoch 209/500\n",
            "117/117 [==============================] - 0s 369us/step - loss: 2.0026 - acc: 0.7521\n",
            "Epoch 210/500\n",
            "117/117 [==============================] - 0s 378us/step - loss: 2.0010 - acc: 0.7521\n",
            "Epoch 211/500\n",
            "117/117 [==============================] - 0s 483us/step - loss: 1.9995 - acc: 0.7521\n",
            "Epoch 212/500\n",
            "117/117 [==============================] - 0s 381us/step - loss: 1.9968 - acc: 0.7521\n",
            "Epoch 213/500\n",
            "117/117 [==============================] - 0s 342us/step - loss: 1.9922 - acc: 0.7607\n",
            "Epoch 214/500\n",
            "117/117 [==============================] - 0s 408us/step - loss: 1.9889 - acc: 0.7521\n",
            "Epoch 215/500\n",
            "117/117 [==============================] - 0s 348us/step - loss: 1.9866 - acc: 0.7521\n",
            "Epoch 216/500\n",
            "117/117 [==============================] - 0s 338us/step - loss: 1.9841 - acc: 0.7607\n",
            "Epoch 217/500\n",
            "117/117 [==============================] - 0s 380us/step - loss: 1.9823 - acc: 0.7521\n",
            "Epoch 218/500\n",
            "117/117 [==============================] - 0s 315us/step - loss: 1.9789 - acc: 0.7521\n",
            "Epoch 219/500\n",
            "117/117 [==============================] - 0s 392us/step - loss: 1.9764 - acc: 0.7607\n",
            "Epoch 220/500\n",
            "117/117 [==============================] - 0s 380us/step - loss: 1.9732 - acc: 0.7521\n",
            "Epoch 221/500\n",
            "117/117 [==============================] - 0s 407us/step - loss: 1.9711 - acc: 0.7607\n",
            "Epoch 222/500\n",
            "117/117 [==============================] - 0s 400us/step - loss: 1.9684 - acc: 0.7607\n",
            "Epoch 223/500\n",
            "117/117 [==============================] - 0s 409us/step - loss: 1.9667 - acc: 0.7521\n",
            "Epoch 224/500\n",
            "117/117 [==============================] - 0s 347us/step - loss: 1.9640 - acc: 0.7521\n",
            "Epoch 225/500\n",
            "117/117 [==============================] - 0s 388us/step - loss: 1.9618 - acc: 0.7521\n",
            "Epoch 226/500\n",
            "117/117 [==============================] - 0s 329us/step - loss: 1.9584 - acc: 0.7521\n",
            "Epoch 227/500\n",
            "117/117 [==============================] - 0s 330us/step - loss: 1.9559 - acc: 0.7521\n",
            "Epoch 228/500\n",
            "117/117 [==============================] - 0s 353us/step - loss: 1.9531 - acc: 0.7607\n",
            "Epoch 229/500\n",
            "117/117 [==============================] - 0s 332us/step - loss: 1.9514 - acc: 0.7521\n",
            "Epoch 230/500\n",
            "117/117 [==============================] - 0s 333us/step - loss: 1.9484 - acc: 0.7521\n",
            "Epoch 231/500\n",
            "117/117 [==============================] - 0s 310us/step - loss: 1.9461 - acc: 0.7607\n",
            "Epoch 232/500\n",
            "117/117 [==============================] - 0s 324us/step - loss: 1.9428 - acc: 0.7607\n",
            "Epoch 233/500\n",
            "117/117 [==============================] - 0s 309us/step - loss: 1.9410 - acc: 0.7607\n",
            "Epoch 234/500\n",
            "117/117 [==============================] - 0s 375us/step - loss: 1.9383 - acc: 0.7521\n",
            "Epoch 235/500\n",
            "117/117 [==============================] - 0s 333us/step - loss: 1.9368 - acc: 0.7607\n",
            "Epoch 236/500\n",
            "117/117 [==============================] - 0s 355us/step - loss: 1.9333 - acc: 0.7521\n",
            "Epoch 237/500\n",
            "117/117 [==============================] - 0s 385us/step - loss: 1.9322 - acc: 0.7521\n",
            "Epoch 238/500\n",
            "117/117 [==============================] - 0s 374us/step - loss: 1.9295 - acc: 0.7607\n",
            "Epoch 239/500\n",
            "117/117 [==============================] - 0s 330us/step - loss: 1.9263 - acc: 0.7607\n",
            "Epoch 240/500\n",
            "117/117 [==============================] - 0s 395us/step - loss: 1.9241 - acc: 0.7436\n",
            "Epoch 241/500\n",
            "117/117 [==============================] - 0s 371us/step - loss: 1.9214 - acc: 0.7521\n",
            "Epoch 242/500\n",
            "117/117 [==============================] - 0s 376us/step - loss: 1.9191 - acc: 0.7521\n",
            "Epoch 243/500\n",
            "117/117 [==============================] - 0s 397us/step - loss: 1.9173 - acc: 0.7436\n",
            "Epoch 244/500\n",
            "117/117 [==============================] - 0s 474us/step - loss: 1.9150 - acc: 0.7607\n",
            "Epoch 245/500\n",
            "117/117 [==============================] - 0s 392us/step - loss: 1.9120 - acc: 0.7521\n",
            "Epoch 246/500\n",
            "117/117 [==============================] - 0s 439us/step - loss: 1.9097 - acc: 0.7521\n",
            "Epoch 247/500\n",
            "117/117 [==============================] - 0s 355us/step - loss: 1.9083 - acc: 0.7521\n",
            "Epoch 248/500\n",
            "117/117 [==============================] - 0s 532us/step - loss: 1.9058 - acc: 0.7521\n",
            "Epoch 249/500\n",
            "117/117 [==============================] - 0s 430us/step - loss: 1.9031 - acc: 0.7521\n",
            "Epoch 250/500\n",
            "117/117 [==============================] - 0s 402us/step - loss: 1.9007 - acc: 0.7521\n",
            "Epoch 251/500\n",
            "117/117 [==============================] - 0s 338us/step - loss: 1.8982 - acc: 0.7521\n",
            "Epoch 252/500\n",
            "117/117 [==============================] - 0s 370us/step - loss: 1.8965 - acc: 0.7521\n",
            "Epoch 253/500\n",
            "117/117 [==============================] - 0s 370us/step - loss: 1.8939 - acc: 0.7521\n",
            "Epoch 254/500\n",
            "117/117 [==============================] - 0s 359us/step - loss: 1.8914 - acc: 0.7521\n",
            "Epoch 255/500\n",
            "117/117 [==============================] - 0s 341us/step - loss: 1.8889 - acc: 0.7521\n",
            "Epoch 256/500\n",
            "117/117 [==============================] - 0s 374us/step - loss: 1.8866 - acc: 0.7521\n",
            "Epoch 257/500\n",
            "117/117 [==============================] - 0s 343us/step - loss: 1.8854 - acc: 0.7521\n",
            "Epoch 258/500\n",
            "117/117 [==============================] - 0s 331us/step - loss: 1.8832 - acc: 0.7521\n",
            "Epoch 259/500\n",
            "117/117 [==============================] - 0s 374us/step - loss: 1.8801 - acc: 0.7521\n",
            "Epoch 260/500\n",
            "117/117 [==============================] - 0s 335us/step - loss: 1.8786 - acc: 0.7521\n",
            "Epoch 261/500\n",
            "117/117 [==============================] - 0s 356us/step - loss: 1.8757 - acc: 0.7521\n",
            "Epoch 262/500\n",
            "117/117 [==============================] - 0s 378us/step - loss: 1.8736 - acc: 0.7521\n",
            "Epoch 263/500\n",
            "117/117 [==============================] - 0s 433us/step - loss: 1.8712 - acc: 0.7521\n",
            "Epoch 264/500\n",
            "117/117 [==============================] - 0s 415us/step - loss: 1.8692 - acc: 0.7521\n",
            "Epoch 265/500\n",
            "117/117 [==============================] - 0s 380us/step - loss: 1.8672 - acc: 0.7521\n",
            "Epoch 266/500\n",
            "117/117 [==============================] - 0s 405us/step - loss: 1.8646 - acc: 0.7521\n",
            "Epoch 267/500\n",
            "117/117 [==============================] - 0s 415us/step - loss: 1.8628 - acc: 0.7521\n",
            "Epoch 268/500\n",
            "117/117 [==============================] - 0s 438us/step - loss: 1.8621 - acc: 0.7521\n",
            "Epoch 269/500\n",
            "117/117 [==============================] - 0s 476us/step - loss: 1.8591 - acc: 0.7521\n",
            "Epoch 270/500\n",
            "117/117 [==============================] - 0s 418us/step - loss: 1.8565 - acc: 0.7521\n",
            "Epoch 271/500\n",
            "117/117 [==============================] - 0s 397us/step - loss: 1.8533 - acc: 0.7521\n",
            "Epoch 272/500\n",
            "117/117 [==============================] - 0s 346us/step - loss: 1.8521 - acc: 0.7521\n",
            "Epoch 273/500\n",
            "117/117 [==============================] - 0s 411us/step - loss: 1.8498 - acc: 0.7521\n",
            "Epoch 274/500\n",
            "117/117 [==============================] - 0s 364us/step - loss: 1.8468 - acc: 0.7521\n",
            "Epoch 275/500\n",
            "117/117 [==============================] - 0s 394us/step - loss: 1.8445 - acc: 0.7521\n",
            "Epoch 276/500\n",
            "117/117 [==============================] - 0s 345us/step - loss: 1.8427 - acc: 0.7521\n",
            "Epoch 277/500\n",
            "117/117 [==============================] - 0s 443us/step - loss: 1.8424 - acc: 0.7521\n",
            "Epoch 278/500\n",
            "117/117 [==============================] - 0s 500us/step - loss: 1.8392 - acc: 0.7521\n",
            "Epoch 279/500\n",
            "117/117 [==============================] - 0s 483us/step - loss: 1.8369 - acc: 0.7521\n",
            "Epoch 280/500\n",
            "117/117 [==============================] - 0s 368us/step - loss: 1.8356 - acc: 0.7521\n",
            "Epoch 281/500\n",
            "117/117 [==============================] - 0s 387us/step - loss: 1.8330 - acc: 0.7521\n",
            "Epoch 282/500\n",
            "117/117 [==============================] - 0s 466us/step - loss: 1.8313 - acc: 0.7521\n",
            "Epoch 283/500\n",
            "117/117 [==============================] - 0s 437us/step - loss: 1.8284 - acc: 0.7521\n",
            "Epoch 284/500\n",
            "117/117 [==============================] - 0s 476us/step - loss: 1.8268 - acc: 0.7607\n",
            "Epoch 285/500\n",
            "117/117 [==============================] - 0s 386us/step - loss: 1.8239 - acc: 0.7521\n",
            "Epoch 286/500\n",
            "117/117 [==============================] - 0s 393us/step - loss: 1.8222 - acc: 0.7607\n",
            "Epoch 287/500\n",
            "117/117 [==============================] - 0s 362us/step - loss: 1.8197 - acc: 0.7607\n",
            "Epoch 288/500\n",
            "117/117 [==============================] - 0s 322us/step - loss: 1.8175 - acc: 0.7521\n",
            "Epoch 289/500\n",
            "117/117 [==============================] - 0s 378us/step - loss: 1.8154 - acc: 0.7607\n",
            "Epoch 290/500\n",
            "117/117 [==============================] - 0s 353us/step - loss: 1.8145 - acc: 0.7607\n",
            "Epoch 291/500\n",
            "117/117 [==============================] - 0s 447us/step - loss: 1.8118 - acc: 0.7607\n",
            "Epoch 292/500\n",
            "117/117 [==============================] - 0s 370us/step - loss: 1.8091 - acc: 0.7607\n",
            "Epoch 293/500\n",
            "117/117 [==============================] - 0s 390us/step - loss: 1.8072 - acc: 0.7607\n",
            "Epoch 294/500\n",
            "117/117 [==============================] - 0s 350us/step - loss: 1.8046 - acc: 0.7607\n",
            "Epoch 295/500\n",
            "117/117 [==============================] - 0s 362us/step - loss: 1.8031 - acc: 0.7607\n",
            "Epoch 296/500\n",
            "117/117 [==============================] - 0s 349us/step - loss: 1.8008 - acc: 0.7607\n",
            "Epoch 297/500\n",
            "117/117 [==============================] - 0s 338us/step - loss: 1.8000 - acc: 0.7607\n",
            "Epoch 298/500\n",
            "117/117 [==============================] - 0s 380us/step - loss: 1.7972 - acc: 0.7607\n",
            "Epoch 299/500\n",
            "117/117 [==============================] - 0s 346us/step - loss: 1.7949 - acc: 0.7607\n",
            "Epoch 300/500\n",
            "117/117 [==============================] - 0s 351us/step - loss: 1.7935 - acc: 0.7607\n",
            "Epoch 301/500\n",
            "117/117 [==============================] - 0s 366us/step - loss: 1.7906 - acc: 0.7607\n",
            "Epoch 302/500\n",
            "117/117 [==============================] - 0s 327us/step - loss: 1.7897 - acc: 0.7607\n",
            "Epoch 303/500\n",
            "117/117 [==============================] - 0s 338us/step - loss: 1.7872 - acc: 0.7607\n",
            "Epoch 304/500\n",
            "117/117 [==============================] - 0s 353us/step - loss: 1.7857 - acc: 0.7692\n",
            "Epoch 305/500\n",
            "117/117 [==============================] - 0s 346us/step - loss: 1.7834 - acc: 0.7607\n",
            "Epoch 306/500\n",
            "117/117 [==============================] - 0s 353us/step - loss: 1.7817 - acc: 0.7692\n",
            "Epoch 307/500\n",
            "117/117 [==============================] - 0s 386us/step - loss: 1.7784 - acc: 0.7607\n",
            "Epoch 308/500\n",
            "117/117 [==============================] - 0s 371us/step - loss: 1.7778 - acc: 0.7521\n",
            "Epoch 309/500\n",
            "117/117 [==============================] - 0s 349us/step - loss: 1.7749 - acc: 0.7607\n",
            "Epoch 310/500\n",
            "117/117 [==============================] - 0s 373us/step - loss: 1.7730 - acc: 0.7607\n",
            "Epoch 311/500\n",
            "117/117 [==============================] - 0s 451us/step - loss: 1.7712 - acc: 0.7607\n",
            "Epoch 312/500\n",
            "117/117 [==============================] - 0s 427us/step - loss: 1.7699 - acc: 0.7607\n",
            "Epoch 313/500\n",
            "117/117 [==============================] - 0s 412us/step - loss: 1.7683 - acc: 0.7607\n",
            "Epoch 314/500\n",
            "117/117 [==============================] - 0s 406us/step - loss: 1.7652 - acc: 0.7607\n",
            "Epoch 315/500\n",
            "117/117 [==============================] - 0s 339us/step - loss: 1.7639 - acc: 0.7521\n",
            "Epoch 316/500\n",
            "117/117 [==============================] - 0s 395us/step - loss: 1.7617 - acc: 0.7607\n",
            "Epoch 317/500\n",
            "117/117 [==============================] - 0s 356us/step - loss: 1.7599 - acc: 0.7521\n",
            "Epoch 318/500\n",
            "117/117 [==============================] - 0s 385us/step - loss: 1.7583 - acc: 0.7521\n",
            "Epoch 319/500\n",
            "117/117 [==============================] - 0s 369us/step - loss: 1.7548 - acc: 0.7521\n",
            "Epoch 320/500\n",
            "117/117 [==============================] - 0s 350us/step - loss: 1.7531 - acc: 0.7607\n",
            "Epoch 321/500\n",
            "117/117 [==============================] - 0s 347us/step - loss: 1.7520 - acc: 0.7607\n",
            "Epoch 322/500\n",
            "117/117 [==============================] - 0s 345us/step - loss: 1.7501 - acc: 0.7607\n",
            "Epoch 323/500\n",
            "117/117 [==============================] - 0s 316us/step - loss: 1.7471 - acc: 0.7521\n",
            "Epoch 324/500\n",
            "117/117 [==============================] - 0s 309us/step - loss: 1.7462 - acc: 0.7607\n",
            "Epoch 325/500\n",
            "117/117 [==============================] - 0s 313us/step - loss: 1.7454 - acc: 0.7607\n",
            "Epoch 326/500\n",
            "117/117 [==============================] - 0s 342us/step - loss: 1.7431 - acc: 0.7607\n",
            "Epoch 327/500\n",
            "117/117 [==============================] - 0s 405us/step - loss: 1.7393 - acc: 0.7607\n",
            "Epoch 328/500\n",
            "117/117 [==============================] - 0s 314us/step - loss: 1.7384 - acc: 0.7607\n",
            "Epoch 329/500\n",
            "117/117 [==============================] - 0s 321us/step - loss: 1.7354 - acc: 0.7607\n",
            "Epoch 330/500\n",
            "117/117 [==============================] - 0s 391us/step - loss: 1.7333 - acc: 0.7521\n",
            "Epoch 331/500\n",
            "117/117 [==============================] - 0s 383us/step - loss: 1.7322 - acc: 0.7521\n",
            "Epoch 332/500\n",
            "117/117 [==============================] - 0s 347us/step - loss: 1.7304 - acc: 0.7607\n",
            "Epoch 333/500\n",
            "117/117 [==============================] - 0s 319us/step - loss: 1.7282 - acc: 0.7607\n",
            "Epoch 334/500\n",
            "117/117 [==============================] - 0s 326us/step - loss: 1.7267 - acc: 0.7607\n",
            "Epoch 335/500\n",
            "117/117 [==============================] - 0s 316us/step - loss: 1.7241 - acc: 0.7521\n",
            "Epoch 336/500\n",
            "117/117 [==============================] - 0s 340us/step - loss: 1.7234 - acc: 0.7607\n",
            "Epoch 337/500\n",
            "117/117 [==============================] - 0s 323us/step - loss: 1.7208 - acc: 0.7521\n",
            "Epoch 338/500\n",
            "117/117 [==============================] - 0s 322us/step - loss: 1.7184 - acc: 0.7521\n",
            "Epoch 339/500\n",
            "117/117 [==============================] - 0s 344us/step - loss: 1.7162 - acc: 0.7521\n",
            "Epoch 340/500\n",
            "117/117 [==============================] - 0s 327us/step - loss: 1.7152 - acc: 0.7521\n",
            "Epoch 341/500\n",
            "117/117 [==============================] - 0s 357us/step - loss: 1.7134 - acc: 0.7521\n",
            "Epoch 342/500\n",
            "117/117 [==============================] - 0s 359us/step - loss: 1.7105 - acc: 0.7521\n",
            "Epoch 343/500\n",
            "117/117 [==============================] - 0s 339us/step - loss: 1.7095 - acc: 0.7521\n",
            "Epoch 344/500\n",
            "117/117 [==============================] - 0s 327us/step - loss: 1.7067 - acc: 0.7521\n",
            "Epoch 345/500\n",
            "117/117 [==============================] - 0s 339us/step - loss: 1.7064 - acc: 0.7607\n",
            "Epoch 346/500\n",
            "117/117 [==============================] - 0s 370us/step - loss: 1.7038 - acc: 0.7607\n",
            "Epoch 347/500\n",
            "117/117 [==============================] - 0s 360us/step - loss: 1.7020 - acc: 0.7607\n",
            "Epoch 348/500\n",
            "117/117 [==============================] - 0s 365us/step - loss: 1.6994 - acc: 0.7607\n",
            "Epoch 349/500\n",
            "117/117 [==============================] - 0s 365us/step - loss: 1.6993 - acc: 0.7692\n",
            "Epoch 350/500\n",
            "117/117 [==============================] - 0s 367us/step - loss: 1.6973 - acc: 0.7607\n",
            "Epoch 351/500\n",
            "117/117 [==============================] - 0s 339us/step - loss: 1.6941 - acc: 0.7521\n",
            "Epoch 352/500\n",
            "117/117 [==============================] - 0s 333us/step - loss: 1.6928 - acc: 0.7607\n",
            "Epoch 353/500\n",
            "117/117 [==============================] - 0s 393us/step - loss: 1.6907 - acc: 0.7607\n",
            "Epoch 354/500\n",
            "117/117 [==============================] - 0s 357us/step - loss: 1.6895 - acc: 0.7607\n",
            "Epoch 355/500\n",
            "117/117 [==============================] - 0s 343us/step - loss: 1.6878 - acc: 0.7607\n",
            "Epoch 356/500\n",
            "117/117 [==============================] - 0s 341us/step - loss: 1.6871 - acc: 0.7607\n",
            "Epoch 357/500\n",
            "117/117 [==============================] - 0s 315us/step - loss: 1.6833 - acc: 0.7521\n",
            "Epoch 358/500\n",
            "117/117 [==============================] - 0s 316us/step - loss: 1.6823 - acc: 0.7607\n",
            "Epoch 359/500\n",
            "117/117 [==============================] - 0s 344us/step - loss: 1.6803 - acc: 0.7607\n",
            "Epoch 360/500\n",
            "117/117 [==============================] - 0s 352us/step - loss: 1.6779 - acc: 0.7607\n",
            "Epoch 361/500\n",
            "117/117 [==============================] - 0s 360us/step - loss: 1.6761 - acc: 0.7521\n",
            "Epoch 362/500\n",
            "117/117 [==============================] - 0s 348us/step - loss: 1.6746 - acc: 0.7521\n",
            "Epoch 363/500\n",
            "117/117 [==============================] - 0s 357us/step - loss: 1.6729 - acc: 0.7607\n",
            "Epoch 364/500\n",
            "117/117 [==============================] - 0s 376us/step - loss: 1.6715 - acc: 0.7607\n",
            "Epoch 365/500\n",
            "117/117 [==============================] - 0s 370us/step - loss: 1.6699 - acc: 0.7607\n",
            "Epoch 366/500\n",
            "117/117 [==============================] - 0s 373us/step - loss: 1.6681 - acc: 0.7521\n",
            "Epoch 367/500\n",
            "117/117 [==============================] - 0s 376us/step - loss: 1.6658 - acc: 0.7521\n",
            "Epoch 368/500\n",
            "117/117 [==============================] - 0s 343us/step - loss: 1.6635 - acc: 0.7521\n",
            "Epoch 369/500\n",
            "117/117 [==============================] - 0s 394us/step - loss: 1.6623 - acc: 0.7607\n",
            "Epoch 370/500\n",
            "117/117 [==============================] - 0s 376us/step - loss: 1.6610 - acc: 0.7607\n",
            "Epoch 371/500\n",
            "117/117 [==============================] - 0s 339us/step - loss: 1.6586 - acc: 0.7607\n",
            "Epoch 372/500\n",
            "117/117 [==============================] - 0s 340us/step - loss: 1.6585 - acc: 0.7607\n",
            "Epoch 373/500\n",
            "117/117 [==============================] - 0s 369us/step - loss: 1.6553 - acc: 0.7607\n",
            "Epoch 374/500\n",
            "117/117 [==============================] - 0s 354us/step - loss: 1.6541 - acc: 0.7607\n",
            "Epoch 375/500\n",
            "117/117 [==============================] - 0s 343us/step - loss: 1.6529 - acc: 0.7607\n",
            "Epoch 376/500\n",
            "117/117 [==============================] - 0s 317us/step - loss: 1.6501 - acc: 0.7607\n",
            "Epoch 377/500\n",
            "117/117 [==============================] - 0s 370us/step - loss: 1.6483 - acc: 0.7607\n",
            "Epoch 378/500\n",
            "117/117 [==============================] - 0s 351us/step - loss: 1.6475 - acc: 0.7607\n",
            "Epoch 379/500\n",
            "117/117 [==============================] - 0s 334us/step - loss: 1.6449 - acc: 0.7521\n",
            "Epoch 380/500\n",
            "117/117 [==============================] - 0s 386us/step - loss: 1.6435 - acc: 0.7607\n",
            "Epoch 381/500\n",
            "117/117 [==============================] - 0s 330us/step - loss: 1.6420 - acc: 0.7607\n",
            "Epoch 382/500\n",
            "117/117 [==============================] - 0s 317us/step - loss: 1.6410 - acc: 0.7607\n",
            "Epoch 383/500\n",
            "117/117 [==============================] - 0s 375us/step - loss: 1.6380 - acc: 0.7607\n",
            "Epoch 384/500\n",
            "117/117 [==============================] - 0s 332us/step - loss: 1.6368 - acc: 0.7607\n",
            "Epoch 385/500\n",
            "117/117 [==============================] - 0s 331us/step - loss: 1.6354 - acc: 0.7607\n",
            "Epoch 386/500\n",
            "117/117 [==============================] - 0s 340us/step - loss: 1.6328 - acc: 0.7607\n",
            "Epoch 387/500\n",
            "117/117 [==============================] - 0s 363us/step - loss: 1.6314 - acc: 0.7607\n",
            "Epoch 388/500\n",
            "117/117 [==============================] - 0s 353us/step - loss: 1.6289 - acc: 0.7607\n",
            "Epoch 389/500\n",
            "117/117 [==============================] - 0s 386us/step - loss: 1.6275 - acc: 0.7607\n",
            "Epoch 390/500\n",
            "117/117 [==============================] - 0s 376us/step - loss: 1.6269 - acc: 0.7607\n",
            "Epoch 391/500\n",
            "117/117 [==============================] - 0s 366us/step - loss: 1.6247 - acc: 0.7692\n",
            "Epoch 392/500\n",
            "117/117 [==============================] - 0s 393us/step - loss: 1.6222 - acc: 0.7607\n",
            "Epoch 393/500\n",
            "117/117 [==============================] - 0s 363us/step - loss: 1.6210 - acc: 0.7607\n",
            "Epoch 394/500\n",
            "117/117 [==============================] - 0s 346us/step - loss: 1.6203 - acc: 0.7607\n",
            "Epoch 395/500\n",
            "117/117 [==============================] - 0s 367us/step - loss: 1.6181 - acc: 0.7521\n",
            "Epoch 396/500\n",
            "117/117 [==============================] - 0s 341us/step - loss: 1.6163 - acc: 0.7607\n",
            "Epoch 397/500\n",
            "117/117 [==============================] - 0s 339us/step - loss: 1.6140 - acc: 0.7607\n",
            "Epoch 398/500\n",
            "117/117 [==============================] - 0s 346us/step - loss: 1.6120 - acc: 0.7607\n",
            "Epoch 399/500\n",
            "117/117 [==============================] - 0s 354us/step - loss: 1.6112 - acc: 0.7607\n",
            "Epoch 400/500\n",
            "117/117 [==============================] - 0s 347us/step - loss: 1.6087 - acc: 0.7607\n",
            "Epoch 401/500\n",
            "117/117 [==============================] - 0s 371us/step - loss: 1.6068 - acc: 0.7607\n",
            "Epoch 402/500\n",
            "117/117 [==============================] - 0s 359us/step - loss: 1.6052 - acc: 0.7607\n",
            "Epoch 403/500\n",
            "117/117 [==============================] - 0s 336us/step - loss: 1.6043 - acc: 0.7607\n",
            "Epoch 404/500\n",
            "117/117 [==============================] - 0s 350us/step - loss: 1.6040 - acc: 0.7607\n",
            "Epoch 405/500\n",
            "117/117 [==============================] - 0s 378us/step - loss: 1.6034 - acc: 0.7692\n",
            "Epoch 406/500\n",
            "117/117 [==============================] - 0s 323us/step - loss: 1.5999 - acc: 0.7607\n",
            "Epoch 407/500\n",
            "117/117 [==============================] - 0s 319us/step - loss: 1.5987 - acc: 0.7607\n",
            "Epoch 408/500\n",
            "117/117 [==============================] - 0s 351us/step - loss: 1.5953 - acc: 0.7692\n",
            "Epoch 409/500\n",
            "117/117 [==============================] - 0s 375us/step - loss: 1.5952 - acc: 0.7521\n",
            "Epoch 410/500\n",
            "117/117 [==============================] - 0s 321us/step - loss: 1.5927 - acc: 0.7521\n",
            "Epoch 411/500\n",
            "117/117 [==============================] - 0s 370us/step - loss: 1.5902 - acc: 0.7607\n",
            "Epoch 412/500\n",
            "117/117 [==============================] - 0s 387us/step - loss: 1.5906 - acc: 0.7692\n",
            "Epoch 413/500\n",
            "117/117 [==============================] - 0s 347us/step - loss: 1.5877 - acc: 0.7692\n",
            "Epoch 414/500\n",
            "117/117 [==============================] - 0s 331us/step - loss: 1.5861 - acc: 0.7778\n",
            "Epoch 415/500\n",
            "117/117 [==============================] - 0s 396us/step - loss: 1.5843 - acc: 0.7692\n",
            "Epoch 416/500\n",
            "117/117 [==============================] - 0s 412us/step - loss: 1.5830 - acc: 0.7778\n",
            "Epoch 417/500\n",
            "117/117 [==============================] - 0s 445us/step - loss: 1.5820 - acc: 0.7607\n",
            "Epoch 418/500\n",
            "117/117 [==============================] - 0s 384us/step - loss: 1.5801 - acc: 0.7607\n",
            "Epoch 419/500\n",
            "117/117 [==============================] - 0s 346us/step - loss: 1.5773 - acc: 0.7692\n",
            "Epoch 420/500\n",
            "117/117 [==============================] - 0s 349us/step - loss: 1.5770 - acc: 0.7692\n",
            "Epoch 421/500\n",
            "117/117 [==============================] - 0s 365us/step - loss: 1.5757 - acc: 0.7692\n",
            "Epoch 422/500\n",
            "117/117 [==============================] - 0s 365us/step - loss: 1.5737 - acc: 0.7692\n",
            "Epoch 423/500\n",
            "117/117 [==============================] - 0s 342us/step - loss: 1.5723 - acc: 0.7778\n",
            "Epoch 424/500\n",
            "117/117 [==============================] - 0s 376us/step - loss: 1.5715 - acc: 0.7692\n",
            "Epoch 425/500\n",
            "117/117 [==============================] - 0s 343us/step - loss: 1.5696 - acc: 0.7607\n",
            "Epoch 426/500\n",
            "117/117 [==============================] - 0s 381us/step - loss: 1.5668 - acc: 0.7607\n",
            "Epoch 427/500\n",
            "117/117 [==============================] - 0s 345us/step - loss: 1.5650 - acc: 0.7778\n",
            "Epoch 428/500\n",
            "117/117 [==============================] - 0s 347us/step - loss: 1.5649 - acc: 0.7692\n",
            "Epoch 429/500\n",
            "117/117 [==============================] - 0s 359us/step - loss: 1.5623 - acc: 0.7692\n",
            "Epoch 430/500\n",
            "117/117 [==============================] - 0s 369us/step - loss: 1.5613 - acc: 0.7778\n",
            "Epoch 431/500\n",
            "117/117 [==============================] - 0s 332us/step - loss: 1.5589 - acc: 0.7607\n",
            "Epoch 432/500\n",
            "117/117 [==============================] - 0s 305us/step - loss: 1.5587 - acc: 0.7778\n",
            "Epoch 433/500\n",
            "117/117 [==============================] - 0s 312us/step - loss: 1.5552 - acc: 0.7607\n",
            "Epoch 434/500\n",
            "117/117 [==============================] - 0s 324us/step - loss: 1.5537 - acc: 0.7692\n",
            "Epoch 435/500\n",
            "117/117 [==============================] - 0s 366us/step - loss: 1.5542 - acc: 0.7692\n",
            "Epoch 436/500\n",
            "117/117 [==============================] - 0s 377us/step - loss: 1.5516 - acc: 0.7863\n",
            "Epoch 437/500\n",
            "117/117 [==============================] - 0s 376us/step - loss: 1.5499 - acc: 0.7778\n",
            "Epoch 438/500\n",
            "117/117 [==============================] - 0s 358us/step - loss: 1.5481 - acc: 0.7692\n",
            "Epoch 439/500\n",
            "117/117 [==============================] - 0s 393us/step - loss: 1.5456 - acc: 0.7778\n",
            "Epoch 440/500\n",
            "117/117 [==============================] - 0s 361us/step - loss: 1.5447 - acc: 0.7778\n",
            "Epoch 441/500\n",
            "117/117 [==============================] - 0s 368us/step - loss: 1.5429 - acc: 0.7692\n",
            "Epoch 442/500\n",
            "117/117 [==============================] - 0s 421us/step - loss: 1.5423 - acc: 0.7778\n",
            "Epoch 443/500\n",
            "117/117 [==============================] - 0s 418us/step - loss: 1.5420 - acc: 0.7692\n",
            "Epoch 444/500\n",
            "117/117 [==============================] - 0s 374us/step - loss: 1.5385 - acc: 0.7778\n",
            "Epoch 445/500\n",
            "117/117 [==============================] - 0s 376us/step - loss: 1.5367 - acc: 0.7692\n",
            "Epoch 446/500\n",
            "117/117 [==============================] - 0s 433us/step - loss: 1.5360 - acc: 0.7778\n",
            "Epoch 447/500\n",
            "117/117 [==============================] - 0s 361us/step - loss: 1.5343 - acc: 0.7863\n",
            "Epoch 448/500\n",
            "117/117 [==============================] - 0s 376us/step - loss: 1.5331 - acc: 0.7778\n",
            "Epoch 449/500\n",
            "117/117 [==============================] - 0s 389us/step - loss: 1.5315 - acc: 0.7778\n",
            "Epoch 450/500\n",
            "117/117 [==============================] - 0s 366us/step - loss: 1.5294 - acc: 0.7863\n",
            "Epoch 451/500\n",
            "117/117 [==============================] - 0s 374us/step - loss: 1.5283 - acc: 0.7778\n",
            "Epoch 452/500\n",
            "117/117 [==============================] - 0s 327us/step - loss: 1.5268 - acc: 0.7778\n",
            "Epoch 453/500\n",
            "117/117 [==============================] - 0s 364us/step - loss: 1.5252 - acc: 0.7863\n",
            "Epoch 454/500\n",
            "117/117 [==============================] - 0s 365us/step - loss: 1.5241 - acc: 0.7778\n",
            "Epoch 455/500\n",
            "117/117 [==============================] - 0s 380us/step - loss: 1.5219 - acc: 0.7863\n",
            "Epoch 456/500\n",
            "117/117 [==============================] - 0s 342us/step - loss: 1.5199 - acc: 0.7863\n",
            "Epoch 457/500\n",
            "117/117 [==============================] - 0s 364us/step - loss: 1.5184 - acc: 0.7778\n",
            "Epoch 458/500\n",
            "117/117 [==============================] - 0s 318us/step - loss: 1.5172 - acc: 0.7778\n",
            "Epoch 459/500\n",
            "117/117 [==============================] - 0s 322us/step - loss: 1.5171 - acc: 0.7863\n",
            "Epoch 460/500\n",
            "117/117 [==============================] - 0s 374us/step - loss: 1.5144 - acc: 0.7778\n",
            "Epoch 461/500\n",
            "117/117 [==============================] - 0s 347us/step - loss: 1.5140 - acc: 0.7863\n",
            "Epoch 462/500\n",
            "117/117 [==============================] - 0s 362us/step - loss: 1.5108 - acc: 0.7778\n",
            "Epoch 463/500\n",
            "117/117 [==============================] - 0s 360us/step - loss: 1.5101 - acc: 0.7778\n",
            "Epoch 464/500\n",
            "117/117 [==============================] - 0s 406us/step - loss: 1.5088 - acc: 0.7778\n",
            "Epoch 465/500\n",
            "117/117 [==============================] - 0s 354us/step - loss: 1.5063 - acc: 0.7778\n",
            "Epoch 466/500\n",
            "117/117 [==============================] - 0s 362us/step - loss: 1.5050 - acc: 0.7863\n",
            "Epoch 467/500\n",
            "117/117 [==============================] - 0s 337us/step - loss: 1.5051 - acc: 0.7863\n",
            "Epoch 468/500\n",
            "117/117 [==============================] - 0s 402us/step - loss: 1.5044 - acc: 0.7863\n",
            "Epoch 469/500\n",
            "117/117 [==============================] - 0s 334us/step - loss: 1.5019 - acc: 0.7863\n",
            "Epoch 470/500\n",
            "117/117 [==============================] - 0s 320us/step - loss: 1.4997 - acc: 0.7778\n",
            "Epoch 471/500\n",
            "117/117 [==============================] - 0s 351us/step - loss: 1.4985 - acc: 0.7863\n",
            "Epoch 472/500\n",
            "117/117 [==============================] - 0s 360us/step - loss: 1.4966 - acc: 0.7863\n",
            "Epoch 473/500\n",
            "117/117 [==============================] - 0s 335us/step - loss: 1.4953 - acc: 0.7778\n",
            "Epoch 474/500\n",
            "117/117 [==============================] - 0s 331us/step - loss: 1.4924 - acc: 0.7863\n",
            "Epoch 475/500\n",
            "117/117 [==============================] - 0s 353us/step - loss: 1.4920 - acc: 0.7863\n",
            "Epoch 476/500\n",
            "117/117 [==============================] - 0s 388us/step - loss: 1.4907 - acc: 0.7863\n",
            "Epoch 477/500\n",
            "117/117 [==============================] - 0s 392us/step - loss: 1.4887 - acc: 0.7863\n",
            "Epoch 478/500\n",
            "117/117 [==============================] - 0s 350us/step - loss: 1.4883 - acc: 0.7778\n",
            "Epoch 479/500\n",
            "117/117 [==============================] - 0s 350us/step - loss: 1.4870 - acc: 0.7863\n",
            "Epoch 480/500\n",
            "117/117 [==============================] - 0s 384us/step - loss: 1.4848 - acc: 0.7778\n",
            "Epoch 481/500\n",
            "117/117 [==============================] - 0s 403us/step - loss: 1.4842 - acc: 0.7778\n",
            "Epoch 482/500\n",
            "117/117 [==============================] - 0s 357us/step - loss: 1.4831 - acc: 0.7949\n",
            "Epoch 483/500\n",
            "117/117 [==============================] - 0s 418us/step - loss: 1.4811 - acc: 0.7778\n",
            "Epoch 484/500\n",
            "117/117 [==============================] - 0s 422us/step - loss: 1.4783 - acc: 0.7778\n",
            "Epoch 485/500\n",
            "117/117 [==============================] - 0s 352us/step - loss: 1.4791 - acc: 0.7778\n",
            "Epoch 486/500\n",
            "117/117 [==============================] - 0s 366us/step - loss: 1.4768 - acc: 0.7863\n",
            "Epoch 487/500\n",
            "117/117 [==============================] - 0s 336us/step - loss: 1.4751 - acc: 0.7863\n",
            "Epoch 488/500\n",
            "117/117 [==============================] - 0s 373us/step - loss: 1.4738 - acc: 0.7778\n",
            "Epoch 489/500\n",
            "117/117 [==============================] - 0s 367us/step - loss: 1.4719 - acc: 0.7778\n",
            "Epoch 490/500\n",
            "117/117 [==============================] - 0s 343us/step - loss: 1.4706 - acc: 0.7863\n",
            "Epoch 491/500\n",
            "117/117 [==============================] - 0s 350us/step - loss: 1.4693 - acc: 0.7863\n",
            "Epoch 492/500\n",
            "117/117 [==============================] - 0s 305us/step - loss: 1.4686 - acc: 0.7949\n",
            "Epoch 493/500\n",
            "117/117 [==============================] - 0s 352us/step - loss: 1.4680 - acc: 0.7949\n",
            "Epoch 494/500\n",
            "117/117 [==============================] - 0s 325us/step - loss: 1.4663 - acc: 0.7778\n",
            "Epoch 495/500\n",
            "117/117 [==============================] - 0s 315us/step - loss: 1.4642 - acc: 0.7949\n",
            "Epoch 496/500\n",
            "117/117 [==============================] - 0s 300us/step - loss: 1.4627 - acc: 0.7778\n",
            "Epoch 497/500\n",
            "117/117 [==============================] - 0s 402us/step - loss: 1.4610 - acc: 0.7863\n",
            "Epoch 498/500\n",
            "117/117 [==============================] - 0s 398us/step - loss: 1.4597 - acc: 0.8034\n",
            "Epoch 499/500\n",
            "117/117 [==============================] - 0s 366us/step - loss: 1.4577 - acc: 0.7949\n",
            "Epoch 500/500\n",
            "117/117 [==============================] - 0s 362us/step - loss: 1.4561 - acc: 0.7949\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe9b965cdd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSFuSXKQ4zwA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "f12aaffc-468d-45d5-c781-1f069d764901"
      },
      "source": [
        "# Predicting vanilla neural network with dropout labels\n",
        "\n",
        "print(nn_penalized.predict_classes(prediction_input_preprocessor.transform(X_test)))\n",
        "\n",
        "prediction_index_nn_penalized=nn_penalized.predict_classes(prediction_input_preprocessor.transform(X_test))\n",
        "\n",
        "labels=processed_y_train.columns\n",
        "\n",
        "def index_to_label(labels,index_n): \n",
        "    return labels[index_n]\n",
        "    \n",
        "index_to_label(labels,1)\n",
        "\n",
        "nn_penalized_predicted_labels=list(map(lambda x: labels[x], prediction_index_nn_penalized))\n",
        "print(nn_penalized_predicted_labels)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 0 0 1 4 1 2 0 0 2 1 2 3 2 1 1 4 4 2 0 4 3 1 3 1 2 0 1 0 4 0 2 1 4 0 4 4\n",
            " 4 1]\n",
            "['High', 'Average', 'Average', 'High', 'Very Low', 'High', 'Low', 'Average', 'Average', 'Low', 'High', 'Low', 'Very High', 'Low', 'High', 'High', 'Very Low', 'Very Low', 'Low', 'Average', 'Very Low', 'Very High', 'High', 'Very High', 'High', 'Low', 'Average', 'High', 'Average', 'Very Low', 'Average', 'Low', 'High', 'Very Low', 'Average', 'Very Low', 'Very Low', 'Very Low', 'High']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMTdiQRu5WTO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "outputId": "232bb228-8123-41b8-b23e-6ed1b00e47df"
      },
      "source": [
        "# Vanilla neural network with L2 penalization evaluation\n",
        "\n",
        "model_eval_metrics(y_test,nn_penalized_predicted_labels,classification=\"TRUE\")\n",
        "nn_penalized_model_eval = model_eval_metrics( y_test,nn_penalized_predicted_labels,classification=\"TRUE\")\n",
        "nn_penalized_model_eval"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>f1_score</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>mse</th>\n",
              "      <th>rmse</th>\n",
              "      <th>mae</th>\n",
              "      <th>r2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.435897</td>\n",
              "      <td>0.441444</td>\n",
              "      <td>0.507648</td>\n",
              "      <td>0.452778</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   accuracy  f1_score  precision    recall  mse  rmse  mae  r2\n",
              "0  0.435897  0.441444   0.507648  0.452778    0     0    0   0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7kfiYIV7cce",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a697237a-cae1-4ae8-8d3c-585ce7f8634c"
      },
      "source": [
        "# L2 penalized NN model with extra hidden layers\n",
        "\n",
        "nn_pen_extra = Sequential()\n",
        "nn_pen_extra.add(Dense(64, input_dim=20, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
        "nn_pen_extra.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
        "nn_pen_extra.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
        "nn_pen_extra.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
        "nn_pen_extra.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
        "nn_pen_extra.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
        "nn_pen_extra.add(Dense(5, activation='softmax')) \n",
        "                                            \n",
        "nn_pen_extra.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "\n",
        "nn_pen_extra.fit(processed_X_train, processed_y_train, epochs = 500) "
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "117/117 [==============================] - 9s 76ms/step - loss: 5.1078 - acc: 0.2479\n",
            "Epoch 2/500\n",
            "117/117 [==============================] - 0s 321us/step - loss: 5.0991 - acc: 0.2479\n",
            "Epoch 3/500\n",
            "117/117 [==============================] - 0s 327us/step - loss: 5.0906 - acc: 0.2479\n",
            "Epoch 4/500\n",
            "117/117 [==============================] - 0s 355us/step - loss: 5.0825 - acc: 0.2906\n",
            "Epoch 5/500\n",
            "117/117 [==============================] - 0s 356us/step - loss: 5.0744 - acc: 0.2821\n",
            "Epoch 6/500\n",
            "117/117 [==============================] - 0s 355us/step - loss: 5.0666 - acc: 0.3077\n",
            "Epoch 7/500\n",
            "117/117 [==============================] - 0s 352us/step - loss: 5.0589 - acc: 0.3077\n",
            "Epoch 8/500\n",
            "117/117 [==============================] - 0s 413us/step - loss: 5.0511 - acc: 0.3077\n",
            "Epoch 9/500\n",
            "117/117 [==============================] - 0s 394us/step - loss: 5.0437 - acc: 0.3248\n",
            "Epoch 10/500\n",
            "117/117 [==============================] - 0s 377us/step - loss: 5.0361 - acc: 0.3333\n",
            "Epoch 11/500\n",
            "117/117 [==============================] - 0s 348us/step - loss: 5.0288 - acc: 0.3162\n",
            "Epoch 12/500\n",
            "117/117 [==============================] - 0s 367us/step - loss: 5.0217 - acc: 0.3333\n",
            "Epoch 13/500\n",
            "117/117 [==============================] - 0s 339us/step - loss: 5.0147 - acc: 0.3504\n",
            "Epoch 14/500\n",
            "117/117 [==============================] - 0s 353us/step - loss: 5.0075 - acc: 0.3675\n",
            "Epoch 15/500\n",
            "117/117 [==============================] - 0s 350us/step - loss: 5.0005 - acc: 0.3846\n",
            "Epoch 16/500\n",
            "117/117 [==============================] - 0s 324us/step - loss: 4.9939 - acc: 0.3846\n",
            "Epoch 17/500\n",
            "117/117 [==============================] - 0s 354us/step - loss: 4.9867 - acc: 0.3419\n",
            "Epoch 18/500\n",
            "117/117 [==============================] - 0s 405us/step - loss: 4.9802 - acc: 0.3590\n",
            "Epoch 19/500\n",
            "117/117 [==============================] - 0s 346us/step - loss: 4.9732 - acc: 0.3846\n",
            "Epoch 20/500\n",
            "117/117 [==============================] - 0s 342us/step - loss: 4.9663 - acc: 0.3846\n",
            "Epoch 21/500\n",
            "117/117 [==============================] - 0s 335us/step - loss: 4.9593 - acc: 0.3846\n",
            "Epoch 22/500\n",
            "117/117 [==============================] - 0s 351us/step - loss: 4.9525 - acc: 0.3846\n",
            "Epoch 23/500\n",
            "117/117 [==============================] - 0s 379us/step - loss: 4.9457 - acc: 0.3675\n",
            "Epoch 24/500\n",
            "117/117 [==============================] - 0s 380us/step - loss: 4.9388 - acc: 0.3761\n",
            "Epoch 25/500\n",
            "117/117 [==============================] - 0s 364us/step - loss: 4.9321 - acc: 0.3846\n",
            "Epoch 26/500\n",
            "117/117 [==============================] - 0s 400us/step - loss: 4.9255 - acc: 0.4017\n",
            "Epoch 27/500\n",
            "117/117 [==============================] - 0s 338us/step - loss: 4.9184 - acc: 0.4103\n",
            "Epoch 28/500\n",
            "117/117 [==============================] - 0s 330us/step - loss: 4.9116 - acc: 0.4103\n",
            "Epoch 29/500\n",
            "117/117 [==============================] - 0s 358us/step - loss: 4.9046 - acc: 0.4444\n",
            "Epoch 30/500\n",
            "117/117 [==============================] - 0s 347us/step - loss: 4.8978 - acc: 0.4530\n",
            "Epoch 31/500\n",
            "117/117 [==============================] - 0s 294us/step - loss: 4.8910 - acc: 0.4444\n",
            "Epoch 32/500\n",
            "117/117 [==============================] - 0s 348us/step - loss: 4.8839 - acc: 0.4530\n",
            "Epoch 33/500\n",
            "117/117 [==============================] - 0s 354us/step - loss: 4.8771 - acc: 0.4701\n",
            "Epoch 34/500\n",
            "117/117 [==============================] - 0s 356us/step - loss: 4.8701 - acc: 0.4615\n",
            "Epoch 35/500\n",
            "117/117 [==============================] - 0s 415us/step - loss: 4.8631 - acc: 0.4786\n",
            "Epoch 36/500\n",
            "117/117 [==============================] - 0s 381us/step - loss: 4.8563 - acc: 0.4615\n",
            "Epoch 37/500\n",
            "117/117 [==============================] - 0s 420us/step - loss: 4.8492 - acc: 0.4786\n",
            "Epoch 38/500\n",
            "117/117 [==============================] - 0s 424us/step - loss: 4.8422 - acc: 0.4786\n",
            "Epoch 39/500\n",
            "117/117 [==============================] - 0s 395us/step - loss: 4.8352 - acc: 0.4957\n",
            "Epoch 40/500\n",
            "117/117 [==============================] - 0s 344us/step - loss: 4.8281 - acc: 0.5128\n",
            "Epoch 41/500\n",
            "117/117 [==============================] - 0s 396us/step - loss: 4.8210 - acc: 0.5128\n",
            "Epoch 42/500\n",
            "117/117 [==============================] - 0s 389us/step - loss: 4.8139 - acc: 0.5043\n",
            "Epoch 43/500\n",
            "117/117 [==============================] - 0s 329us/step - loss: 4.8067 - acc: 0.5128\n",
            "Epoch 44/500\n",
            "117/117 [==============================] - 0s 380us/step - loss: 4.7994 - acc: 0.5299\n",
            "Epoch 45/500\n",
            "117/117 [==============================] - 0s 365us/step - loss: 4.7922 - acc: 0.5299\n",
            "Epoch 46/500\n",
            "117/117 [==============================] - 0s 394us/step - loss: 4.7847 - acc: 0.5214\n",
            "Epoch 47/500\n",
            "117/117 [==============================] - 0s 358us/step - loss: 4.7774 - acc: 0.5299\n",
            "Epoch 48/500\n",
            "117/117 [==============================] - 0s 344us/step - loss: 4.7698 - acc: 0.5385\n",
            "Epoch 49/500\n",
            "117/117 [==============================] - 0s 347us/step - loss: 4.7626 - acc: 0.5385\n",
            "Epoch 50/500\n",
            "117/117 [==============================] - 0s 378us/step - loss: 4.7551 - acc: 0.5299\n",
            "Epoch 51/500\n",
            "117/117 [==============================] - 0s 412us/step - loss: 4.7474 - acc: 0.5299\n",
            "Epoch 52/500\n",
            "117/117 [==============================] - 0s 351us/step - loss: 4.7397 - acc: 0.5299\n",
            "Epoch 53/500\n",
            "117/117 [==============================] - 0s 326us/step - loss: 4.7318 - acc: 0.5214\n",
            "Epoch 54/500\n",
            "117/117 [==============================] - 0s 328us/step - loss: 4.7241 - acc: 0.5214\n",
            "Epoch 55/500\n",
            "117/117 [==============================] - 0s 357us/step - loss: 4.7161 - acc: 0.5299\n",
            "Epoch 56/500\n",
            "117/117 [==============================] - 0s 359us/step - loss: 4.7080 - acc: 0.5299\n",
            "Epoch 57/500\n",
            "117/117 [==============================] - 0s 308us/step - loss: 4.6999 - acc: 0.5299\n",
            "Epoch 58/500\n",
            "117/117 [==============================] - 0s 325us/step - loss: 4.6919 - acc: 0.5214\n",
            "Epoch 59/500\n",
            "117/117 [==============================] - 0s 319us/step - loss: 4.6835 - acc: 0.5214\n",
            "Epoch 60/500\n",
            "117/117 [==============================] - 0s 357us/step - loss: 4.6754 - acc: 0.5214\n",
            "Epoch 61/500\n",
            "117/117 [==============================] - 0s 331us/step - loss: 4.6669 - acc: 0.5214\n",
            "Epoch 62/500\n",
            "117/117 [==============================] - 0s 370us/step - loss: 4.6584 - acc: 0.5214\n",
            "Epoch 63/500\n",
            "117/117 [==============================] - 0s 414us/step - loss: 4.6495 - acc: 0.5128\n",
            "Epoch 64/500\n",
            "117/117 [==============================] - 0s 359us/step - loss: 4.6409 - acc: 0.5214\n",
            "Epoch 65/500\n",
            "117/117 [==============================] - 0s 324us/step - loss: 4.6319 - acc: 0.5214\n",
            "Epoch 66/500\n",
            "117/117 [==============================] - 0s 320us/step - loss: 4.6231 - acc: 0.5299\n",
            "Epoch 67/500\n",
            "117/117 [==============================] - 0s 344us/step - loss: 4.6137 - acc: 0.5214\n",
            "Epoch 68/500\n",
            "117/117 [==============================] - 0s 296us/step - loss: 4.6045 - acc: 0.5214\n",
            "Epoch 69/500\n",
            "117/117 [==============================] - 0s 347us/step - loss: 4.5954 - acc: 0.5214\n",
            "Epoch 70/500\n",
            "117/117 [==============================] - 0s 425us/step - loss: 4.5855 - acc: 0.5214\n",
            "Epoch 71/500\n",
            "117/117 [==============================] - 0s 389us/step - loss: 4.5768 - acc: 0.5214\n",
            "Epoch 72/500\n",
            "117/117 [==============================] - 0s 322us/step - loss: 4.5663 - acc: 0.5214\n",
            "Epoch 73/500\n",
            "117/117 [==============================] - 0s 375us/step - loss: 4.5567 - acc: 0.5128\n",
            "Epoch 74/500\n",
            "117/117 [==============================] - 0s 312us/step - loss: 4.5461 - acc: 0.5214\n",
            "Epoch 75/500\n",
            "117/117 [==============================] - 0s 315us/step - loss: 4.5363 - acc: 0.5043\n",
            "Epoch 76/500\n",
            "117/117 [==============================] - 0s 340us/step - loss: 4.5255 - acc: 0.5128\n",
            "Epoch 77/500\n",
            "117/117 [==============================] - 0s 317us/step - loss: 4.5147 - acc: 0.5043\n",
            "Epoch 78/500\n",
            "117/117 [==============================] - 0s 466us/step - loss: 4.5038 - acc: 0.5043\n",
            "Epoch 79/500\n",
            "117/117 [==============================] - 0s 366us/step - loss: 4.4928 - acc: 0.5043\n",
            "Epoch 80/500\n",
            "117/117 [==============================] - 0s 335us/step - loss: 4.4814 - acc: 0.4957\n",
            "Epoch 81/500\n",
            "117/117 [==============================] - 0s 375us/step - loss: 4.4701 - acc: 0.5043\n",
            "Epoch 82/500\n",
            "117/117 [==============================] - 0s 467us/step - loss: 4.4585 - acc: 0.4957\n",
            "Epoch 83/500\n",
            "117/117 [==============================] - 0s 343us/step - loss: 4.4462 - acc: 0.5043\n",
            "Epoch 84/500\n",
            "117/117 [==============================] - 0s 391us/step - loss: 4.4341 - acc: 0.5043\n",
            "Epoch 85/500\n",
            "117/117 [==============================] - 0s 353us/step - loss: 4.4217 - acc: 0.5043\n",
            "Epoch 86/500\n",
            "117/117 [==============================] - 0s 353us/step - loss: 4.4097 - acc: 0.5043\n",
            "Epoch 87/500\n",
            "117/117 [==============================] - 0s 436us/step - loss: 4.3970 - acc: 0.5043\n",
            "Epoch 88/500\n",
            "117/117 [==============================] - 0s 395us/step - loss: 4.3845 - acc: 0.5043\n",
            "Epoch 89/500\n",
            "117/117 [==============================] - 0s 303us/step - loss: 4.3716 - acc: 0.5128\n",
            "Epoch 90/500\n",
            "117/117 [==============================] - 0s 354us/step - loss: 4.3591 - acc: 0.5128\n",
            "Epoch 91/500\n",
            "117/117 [==============================] - 0s 378us/step - loss: 4.3469 - acc: 0.5043\n",
            "Epoch 92/500\n",
            "117/117 [==============================] - 0s 383us/step - loss: 4.3338 - acc: 0.5128\n",
            "Epoch 93/500\n",
            "117/117 [==============================] - 0s 378us/step - loss: 4.3215 - acc: 0.5043\n",
            "Epoch 94/500\n",
            "117/117 [==============================] - 0s 369us/step - loss: 4.3082 - acc: 0.4872\n",
            "Epoch 95/500\n",
            "117/117 [==============================] - 0s 346us/step - loss: 4.2958 - acc: 0.4957\n",
            "Epoch 96/500\n",
            "117/117 [==============================] - 0s 351us/step - loss: 4.2831 - acc: 0.4957\n",
            "Epoch 97/500\n",
            "117/117 [==============================] - 0s 381us/step - loss: 4.2707 - acc: 0.4957\n",
            "Epoch 98/500\n",
            "117/117 [==============================] - 0s 382us/step - loss: 4.2589 - acc: 0.5128\n",
            "Epoch 99/500\n",
            "117/117 [==============================] - 0s 381us/step - loss: 4.2461 - acc: 0.5128\n",
            "Epoch 100/500\n",
            "117/117 [==============================] - 0s 404us/step - loss: 4.2344 - acc: 0.5043\n",
            "Epoch 101/500\n",
            "117/117 [==============================] - 0s 351us/step - loss: 4.2227 - acc: 0.4957\n",
            "Epoch 102/500\n",
            "117/117 [==============================] - 0s 370us/step - loss: 4.2098 - acc: 0.5128\n",
            "Epoch 103/500\n",
            "117/117 [==============================] - 0s 352us/step - loss: 4.1983 - acc: 0.5128\n",
            "Epoch 104/500\n",
            "117/117 [==============================] - 0s 385us/step - loss: 4.1860 - acc: 0.5214\n",
            "Epoch 105/500\n",
            "117/117 [==============================] - 0s 375us/step - loss: 4.1748 - acc: 0.5214\n",
            "Epoch 106/500\n",
            "117/117 [==============================] - 0s 350us/step - loss: 4.1630 - acc: 0.5128\n",
            "Epoch 107/500\n",
            "117/117 [==============================] - 0s 350us/step - loss: 4.1524 - acc: 0.5385\n",
            "Epoch 108/500\n",
            "117/117 [==============================] - 0s 444us/step - loss: 4.1411 - acc: 0.5385\n",
            "Epoch 109/500\n",
            "117/117 [==============================] - 0s 421us/step - loss: 4.1302 - acc: 0.5385\n",
            "Epoch 110/500\n",
            "117/117 [==============================] - 0s 452us/step - loss: 4.1182 - acc: 0.5470\n",
            "Epoch 111/500\n",
            "117/117 [==============================] - 0s 402us/step - loss: 4.1078 - acc: 0.5470\n",
            "Epoch 112/500\n",
            "117/117 [==============================] - 0s 367us/step - loss: 4.0957 - acc: 0.5470\n",
            "Epoch 113/500\n",
            "117/117 [==============================] - 0s 401us/step - loss: 4.0859 - acc: 0.5470\n",
            "Epoch 114/500\n",
            "117/117 [==============================] - 0s 513us/step - loss: 4.0752 - acc: 0.5726\n",
            "Epoch 115/500\n",
            "117/117 [==============================] - 0s 408us/step - loss: 4.0640 - acc: 0.5726\n",
            "Epoch 116/500\n",
            "117/117 [==============================] - 0s 404us/step - loss: 4.0541 - acc: 0.5726\n",
            "Epoch 117/500\n",
            "117/117 [==============================] - 0s 446us/step - loss: 4.0444 - acc: 0.5726\n",
            "Epoch 118/500\n",
            "117/117 [==============================] - 0s 471us/step - loss: 4.0344 - acc: 0.5726\n",
            "Epoch 119/500\n",
            "117/117 [==============================] - 0s 437us/step - loss: 4.0239 - acc: 0.5726\n",
            "Epoch 120/500\n",
            "117/117 [==============================] - 0s 363us/step - loss: 4.0153 - acc: 0.5726\n",
            "Epoch 121/500\n",
            "117/117 [==============================] - 0s 453us/step - loss: 4.0042 - acc: 0.5726\n",
            "Epoch 122/500\n",
            "117/117 [==============================] - 0s 419us/step - loss: 3.9944 - acc: 0.5726\n",
            "Epoch 123/500\n",
            "117/117 [==============================] - 0s 386us/step - loss: 3.9855 - acc: 0.5812\n",
            "Epoch 124/500\n",
            "117/117 [==============================] - 0s 423us/step - loss: 3.9764 - acc: 0.5812\n",
            "Epoch 125/500\n",
            "117/117 [==============================] - 0s 396us/step - loss: 3.9668 - acc: 0.5812\n",
            "Epoch 126/500\n",
            "117/117 [==============================] - 0s 394us/step - loss: 3.9588 - acc: 0.5897\n",
            "Epoch 127/500\n",
            "117/117 [==============================] - 0s 510us/step - loss: 3.9497 - acc: 0.5812\n",
            "Epoch 128/500\n",
            "117/117 [==============================] - 0s 387us/step - loss: 3.9416 - acc: 0.5897\n",
            "Epoch 129/500\n",
            "117/117 [==============================] - 0s 410us/step - loss: 3.9329 - acc: 0.5812\n",
            "Epoch 130/500\n",
            "117/117 [==============================] - 0s 437us/step - loss: 3.9231 - acc: 0.5897\n",
            "Epoch 131/500\n",
            "117/117 [==============================] - 0s 415us/step - loss: 3.9142 - acc: 0.5726\n",
            "Epoch 132/500\n",
            "117/117 [==============================] - 0s 416us/step - loss: 3.9076 - acc: 0.5812\n",
            "Epoch 133/500\n",
            "117/117 [==============================] - 0s 372us/step - loss: 3.8987 - acc: 0.5897\n",
            "Epoch 134/500\n",
            "117/117 [==============================] - 0s 364us/step - loss: 3.8908 - acc: 0.5812\n",
            "Epoch 135/500\n",
            "117/117 [==============================] - 0s 428us/step - loss: 3.8821 - acc: 0.5897\n",
            "Epoch 136/500\n",
            "117/117 [==============================] - 0s 362us/step - loss: 3.8737 - acc: 0.5812\n",
            "Epoch 137/500\n",
            "117/117 [==============================] - 0s 340us/step - loss: 3.8659 - acc: 0.5812\n",
            "Epoch 138/500\n",
            "117/117 [==============================] - 0s 345us/step - loss: 3.8601 - acc: 0.5897\n",
            "Epoch 139/500\n",
            "117/117 [==============================] - 0s 340us/step - loss: 3.8498 - acc: 0.5897\n",
            "Epoch 140/500\n",
            "117/117 [==============================] - 0s 360us/step - loss: 3.8435 - acc: 0.6154\n",
            "Epoch 141/500\n",
            "117/117 [==============================] - 0s 351us/step - loss: 3.8348 - acc: 0.5983\n",
            "Epoch 142/500\n",
            "117/117 [==============================] - 0s 316us/step - loss: 3.8267 - acc: 0.6068\n",
            "Epoch 143/500\n",
            "117/117 [==============================] - 0s 359us/step - loss: 3.8184 - acc: 0.6154\n",
            "Epoch 144/500\n",
            "117/117 [==============================] - 0s 350us/step - loss: 3.8121 - acc: 0.6239\n",
            "Epoch 145/500\n",
            "117/117 [==============================] - 0s 392us/step - loss: 3.8033 - acc: 0.6239\n",
            "Epoch 146/500\n",
            "117/117 [==============================] - 0s 415us/step - loss: 3.7964 - acc: 0.6239\n",
            "Epoch 147/500\n",
            "117/117 [==============================] - 0s 422us/step - loss: 3.7892 - acc: 0.6239\n",
            "Epoch 148/500\n",
            "117/117 [==============================] - 0s 392us/step - loss: 3.7836 - acc: 0.6239\n",
            "Epoch 149/500\n",
            "117/117 [==============================] - 0s 412us/step - loss: 3.7766 - acc: 0.6325\n",
            "Epoch 150/500\n",
            "117/117 [==============================] - 0s 354us/step - loss: 3.7684 - acc: 0.6239\n",
            "Epoch 151/500\n",
            "117/117 [==============================] - 0s 372us/step - loss: 3.7594 - acc: 0.6239\n",
            "Epoch 152/500\n",
            "117/117 [==============================] - 0s 398us/step - loss: 3.7559 - acc: 0.6239\n",
            "Epoch 153/500\n",
            "117/117 [==============================] - 0s 369us/step - loss: 3.7476 - acc: 0.6239\n",
            "Epoch 154/500\n",
            "117/117 [==============================] - 0s 372us/step - loss: 3.7413 - acc: 0.6239\n",
            "Epoch 155/500\n",
            "117/117 [==============================] - 0s 378us/step - loss: 3.7328 - acc: 0.6410\n",
            "Epoch 156/500\n",
            "117/117 [==============================] - 0s 402us/step - loss: 3.7281 - acc: 0.6325\n",
            "Epoch 157/500\n",
            "117/117 [==============================] - 0s 422us/step - loss: 3.7200 - acc: 0.6325\n",
            "Epoch 158/500\n",
            "117/117 [==============================] - 0s 400us/step - loss: 3.7139 - acc: 0.6496\n",
            "Epoch 159/500\n",
            "117/117 [==============================] - 0s 376us/step - loss: 3.7085 - acc: 0.6496\n",
            "Epoch 160/500\n",
            "117/117 [==============================] - 0s 378us/step - loss: 3.6997 - acc: 0.6410\n",
            "Epoch 161/500\n",
            "117/117 [==============================] - 0s 371us/step - loss: 3.6949 - acc: 0.6496\n",
            "Epoch 162/500\n",
            "117/117 [==============================] - 0s 365us/step - loss: 3.6872 - acc: 0.6496\n",
            "Epoch 163/500\n",
            "117/117 [==============================] - 0s 346us/step - loss: 3.6803 - acc: 0.6410\n",
            "Epoch 164/500\n",
            "117/117 [==============================] - 0s 410us/step - loss: 3.6762 - acc: 0.6581\n",
            "Epoch 165/500\n",
            "117/117 [==============================] - 0s 352us/step - loss: 3.6693 - acc: 0.6496\n",
            "Epoch 166/500\n",
            "117/117 [==============================] - 0s 350us/step - loss: 3.6619 - acc: 0.6581\n",
            "Epoch 167/500\n",
            "117/117 [==============================] - 0s 367us/step - loss: 3.6557 - acc: 0.6496\n",
            "Epoch 168/500\n",
            "117/117 [==============================] - 0s 333us/step - loss: 3.6512 - acc: 0.6496\n",
            "Epoch 169/500\n",
            "117/117 [==============================] - 0s 323us/step - loss: 3.6435 - acc: 0.6581\n",
            "Epoch 170/500\n",
            "117/117 [==============================] - 0s 339us/step - loss: 3.6384 - acc: 0.6667\n",
            "Epoch 171/500\n",
            "117/117 [==============================] - 0s 352us/step - loss: 3.6297 - acc: 0.6581\n",
            "Epoch 172/500\n",
            "117/117 [==============================] - 0s 404us/step - loss: 3.6254 - acc: 0.6667\n",
            "Epoch 173/500\n",
            "117/117 [==============================] - 0s 376us/step - loss: 3.6196 - acc: 0.6496\n",
            "Epoch 174/500\n",
            "117/117 [==============================] - 0s 411us/step - loss: 3.6127 - acc: 0.6496\n",
            "Epoch 175/500\n",
            "117/117 [==============================] - 0s 394us/step - loss: 3.6072 - acc: 0.6581\n",
            "Epoch 176/500\n",
            "117/117 [==============================] - 0s 400us/step - loss: 3.6008 - acc: 0.6581\n",
            "Epoch 177/500\n",
            "117/117 [==============================] - 0s 381us/step - loss: 3.5947 - acc: 0.6752\n",
            "Epoch 178/500\n",
            "117/117 [==============================] - 0s 391us/step - loss: 3.5921 - acc: 0.6667\n",
            "Epoch 179/500\n",
            "117/117 [==============================] - 0s 406us/step - loss: 3.5841 - acc: 0.6581\n",
            "Epoch 180/500\n",
            "117/117 [==============================] - 0s 386us/step - loss: 3.5800 - acc: 0.6581\n",
            "Epoch 181/500\n",
            "117/117 [==============================] - 0s 382us/step - loss: 3.5715 - acc: 0.6838\n",
            "Epoch 182/500\n",
            "117/117 [==============================] - 0s 444us/step - loss: 3.5667 - acc: 0.6667\n",
            "Epoch 183/500\n",
            "117/117 [==============================] - 0s 459us/step - loss: 3.5671 - acc: 0.6667\n",
            "Epoch 184/500\n",
            "117/117 [==============================] - 0s 467us/step - loss: 3.5573 - acc: 0.6667\n",
            "Epoch 185/500\n",
            "117/117 [==============================] - 0s 397us/step - loss: 3.5485 - acc: 0.6752\n",
            "Epoch 186/500\n",
            "117/117 [==============================] - 0s 355us/step - loss: 3.5429 - acc: 0.6752\n",
            "Epoch 187/500\n",
            "117/117 [==============================] - 0s 380us/step - loss: 3.5383 - acc: 0.6752\n",
            "Epoch 188/500\n",
            "117/117 [==============================] - 0s 372us/step - loss: 3.5316 - acc: 0.6838\n",
            "Epoch 189/500\n",
            "117/117 [==============================] - 0s 372us/step - loss: 3.5258 - acc: 0.6923\n",
            "Epoch 190/500\n",
            "117/117 [==============================] - 0s 340us/step - loss: 3.5192 - acc: 0.6667\n",
            "Epoch 191/500\n",
            "117/117 [==============================] - 0s 356us/step - loss: 3.5142 - acc: 0.6838\n",
            "Epoch 192/500\n",
            "117/117 [==============================] - 0s 361us/step - loss: 3.5083 - acc: 0.6838\n",
            "Epoch 193/500\n",
            "117/117 [==============================] - 0s 355us/step - loss: 3.5042 - acc: 0.6923\n",
            "Epoch 194/500\n",
            "117/117 [==============================] - 0s 322us/step - loss: 3.5004 - acc: 0.6923\n",
            "Epoch 195/500\n",
            "117/117 [==============================] - 0s 346us/step - loss: 3.4916 - acc: 0.6923\n",
            "Epoch 196/500\n",
            "117/117 [==============================] - 0s 353us/step - loss: 3.4879 - acc: 0.6838\n",
            "Epoch 197/500\n",
            "117/117 [==============================] - 0s 350us/step - loss: 3.4801 - acc: 0.6923\n",
            "Epoch 198/500\n",
            "117/117 [==============================] - 0s 333us/step - loss: 3.4747 - acc: 0.6923\n",
            "Epoch 199/500\n",
            "117/117 [==============================] - 0s 348us/step - loss: 3.4685 - acc: 0.6923\n",
            "Epoch 200/500\n",
            "117/117 [==============================] - 0s 338us/step - loss: 3.4643 - acc: 0.6923\n",
            "Epoch 201/500\n",
            "117/117 [==============================] - 0s 365us/step - loss: 3.4604 - acc: 0.6838\n",
            "Epoch 202/500\n",
            "117/117 [==============================] - 0s 334us/step - loss: 3.4539 - acc: 0.7009\n",
            "Epoch 203/500\n",
            "117/117 [==============================] - 0s 334us/step - loss: 3.4509 - acc: 0.6923\n",
            "Epoch 204/500\n",
            "117/117 [==============================] - 0s 322us/step - loss: 3.4442 - acc: 0.6923\n",
            "Epoch 205/500\n",
            "117/117 [==============================] - 0s 323us/step - loss: 3.4373 - acc: 0.6923\n",
            "Epoch 206/500\n",
            "117/117 [==============================] - 0s 345us/step - loss: 3.4386 - acc: 0.6838\n",
            "Epoch 207/500\n",
            "117/117 [==============================] - 0s 386us/step - loss: 3.4296 - acc: 0.6923\n",
            "Epoch 208/500\n",
            "117/117 [==============================] - 0s 328us/step - loss: 3.4242 - acc: 0.6923\n",
            "Epoch 209/500\n",
            "117/117 [==============================] - 0s 356us/step - loss: 3.4228 - acc: 0.7009\n",
            "Epoch 210/500\n",
            "117/117 [==============================] - 0s 366us/step - loss: 3.4116 - acc: 0.7009\n",
            "Epoch 211/500\n",
            "117/117 [==============================] - 0s 342us/step - loss: 3.4066 - acc: 0.6923\n",
            "Epoch 212/500\n",
            "117/117 [==============================] - 0s 372us/step - loss: 3.4020 - acc: 0.7009\n",
            "Epoch 213/500\n",
            "117/117 [==============================] - 0s 323us/step - loss: 3.3965 - acc: 0.7009\n",
            "Epoch 214/500\n",
            "117/117 [==============================] - 0s 352us/step - loss: 3.3953 - acc: 0.6923\n",
            "Epoch 215/500\n",
            "117/117 [==============================] - 0s 372us/step - loss: 3.3858 - acc: 0.7009\n",
            "Epoch 216/500\n",
            "117/117 [==============================] - 0s 338us/step - loss: 3.3810 - acc: 0.7009\n",
            "Epoch 217/500\n",
            "117/117 [==============================] - 0s 408us/step - loss: 3.3848 - acc: 0.6923\n",
            "Epoch 218/500\n",
            "117/117 [==============================] - 0s 518us/step - loss: 3.3767 - acc: 0.7179\n",
            "Epoch 219/500\n",
            "117/117 [==============================] - 0s 401us/step - loss: 3.3690 - acc: 0.7179\n",
            "Epoch 220/500\n",
            "117/117 [==============================] - 0s 349us/step - loss: 3.3668 - acc: 0.7179\n",
            "Epoch 221/500\n",
            "117/117 [==============================] - 0s 347us/step - loss: 3.3563 - acc: 0.7179\n",
            "Epoch 222/500\n",
            "117/117 [==============================] - 0s 358us/step - loss: 3.3534 - acc: 0.7094\n",
            "Epoch 223/500\n",
            "117/117 [==============================] - 0s 355us/step - loss: 3.3476 - acc: 0.7009\n",
            "Epoch 224/500\n",
            "117/117 [==============================] - 0s 330us/step - loss: 3.3393 - acc: 0.7009\n",
            "Epoch 225/500\n",
            "117/117 [==============================] - 0s 348us/step - loss: 3.3376 - acc: 0.7009\n",
            "Epoch 226/500\n",
            "117/117 [==============================] - 0s 336us/step - loss: 3.3312 - acc: 0.7179\n",
            "Epoch 227/500\n",
            "117/117 [==============================] - 0s 364us/step - loss: 3.3280 - acc: 0.7179\n",
            "Epoch 228/500\n",
            "117/117 [==============================] - 0s 356us/step - loss: 3.3221 - acc: 0.7094\n",
            "Epoch 229/500\n",
            "117/117 [==============================] - 0s 389us/step - loss: 3.3172 - acc: 0.7179\n",
            "Epoch 230/500\n",
            "117/117 [==============================] - 0s 328us/step - loss: 3.3121 - acc: 0.7350\n",
            "Epoch 231/500\n",
            "117/117 [==============================] - 0s 342us/step - loss: 3.3083 - acc: 0.7265\n",
            "Epoch 232/500\n",
            "117/117 [==============================] - 0s 385us/step - loss: 3.3068 - acc: 0.7094\n",
            "Epoch 233/500\n",
            "117/117 [==============================] - 0s 356us/step - loss: 3.3005 - acc: 0.7265\n",
            "Epoch 234/500\n",
            "117/117 [==============================] - 0s 342us/step - loss: 3.2929 - acc: 0.7179\n",
            "Epoch 235/500\n",
            "117/117 [==============================] - 0s 378us/step - loss: 3.2891 - acc: 0.7179\n",
            "Epoch 236/500\n",
            "117/117 [==============================] - 0s 427us/step - loss: 3.2839 - acc: 0.7350\n",
            "Epoch 237/500\n",
            "117/117 [==============================] - 0s 380us/step - loss: 3.2790 - acc: 0.7179\n",
            "Epoch 238/500\n",
            "117/117 [==============================] - 0s 362us/step - loss: 3.2766 - acc: 0.7094\n",
            "Epoch 239/500\n",
            "117/117 [==============================] - 0s 352us/step - loss: 3.2720 - acc: 0.7179\n",
            "Epoch 240/500\n",
            "117/117 [==============================] - 0s 377us/step - loss: 3.2642 - acc: 0.7179\n",
            "Epoch 241/500\n",
            "117/117 [==============================] - 0s 323us/step - loss: 3.2643 - acc: 0.7265\n",
            "Epoch 242/500\n",
            "117/117 [==============================] - 0s 360us/step - loss: 3.2545 - acc: 0.7179\n",
            "Epoch 243/500\n",
            "117/117 [==============================] - 0s 385us/step - loss: 3.2523 - acc: 0.7179\n",
            "Epoch 244/500\n",
            "117/117 [==============================] - 0s 355us/step - loss: 3.2449 - acc: 0.7179\n",
            "Epoch 245/500\n",
            "117/117 [==============================] - 0s 364us/step - loss: 3.2412 - acc: 0.7350\n",
            "Epoch 246/500\n",
            "117/117 [==============================] - 0s 372us/step - loss: 3.2363 - acc: 0.7179\n",
            "Epoch 247/500\n",
            "117/117 [==============================] - 0s 369us/step - loss: 3.2318 - acc: 0.7350\n",
            "Epoch 248/500\n",
            "117/117 [==============================] - 0s 365us/step - loss: 3.2263 - acc: 0.7179\n",
            "Epoch 249/500\n",
            "117/117 [==============================] - 0s 357us/step - loss: 3.2298 - acc: 0.7179\n",
            "Epoch 250/500\n",
            "117/117 [==============================] - 0s 406us/step - loss: 3.2167 - acc: 0.7265\n",
            "Epoch 251/500\n",
            "117/117 [==============================] - 0s 409us/step - loss: 3.2128 - acc: 0.7179\n",
            "Epoch 252/500\n",
            "117/117 [==============================] - 0s 353us/step - loss: 3.2097 - acc: 0.7179\n",
            "Epoch 253/500\n",
            "117/117 [==============================] - 0s 379us/step - loss: 3.2025 - acc: 0.7350\n",
            "Epoch 254/500\n",
            "117/117 [==============================] - 0s 363us/step - loss: 3.1995 - acc: 0.7179\n",
            "Epoch 255/500\n",
            "117/117 [==============================] - 0s 323us/step - loss: 3.1945 - acc: 0.7179\n",
            "Epoch 256/500\n",
            "117/117 [==============================] - 0s 366us/step - loss: 3.1911 - acc: 0.7350\n",
            "Epoch 257/500\n",
            "117/117 [==============================] - 0s 324us/step - loss: 3.1872 - acc: 0.7094\n",
            "Epoch 258/500\n",
            "117/117 [==============================] - 0s 359us/step - loss: 3.1812 - acc: 0.7094\n",
            "Epoch 259/500\n",
            "117/117 [==============================] - 0s 439us/step - loss: 3.1771 - acc: 0.7179\n",
            "Epoch 260/500\n",
            "117/117 [==============================] - 0s 350us/step - loss: 3.1757 - acc: 0.7094\n",
            "Epoch 261/500\n",
            "117/117 [==============================] - 0s 348us/step - loss: 3.1671 - acc: 0.7350\n",
            "Epoch 262/500\n",
            "117/117 [==============================] - 0s 338us/step - loss: 3.1640 - acc: 0.7265\n",
            "Epoch 263/500\n",
            "117/117 [==============================] - 0s 375us/step - loss: 3.1562 - acc: 0.7179\n",
            "Epoch 264/500\n",
            "117/117 [==============================] - 0s 407us/step - loss: 3.1556 - acc: 0.7265\n",
            "Epoch 265/500\n",
            "117/117 [==============================] - 0s 386us/step - loss: 3.1517 - acc: 0.7350\n",
            "Epoch 266/500\n",
            "117/117 [==============================] - 0s 335us/step - loss: 3.1513 - acc: 0.7179\n",
            "Epoch 267/500\n",
            "117/117 [==============================] - 0s 331us/step - loss: 3.1437 - acc: 0.7265\n",
            "Epoch 268/500\n",
            "117/117 [==============================] - 0s 363us/step - loss: 3.1374 - acc: 0.7265\n",
            "Epoch 269/500\n",
            "117/117 [==============================] - 0s 363us/step - loss: 3.1312 - acc: 0.7350\n",
            "Epoch 270/500\n",
            "117/117 [==============================] - 0s 385us/step - loss: 3.1310 - acc: 0.7350\n",
            "Epoch 271/500\n",
            "117/117 [==============================] - 0s 396us/step - loss: 3.1269 - acc: 0.7094\n",
            "Epoch 272/500\n",
            "117/117 [==============================] - 0s 404us/step - loss: 3.1209 - acc: 0.7265\n",
            "Epoch 273/500\n",
            "117/117 [==============================] - 0s 360us/step - loss: 3.1159 - acc: 0.7179\n",
            "Epoch 274/500\n",
            "117/117 [==============================] - 0s 436us/step - loss: 3.1091 - acc: 0.7350\n",
            "Epoch 275/500\n",
            "117/117 [==============================] - 0s 378us/step - loss: 3.1062 - acc: 0.7265\n",
            "Epoch 276/500\n",
            "117/117 [==============================] - 0s 395us/step - loss: 3.1082 - acc: 0.7350\n",
            "Epoch 277/500\n",
            "117/117 [==============================] - 0s 411us/step - loss: 3.0984 - acc: 0.7265\n",
            "Epoch 278/500\n",
            "117/117 [==============================] - 0s 342us/step - loss: 3.0969 - acc: 0.7350\n",
            "Epoch 279/500\n",
            "117/117 [==============================] - 0s 334us/step - loss: 3.0885 - acc: 0.7265\n",
            "Epoch 280/500\n",
            "117/117 [==============================] - 0s 369us/step - loss: 3.0854 - acc: 0.7350\n",
            "Epoch 281/500\n",
            "117/117 [==============================] - 0s 345us/step - loss: 3.0825 - acc: 0.7521\n",
            "Epoch 282/500\n",
            "117/117 [==============================] - 0s 349us/step - loss: 3.0733 - acc: 0.7350\n",
            "Epoch 283/500\n",
            "117/117 [==============================] - 0s 334us/step - loss: 3.0743 - acc: 0.7436\n",
            "Epoch 284/500\n",
            "117/117 [==============================] - 0s 316us/step - loss: 3.0659 - acc: 0.7521\n",
            "Epoch 285/500\n",
            "117/117 [==============================] - 0s 313us/step - loss: 3.0685 - acc: 0.7350\n",
            "Epoch 286/500\n",
            "117/117 [==============================] - 0s 359us/step - loss: 3.0584 - acc: 0.7265\n",
            "Epoch 287/500\n",
            "117/117 [==============================] - 0s 350us/step - loss: 3.0557 - acc: 0.7521\n",
            "Epoch 288/500\n",
            "117/117 [==============================] - 0s 299us/step - loss: 3.0527 - acc: 0.7436\n",
            "Epoch 289/500\n",
            "117/117 [==============================] - 0s 347us/step - loss: 3.0444 - acc: 0.7350\n",
            "Epoch 290/500\n",
            "117/117 [==============================] - 0s 364us/step - loss: 3.0450 - acc: 0.7607\n",
            "Epoch 291/500\n",
            "117/117 [==============================] - 0s 362us/step - loss: 3.0488 - acc: 0.7436\n",
            "Epoch 292/500\n",
            "117/117 [==============================] - 0s 369us/step - loss: 3.0430 - acc: 0.7350\n",
            "Epoch 293/500\n",
            "117/117 [==============================] - 0s 325us/step - loss: 3.0309 - acc: 0.7436\n",
            "Epoch 294/500\n",
            "117/117 [==============================] - 0s 350us/step - loss: 3.0283 - acc: 0.7521\n",
            "Epoch 295/500\n",
            "117/117 [==============================] - 0s 343us/step - loss: 3.0276 - acc: 0.7350\n",
            "Epoch 296/500\n",
            "117/117 [==============================] - 0s 351us/step - loss: 3.0183 - acc: 0.7436\n",
            "Epoch 297/500\n",
            "117/117 [==============================] - 0s 372us/step - loss: 3.0111 - acc: 0.7521\n",
            "Epoch 298/500\n",
            "117/117 [==============================] - 0s 331us/step - loss: 3.0125 - acc: 0.7521\n",
            "Epoch 299/500\n",
            "117/117 [==============================] - 0s 354us/step - loss: 3.0063 - acc: 0.7607\n",
            "Epoch 300/500\n",
            "117/117 [==============================] - 0s 369us/step - loss: 3.0046 - acc: 0.7265\n",
            "Epoch 301/500\n",
            "117/117 [==============================] - 0s 358us/step - loss: 2.9964 - acc: 0.7436\n",
            "Epoch 302/500\n",
            "117/117 [==============================] - 0s 366us/step - loss: 2.9931 - acc: 0.7521\n",
            "Epoch 303/500\n",
            "117/117 [==============================] - 0s 396us/step - loss: 2.9867 - acc: 0.7521\n",
            "Epoch 304/500\n",
            "117/117 [==============================] - 0s 496us/step - loss: 2.9891 - acc: 0.7436\n",
            "Epoch 305/500\n",
            "117/117 [==============================] - 0s 378us/step - loss: 2.9782 - acc: 0.7607\n",
            "Epoch 306/500\n",
            "117/117 [==============================] - 0s 396us/step - loss: 2.9738 - acc: 0.7521\n",
            "Epoch 307/500\n",
            "117/117 [==============================] - 0s 354us/step - loss: 2.9705 - acc: 0.7607\n",
            "Epoch 308/500\n",
            "117/117 [==============================] - 0s 352us/step - loss: 2.9755 - acc: 0.7436\n",
            "Epoch 309/500\n",
            "117/117 [==============================] - 0s 417us/step - loss: 2.9643 - acc: 0.7436\n",
            "Epoch 310/500\n",
            "117/117 [==============================] - 0s 377us/step - loss: 2.9640 - acc: 0.7607\n",
            "Epoch 311/500\n",
            "117/117 [==============================] - 0s 403us/step - loss: 2.9564 - acc: 0.7521\n",
            "Epoch 312/500\n",
            "117/117 [==============================] - 0s 403us/step - loss: 2.9570 - acc: 0.7692\n",
            "Epoch 313/500\n",
            "117/117 [==============================] - 0s 423us/step - loss: 2.9473 - acc: 0.7692\n",
            "Epoch 314/500\n",
            "117/117 [==============================] - 0s 411us/step - loss: 2.9408 - acc: 0.7692\n",
            "Epoch 315/500\n",
            "117/117 [==============================] - 0s 374us/step - loss: 2.9383 - acc: 0.7692\n",
            "Epoch 316/500\n",
            "117/117 [==============================] - 0s 460us/step - loss: 2.9353 - acc: 0.7692\n",
            "Epoch 317/500\n",
            "117/117 [==============================] - 0s 382us/step - loss: 2.9294 - acc: 0.7692\n",
            "Epoch 318/500\n",
            "117/117 [==============================] - 0s 390us/step - loss: 2.9252 - acc: 0.7692\n",
            "Epoch 319/500\n",
            "117/117 [==============================] - 0s 411us/step - loss: 2.9268 - acc: 0.7436\n",
            "Epoch 320/500\n",
            "117/117 [==============================] - 0s 400us/step - loss: 2.9167 - acc: 0.7692\n",
            "Epoch 321/500\n",
            "117/117 [==============================] - 0s 406us/step - loss: 2.9172 - acc: 0.7692\n",
            "Epoch 322/500\n",
            "117/117 [==============================] - 0s 465us/step - loss: 2.9124 - acc: 0.7607\n",
            "Epoch 323/500\n",
            "117/117 [==============================] - 0s 424us/step - loss: 2.9127 - acc: 0.7607\n",
            "Epoch 324/500\n",
            "117/117 [==============================] - 0s 404us/step - loss: 2.9058 - acc: 0.7692\n",
            "Epoch 325/500\n",
            "117/117 [==============================] - 0s 405us/step - loss: 2.9014 - acc: 0.7521\n",
            "Epoch 326/500\n",
            "117/117 [==============================] - 0s 435us/step - loss: 2.8939 - acc: 0.7692\n",
            "Epoch 327/500\n",
            "117/117 [==============================] - 0s 379us/step - loss: 2.8891 - acc: 0.7607\n",
            "Epoch 328/500\n",
            "117/117 [==============================] - 0s 390us/step - loss: 2.8879 - acc: 0.7692\n",
            "Epoch 329/500\n",
            "117/117 [==============================] - 0s 403us/step - loss: 2.8869 - acc: 0.7436\n",
            "Epoch 330/500\n",
            "117/117 [==============================] - 0s 421us/step - loss: 2.8830 - acc: 0.7521\n",
            "Epoch 331/500\n",
            "117/117 [==============================] - 0s 418us/step - loss: 2.8757 - acc: 0.7607\n",
            "Epoch 332/500\n",
            "117/117 [==============================] - 0s 455us/step - loss: 2.8700 - acc: 0.7607\n",
            "Epoch 333/500\n",
            "117/117 [==============================] - 0s 433us/step - loss: 2.8868 - acc: 0.7436\n",
            "Epoch 334/500\n",
            "117/117 [==============================] - 0s 449us/step - loss: 2.8655 - acc: 0.7607\n",
            "Epoch 335/500\n",
            "117/117 [==============================] - 0s 437us/step - loss: 2.8599 - acc: 0.7607\n",
            "Epoch 336/500\n",
            "117/117 [==============================] - 0s 403us/step - loss: 2.8634 - acc: 0.7521\n",
            "Epoch 337/500\n",
            "117/117 [==============================] - 0s 403us/step - loss: 2.8522 - acc: 0.7607\n",
            "Epoch 338/500\n",
            "117/117 [==============================] - 0s 417us/step - loss: 2.8479 - acc: 0.7692\n",
            "Epoch 339/500\n",
            "117/117 [==============================] - 0s 422us/step - loss: 2.8432 - acc: 0.7607\n",
            "Epoch 340/500\n",
            "117/117 [==============================] - 0s 425us/step - loss: 2.8396 - acc: 0.7607\n",
            "Epoch 341/500\n",
            "117/117 [==============================] - 0s 433us/step - loss: 2.8389 - acc: 0.7607\n",
            "Epoch 342/500\n",
            "117/117 [==============================] - 0s 394us/step - loss: 2.8427 - acc: 0.7350\n",
            "Epoch 343/500\n",
            "117/117 [==============================] - 0s 358us/step - loss: 2.8313 - acc: 0.7692\n",
            "Epoch 344/500\n",
            "117/117 [==============================] - 0s 412us/step - loss: 2.8283 - acc: 0.7607\n",
            "Epoch 345/500\n",
            "117/117 [==============================] - 0s 397us/step - loss: 2.8239 - acc: 0.7692\n",
            "Epoch 346/500\n",
            "117/117 [==============================] - 0s 411us/step - loss: 2.8173 - acc: 0.7778\n",
            "Epoch 347/500\n",
            "117/117 [==============================] - 0s 384us/step - loss: 2.8191 - acc: 0.7607\n",
            "Epoch 348/500\n",
            "117/117 [==============================] - 0s 371us/step - loss: 2.8116 - acc: 0.7692\n",
            "Epoch 349/500\n",
            "117/117 [==============================] - 0s 402us/step - loss: 2.8035 - acc: 0.7607\n",
            "Epoch 350/500\n",
            "117/117 [==============================] - 0s 349us/step - loss: 2.8091 - acc: 0.7607\n",
            "Epoch 351/500\n",
            "117/117 [==============================] - 0s 361us/step - loss: 2.8022 - acc: 0.7692\n",
            "Epoch 352/500\n",
            "117/117 [==============================] - 0s 407us/step - loss: 2.7939 - acc: 0.7692\n",
            "Epoch 353/500\n",
            "117/117 [==============================] - 0s 374us/step - loss: 2.7967 - acc: 0.7607\n",
            "Epoch 354/500\n",
            "117/117 [==============================] - 0s 395us/step - loss: 2.7921 - acc: 0.7863\n",
            "Epoch 355/500\n",
            "117/117 [==============================] - 0s 368us/step - loss: 2.7931 - acc: 0.7778\n",
            "Epoch 356/500\n",
            "117/117 [==============================] - 0s 369us/step - loss: 2.7895 - acc: 0.7521\n",
            "Epoch 357/500\n",
            "117/117 [==============================] - 0s 343us/step - loss: 2.7786 - acc: 0.7692\n",
            "Epoch 358/500\n",
            "117/117 [==============================] - 0s 347us/step - loss: 2.7703 - acc: 0.7863\n",
            "Epoch 359/500\n",
            "117/117 [==============================] - 0s 390us/step - loss: 2.7682 - acc: 0.7863\n",
            "Epoch 360/500\n",
            "117/117 [==============================] - 0s 355us/step - loss: 2.7665 - acc: 0.7607\n",
            "Epoch 361/500\n",
            "117/117 [==============================] - 0s 378us/step - loss: 2.7728 - acc: 0.7350\n",
            "Epoch 362/500\n",
            "117/117 [==============================] - 0s 380us/step - loss: 2.7643 - acc: 0.7778\n",
            "Epoch 363/500\n",
            "117/117 [==============================] - 0s 397us/step - loss: 2.7522 - acc: 0.7692\n",
            "Epoch 364/500\n",
            "117/117 [==============================] - 0s 448us/step - loss: 2.7520 - acc: 0.7778\n",
            "Epoch 365/500\n",
            "117/117 [==============================] - 0s 428us/step - loss: 2.7463 - acc: 0.7863\n",
            "Epoch 366/500\n",
            "117/117 [==============================] - 0s 417us/step - loss: 2.7411 - acc: 0.7863\n",
            "Epoch 367/500\n",
            "117/117 [==============================] - 0s 485us/step - loss: 2.7453 - acc: 0.7692\n",
            "Epoch 368/500\n",
            "117/117 [==============================] - 0s 412us/step - loss: 2.7363 - acc: 0.7778\n",
            "Epoch 369/500\n",
            "117/117 [==============================] - 0s 372us/step - loss: 2.7362 - acc: 0.7607\n",
            "Epoch 370/500\n",
            "117/117 [==============================] - 0s 401us/step - loss: 2.7300 - acc: 0.7778\n",
            "Epoch 371/500\n",
            "117/117 [==============================] - 0s 387us/step - loss: 2.7246 - acc: 0.7778\n",
            "Epoch 372/500\n",
            "117/117 [==============================] - 0s 383us/step - loss: 2.7253 - acc: 0.7778\n",
            "Epoch 373/500\n",
            "117/117 [==============================] - 0s 390us/step - loss: 2.7206 - acc: 0.7607\n",
            "Epoch 374/500\n",
            "117/117 [==============================] - 0s 361us/step - loss: 2.7190 - acc: 0.7692\n",
            "Epoch 375/500\n",
            "117/117 [==============================] - 0s 432us/step - loss: 2.7115 - acc: 0.7778\n",
            "Epoch 376/500\n",
            "117/117 [==============================] - 0s 412us/step - loss: 2.7073 - acc: 0.7949\n",
            "Epoch 377/500\n",
            "117/117 [==============================] - 0s 363us/step - loss: 2.7039 - acc: 0.7607\n",
            "Epoch 378/500\n",
            "117/117 [==============================] - 0s 369us/step - loss: 2.6983 - acc: 0.7778\n",
            "Epoch 379/500\n",
            "117/117 [==============================] - 0s 470us/step - loss: 2.6986 - acc: 0.7863\n",
            "Epoch 380/500\n",
            "117/117 [==============================] - 0s 461us/step - loss: 2.6974 - acc: 0.7778\n",
            "Epoch 381/500\n",
            "117/117 [==============================] - 0s 442us/step - loss: 2.6884 - acc: 0.7692\n",
            "Epoch 382/500\n",
            "117/117 [==============================] - 0s 378us/step - loss: 2.6830 - acc: 0.7692\n",
            "Epoch 383/500\n",
            "117/117 [==============================] - 0s 421us/step - loss: 2.6847 - acc: 0.7778\n",
            "Epoch 384/500\n",
            "117/117 [==============================] - 0s 461us/step - loss: 2.6774 - acc: 0.7778\n",
            "Epoch 385/500\n",
            "117/117 [==============================] - 0s 514us/step - loss: 2.6796 - acc: 0.7863\n",
            "Epoch 386/500\n",
            "117/117 [==============================] - 0s 413us/step - loss: 2.6703 - acc: 0.7778\n",
            "Epoch 387/500\n",
            "117/117 [==============================] - 0s 403us/step - loss: 2.6688 - acc: 0.7692\n",
            "Epoch 388/500\n",
            "117/117 [==============================] - 0s 450us/step - loss: 2.6654 - acc: 0.7692\n",
            "Epoch 389/500\n",
            "117/117 [==============================] - 0s 363us/step - loss: 2.6643 - acc: 0.7778\n",
            "Epoch 390/500\n",
            "117/117 [==============================] - 0s 349us/step - loss: 2.6587 - acc: 0.7607\n",
            "Epoch 391/500\n",
            "117/117 [==============================] - 0s 379us/step - loss: 2.6554 - acc: 0.7778\n",
            "Epoch 392/500\n",
            "117/117 [==============================] - 0s 406us/step - loss: 2.6504 - acc: 0.7863\n",
            "Epoch 393/500\n",
            "117/117 [==============================] - 0s 372us/step - loss: 2.6507 - acc: 0.7692\n",
            "Epoch 394/500\n",
            "117/117 [==============================] - 0s 367us/step - loss: 2.6408 - acc: 0.7778\n",
            "Epoch 395/500\n",
            "117/117 [==============================] - 0s 402us/step - loss: 2.6393 - acc: 0.7949\n",
            "Epoch 396/500\n",
            "117/117 [==============================] - 0s 471us/step - loss: 2.6362 - acc: 0.7778\n",
            "Epoch 397/500\n",
            "117/117 [==============================] - 0s 396us/step - loss: 2.6360 - acc: 0.7778\n",
            "Epoch 398/500\n",
            "117/117 [==============================] - 0s 461us/step - loss: 2.6300 - acc: 0.7778\n",
            "Epoch 399/500\n",
            "117/117 [==============================] - 0s 484us/step - loss: 2.6262 - acc: 0.7778\n",
            "Epoch 400/500\n",
            "117/117 [==============================] - 0s 488us/step - loss: 2.6235 - acc: 0.7778\n",
            "Epoch 401/500\n",
            "117/117 [==============================] - 0s 450us/step - loss: 2.6168 - acc: 0.8034\n",
            "Epoch 402/500\n",
            "117/117 [==============================] - 0s 484us/step - loss: 2.6129 - acc: 0.7863\n",
            "Epoch 403/500\n",
            "117/117 [==============================] - 0s 468us/step - loss: 2.6100 - acc: 0.7863\n",
            "Epoch 404/500\n",
            "117/117 [==============================] - 0s 497us/step - loss: 2.6056 - acc: 0.7778\n",
            "Epoch 405/500\n",
            "117/117 [==============================] - 0s 452us/step - loss: 2.6030 - acc: 0.7778\n",
            "Epoch 406/500\n",
            "117/117 [==============================] - 0s 440us/step - loss: 2.5977 - acc: 0.7863\n",
            "Epoch 407/500\n",
            "117/117 [==============================] - 0s 424us/step - loss: 2.5936 - acc: 0.7863\n",
            "Epoch 408/500\n",
            "117/117 [==============================] - 0s 452us/step - loss: 2.5954 - acc: 0.7863\n",
            "Epoch 409/500\n",
            "117/117 [==============================] - 0s 479us/step - loss: 2.5950 - acc: 0.7778\n",
            "Epoch 410/500\n",
            "117/117 [==============================] - 0s 458us/step - loss: 2.5908 - acc: 0.7863\n",
            "Epoch 411/500\n",
            "117/117 [==============================] - 0s 479us/step - loss: 2.5834 - acc: 0.7692\n",
            "Epoch 412/500\n",
            "117/117 [==============================] - 0s 449us/step - loss: 2.5785 - acc: 0.8120\n",
            "Epoch 413/500\n",
            "117/117 [==============================] - 0s 445us/step - loss: 2.5792 - acc: 0.7778\n",
            "Epoch 414/500\n",
            "117/117 [==============================] - 0s 408us/step - loss: 2.5730 - acc: 0.7949\n",
            "Epoch 415/500\n",
            "117/117 [==============================] - 0s 387us/step - loss: 2.5731 - acc: 0.7863\n",
            "Epoch 416/500\n",
            "117/117 [==============================] - 0s 411us/step - loss: 2.5728 - acc: 0.7863\n",
            "Epoch 417/500\n",
            "117/117 [==============================] - 0s 382us/step - loss: 2.5621 - acc: 0.7949\n",
            "Epoch 418/500\n",
            "117/117 [==============================] - 0s 421us/step - loss: 2.5564 - acc: 0.7949\n",
            "Epoch 419/500\n",
            "117/117 [==============================] - 0s 449us/step - loss: 2.5601 - acc: 0.7692\n",
            "Epoch 420/500\n",
            "117/117 [==============================] - 0s 401us/step - loss: 2.5527 - acc: 0.7863\n",
            "Epoch 421/500\n",
            "117/117 [==============================] - 0s 389us/step - loss: 2.5532 - acc: 0.7949\n",
            "Epoch 422/500\n",
            "117/117 [==============================] - 0s 317us/step - loss: 2.5444 - acc: 0.7949\n",
            "Epoch 423/500\n",
            "117/117 [==============================] - 0s 372us/step - loss: 2.5470 - acc: 0.7949\n",
            "Epoch 424/500\n",
            "117/117 [==============================] - 0s 353us/step - loss: 2.5380 - acc: 0.7949\n",
            "Epoch 425/500\n",
            "117/117 [==============================] - 0s 306us/step - loss: 2.5411 - acc: 0.7863\n",
            "Epoch 426/500\n",
            "117/117 [==============================] - 0s 262us/step - loss: 2.5364 - acc: 0.8120\n",
            "Epoch 427/500\n",
            "117/117 [==============================] - 0s 291us/step - loss: 2.5333 - acc: 0.7949\n",
            "Epoch 428/500\n",
            "117/117 [==============================] - 0s 350us/step - loss: 2.5271 - acc: 0.7778\n",
            "Epoch 429/500\n",
            "117/117 [==============================] - 0s 280us/step - loss: 2.5324 - acc: 0.7778\n",
            "Epoch 430/500\n",
            "117/117 [==============================] - 0s 279us/step - loss: 2.5203 - acc: 0.7949\n",
            "Epoch 431/500\n",
            "117/117 [==============================] - 0s 425us/step - loss: 2.5137 - acc: 0.8120\n",
            "Epoch 432/500\n",
            "117/117 [==============================] - 0s 394us/step - loss: 2.5124 - acc: 0.8120\n",
            "Epoch 433/500\n",
            "117/117 [==============================] - 0s 349us/step - loss: 2.5048 - acc: 0.8205\n",
            "Epoch 434/500\n",
            "117/117 [==============================] - 0s 379us/step - loss: 2.5108 - acc: 0.8034\n",
            "Epoch 435/500\n",
            "117/117 [==============================] - 0s 339us/step - loss: 2.5054 - acc: 0.8034\n",
            "Epoch 436/500\n",
            "117/117 [==============================] - 0s 289us/step - loss: 2.4983 - acc: 0.8120\n",
            "Epoch 437/500\n",
            "117/117 [==============================] - 0s 326us/step - loss: 2.4949 - acc: 0.8034\n",
            "Epoch 438/500\n",
            "117/117 [==============================] - 0s 360us/step - loss: 2.4911 - acc: 0.8034\n",
            "Epoch 439/500\n",
            "117/117 [==============================] - 0s 327us/step - loss: 2.4957 - acc: 0.7949\n",
            "Epoch 440/500\n",
            "117/117 [==============================] - 0s 372us/step - loss: 2.4878 - acc: 0.8120\n",
            "Epoch 441/500\n",
            "117/117 [==============================] - 0s 371us/step - loss: 2.4882 - acc: 0.7863\n",
            "Epoch 442/500\n",
            "117/117 [==============================] - 0s 359us/step - loss: 2.4769 - acc: 0.8205\n",
            "Epoch 443/500\n",
            "117/117 [==============================] - 0s 390us/step - loss: 2.4765 - acc: 0.7949\n",
            "Epoch 444/500\n",
            "117/117 [==============================] - 0s 391us/step - loss: 2.4734 - acc: 0.8120\n",
            "Epoch 445/500\n",
            "117/117 [==============================] - 0s 366us/step - loss: 2.4718 - acc: 0.8120\n",
            "Epoch 446/500\n",
            "117/117 [==============================] - 0s 408us/step - loss: 2.4636 - acc: 0.8205\n",
            "Epoch 447/500\n",
            "117/117 [==============================] - 0s 425us/step - loss: 2.4621 - acc: 0.8205\n",
            "Epoch 448/500\n",
            "117/117 [==============================] - 0s 418us/step - loss: 2.4590 - acc: 0.8120\n",
            "Epoch 449/500\n",
            "117/117 [==============================] - 0s 399us/step - loss: 2.4595 - acc: 0.8120\n",
            "Epoch 450/500\n",
            "117/117 [==============================] - 0s 472us/step - loss: 2.4546 - acc: 0.8120\n",
            "Epoch 451/500\n",
            "117/117 [==============================] - 0s 415us/step - loss: 2.4512 - acc: 0.8205\n",
            "Epoch 452/500\n",
            "117/117 [==============================] - 0s 416us/step - loss: 2.4573 - acc: 0.7863\n",
            "Epoch 453/500\n",
            "117/117 [==============================] - 0s 387us/step - loss: 2.4477 - acc: 0.8205\n",
            "Epoch 454/500\n",
            "117/117 [==============================] - 0s 423us/step - loss: 2.4423 - acc: 0.8120\n",
            "Epoch 455/500\n",
            "117/117 [==============================] - 0s 417us/step - loss: 2.4397 - acc: 0.8120\n",
            "Epoch 456/500\n",
            "117/117 [==============================] - 0s 431us/step - loss: 2.4319 - acc: 0.8376\n",
            "Epoch 457/500\n",
            "117/117 [==============================] - 0s 387us/step - loss: 2.4342 - acc: 0.8205\n",
            "Epoch 458/500\n",
            "117/117 [==============================] - 0s 356us/step - loss: 2.4318 - acc: 0.7949\n",
            "Epoch 459/500\n",
            "117/117 [==============================] - 0s 392us/step - loss: 2.4236 - acc: 0.8376\n",
            "Epoch 460/500\n",
            "117/117 [==============================] - 0s 415us/step - loss: 2.4255 - acc: 0.8034\n",
            "Epoch 461/500\n",
            "117/117 [==============================] - 0s 408us/step - loss: 2.4165 - acc: 0.8205\n",
            "Epoch 462/500\n",
            "117/117 [==============================] - 0s 407us/step - loss: 2.4135 - acc: 0.8376\n",
            "Epoch 463/500\n",
            "117/117 [==============================] - 0s 440us/step - loss: 2.4185 - acc: 0.8291\n",
            "Epoch 464/500\n",
            "117/117 [==============================] - 0s 377us/step - loss: 2.4123 - acc: 0.8205\n",
            "Epoch 465/500\n",
            "117/117 [==============================] - 0s 377us/step - loss: 2.4072 - acc: 0.8120\n",
            "Epoch 466/500\n",
            "117/117 [==============================] - 0s 391us/step - loss: 2.4072 - acc: 0.8034\n",
            "Epoch 467/500\n",
            "117/117 [==============================] - 0s 385us/step - loss: 2.4082 - acc: 0.8291\n",
            "Epoch 468/500\n",
            "117/117 [==============================] - 0s 360us/step - loss: 2.3961 - acc: 0.8291\n",
            "Epoch 469/500\n",
            "117/117 [==============================] - 0s 387us/step - loss: 2.3956 - acc: 0.8120\n",
            "Epoch 470/500\n",
            "117/117 [==============================] - 0s 409us/step - loss: 2.3870 - acc: 0.8376\n",
            "Epoch 471/500\n",
            "117/117 [==============================] - 0s 394us/step - loss: 2.3922 - acc: 0.8291\n",
            "Epoch 472/500\n",
            "117/117 [==============================] - 0s 398us/step - loss: 2.3877 - acc: 0.8205\n",
            "Epoch 473/500\n",
            "117/117 [==============================] - 0s 400us/step - loss: 2.3797 - acc: 0.8205\n",
            "Epoch 474/500\n",
            "117/117 [==============================] - 0s 411us/step - loss: 2.3766 - acc: 0.8376\n",
            "Epoch 475/500\n",
            "117/117 [==============================] - 0s 425us/step - loss: 2.3760 - acc: 0.8120\n",
            "Epoch 476/500\n",
            "117/117 [==============================] - 0s 377us/step - loss: 2.3789 - acc: 0.8291\n",
            "Epoch 477/500\n",
            "117/117 [==============================] - 0s 406us/step - loss: 2.3751 - acc: 0.8120\n",
            "Epoch 478/500\n",
            "117/117 [==============================] - 0s 432us/step - loss: 2.3655 - acc: 0.8205\n",
            "Epoch 479/500\n",
            "117/117 [==============================] - 0s 480us/step - loss: 2.3649 - acc: 0.8205\n",
            "Epoch 480/500\n",
            "117/117 [==============================] - 0s 379us/step - loss: 2.3601 - acc: 0.8120\n",
            "Epoch 481/500\n",
            "117/117 [==============================] - 0s 407us/step - loss: 2.3553 - acc: 0.8205\n",
            "Epoch 482/500\n",
            "117/117 [==============================] - 0s 378us/step - loss: 2.3515 - acc: 0.8376\n",
            "Epoch 483/500\n",
            "117/117 [==============================] - 0s 468us/step - loss: 2.3509 - acc: 0.8376\n",
            "Epoch 484/500\n",
            "117/117 [==============================] - 0s 462us/step - loss: 2.3511 - acc: 0.8205\n",
            "Epoch 485/500\n",
            "117/117 [==============================] - 0s 373us/step - loss: 2.3516 - acc: 0.8205\n",
            "Epoch 486/500\n",
            "117/117 [==============================] - 0s 381us/step - loss: 2.3404 - acc: 0.8291\n",
            "Epoch 487/500\n",
            "117/117 [==============================] - 0s 452us/step - loss: 2.3361 - acc: 0.8291\n",
            "Epoch 488/500\n",
            "117/117 [==============================] - 0s 434us/step - loss: 2.3326 - acc: 0.8205\n",
            "Epoch 489/500\n",
            "117/117 [==============================] - 0s 375us/step - loss: 2.3319 - acc: 0.8205\n",
            "Epoch 490/500\n",
            "117/117 [==============================] - 0s 358us/step - loss: 2.3303 - acc: 0.8205\n",
            "Epoch 491/500\n",
            "117/117 [==============================] - 0s 381us/step - loss: 2.3281 - acc: 0.8120\n",
            "Epoch 492/500\n",
            "117/117 [==============================] - 0s 400us/step - loss: 2.3257 - acc: 0.8376\n",
            "Epoch 493/500\n",
            "117/117 [==============================] - 0s 390us/step - loss: 2.3191 - acc: 0.8291\n",
            "Epoch 494/500\n",
            "117/117 [==============================] - 0s 355us/step - loss: 2.3162 - acc: 0.8291\n",
            "Epoch 495/500\n",
            "117/117 [==============================] - 0s 414us/step - loss: 2.3161 - acc: 0.8376\n",
            "Epoch 496/500\n",
            "117/117 [==============================] - 0s 377us/step - loss: 2.3089 - acc: 0.8376\n",
            "Epoch 497/500\n",
            "117/117 [==============================] - 0s 402us/step - loss: 2.3265 - acc: 0.8120\n",
            "Epoch 498/500\n",
            "117/117 [==============================] - 0s 424us/step - loss: 2.3083 - acc: 0.8205\n",
            "Epoch 499/500\n",
            "117/117 [==============================] - 0s 399us/step - loss: 2.3058 - acc: 0.8205\n",
            "Epoch 500/500\n",
            "117/117 [==============================] - 0s 404us/step - loss: 2.3050 - acc: 0.8205\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe9b942aef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Zm3Au6w8WFa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "3ff3efc6-8c9a-4219-8bde-003666bd936c"
      },
      "source": [
        "# Predicting neural network with L2 penalization and extra hidden layers labels\n",
        "\n",
        "print(nn_pen_extra.predict_classes(prediction_input_preprocessor.transform(X_test)))\n",
        "\n",
        "prediction_index_nn_pen_extra=nn_pen_extra.predict_classes(prediction_input_preprocessor.transform(X_test))\n",
        "\n",
        "labels=processed_y_train.columns\n",
        "\n",
        "def index_to_label(labels,index_n): \n",
        "    return labels[index_n]\n",
        "    \n",
        "index_to_label(labels,1)\n",
        "\n",
        "nn_pen_extra_predicted_labels=list(map(lambda x: labels[x], prediction_index_nn_pen_extra))\n",
        "print(nn_pen_extra_predicted_labels)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 1 4 1 4 0 0 2 1 0 3 2 1 1 4 4 2 0 4 3 1 3 1 2 0 1 0 2 0 2 0 4 2 4 4\n",
            " 4 1]\n",
            "['Average', 'Average', 'Average', 'High', 'Very Low', 'High', 'Very Low', 'Average', 'Average', 'Low', 'High', 'Average', 'Very High', 'Low', 'High', 'High', 'Very Low', 'Very Low', 'Low', 'Average', 'Very Low', 'Very High', 'High', 'Very High', 'High', 'Low', 'Average', 'High', 'Average', 'Low', 'Average', 'Low', 'Average', 'Very Low', 'Low', 'Very Low', 'Very Low', 'Very Low', 'High']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgGwib6N8y3B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "outputId": "ffc7b2f1-f05e-4da2-f529-e72854c722f3"
      },
      "source": [
        "# Neural network with L2 penalization and extra hidden layers evaluation\n",
        "\n",
        "model_eval_metrics(y_test,nn_pen_extra_predicted_labels,classification=\"TRUE\")\n",
        "nn_pen_extra_model_eval = model_eval_metrics( y_test,nn_pen_extra_predicted_labels,classification=\"TRUE\")\n",
        "nn_pen_extra_model_eval"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>f1_score</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>mse</th>\n",
              "      <th>rmse</th>\n",
              "      <th>mae</th>\n",
              "      <th>r2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.461538</td>\n",
              "      <td>0.460995</td>\n",
              "      <td>0.528139</td>\n",
              "      <td>0.466667</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   accuracy  f1_score  precision    recall  mse  rmse  mae  r2\n",
              "0  0.461538  0.460995   0.528139  0.466667    0     0    0   0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqXRsWMLENbm",
        "colab_type": "text"
      },
      "source": [
        "##### Above I ran four neural network models. A vanilla neural network, a neural network with dropout regularization, a neural network for L2 penalization and a neural network with L2 penalization + extra hidden layers. The test accuracy scores for each model are below:\n",
        "\n",
        "##### 1. Vanilla network test score: .435897\n",
        "##### 2. Dropout regularization score: .512821\n",
        "##### 3. L2 penalization score: .435897 (different precision score than vanilla so I know a different model was evaluated)\n",
        "##### 4. L2 penalization and extra hidden layers score: .461538\n",
        "\n",
        "##### Of each neural network I ran, the model with dropout regularization returned the highest test accuracy score. I posted this score to the leaderboard. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzyk7-D0JILM",
        "colab_type": "text"
      },
      "source": [
        "### Posting to World Happiness AI Model Share Leaderboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aKGOLlLJILq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "183966c3-905c-47c1-fe12-9538dd3ca34e"
      },
      "source": [
        "pip install https://github.com/mikedparrott/aimodelshare/blob/master/aimodelshare-0.0.2.tar.gz?raw=true"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting https://github.com/mikedparrott/aimodelshare/blob/master/aimodelshare-0.0.2.tar.gz?raw=true\n",
            "  Downloading https://github.com/mikedparrott/aimodelshare/blob/master/aimodelshare-0.0.2.tar.gz?raw=true\n",
            "Building wheels for collected packages: aimodelshare\n",
            "  Building wheel for aimodelshare (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for aimodelshare: filename=aimodelshare-0.0.2-cp36-none-any.whl size=5375 sha256=81f366553bc4b8fe63427acabd08300d73685fdad747dd3771b07fae6437c7b5\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/8d/ac/09cb6ef7374ec79e02843c347195e5478144006b11def6799a\n",
            "Successfully built aimodelshare\n",
            "Installing collected packages: aimodelshare\n",
            "Successfully installed aimodelshare-0.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtm0_TqQJIL5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "88301667-3965-48bb-f8be-3acc440a6657"
      },
      "source": [
        "pip install onnxruntime"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.6/dist-packages (1.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bStGKRADHU1v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "outputId": "0072e0d4-b163-47af-ae01-8657f12f1cb1"
      },
      "source": [
        "! pip3 install keras2onnx"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras2onnx\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/df/38475abd5ef1e0c5a19f021add159e8a07d10525b2c01f13bf06371aedd4/keras2onnx-1.6.0-py3-none-any.whl (219kB)\n",
            "\r\u001b[K     |█▌                              | 10kB 17.6MB/s eta 0:00:01\r\u001b[K     |███                             | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |████▌                           | 30kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████                          | 40kB 2.0MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 51kB 2.3MB/s eta 0:00:01\r\u001b[K     |█████████                       | 61kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 71kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████                    | 81kB 3.1MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 92kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 102kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 112kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 122kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 133kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 143kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 153kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 163kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 174kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 184kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 194kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 204kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 215kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 225kB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from keras2onnx) (2.21.0)\n",
            "Collecting onnxconverter-common>=1.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/3d/6112c19223d1eabbedf1b063567034e1463a11d7c82d1820f26b75d14e3c/onnxconverter_common-1.6.0-py2.py3-none-any.whl (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.9MB/s \n",
            "\u001b[?25hCollecting fire\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/69/faeaae8687f4de0f5973694d02e9d6c3eb827636a009157352d98de1129e/fire-0.2.1.tar.gz (76kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 5.3MB/s \n",
            "\u001b[?25hCollecting onnx\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/f4/e126b60d109ad1e80020071484b935980b7cce1e4796073aab086a2d6902/onnx-1.6.0-cp36-cp36m-manylinux1_x86_64.whl (4.8MB)\n",
            "\u001b[K     |████████████████████████████████| 4.8MB 10.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras2onnx) (1.17.5)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from keras2onnx) (3.10.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->keras2onnx) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->keras2onnx) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->keras2onnx) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->keras2onnx) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from onnxconverter-common>=1.6.0->keras2onnx) (1.12.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire->keras2onnx) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.6/dist-packages (from onnx->keras2onnx) (3.6.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->keras2onnx) (45.1.0)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.2.1-py2.py3-none-any.whl size=103528 sha256=9f398be631d2888a30784e214a3e71387e699ccdffdd81bdfe2b3fa3ea7459a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/9c/c0/07b6dc7faf1844bb4688f46b569efe6cafaa2179c95db821da\n",
            "Successfully built fire\n",
            "Installing collected packages: onnx, onnxconverter-common, fire, keras2onnx\n",
            "Successfully installed fire-0.2.1 keras2onnx-1.6.0 onnx-1.6.0 onnxconverter-common-1.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iw5dkMYhJIMF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "outputId": "105a1c33-9872-4831-db2d-e33f10545483"
      },
      "source": [
        "pip install skl2onnx"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting skl2onnx\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/8b/21a32b9f4669959678db5abe41774b2ce0544a92ed42683cf79b065d7a8b/skl2onnx-1.6.0-py2.py3-none-any.whl (161kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 5.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.19 in /usr/local/lib/python3.6/dist-packages (from skl2onnx) (0.22.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from skl2onnx) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from skl2onnx) (1.17.5)\n",
            "Collecting onnxconverter-common>=1.5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/3d/6112c19223d1eabbedf1b063567034e1463a11d7c82d1820f26b75d14e3c/onnxconverter_common-1.6.0-py2.py3-none-any.whl (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from skl2onnx) (1.12.0)\n",
            "Collecting onnx>=1.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/f4/e126b60d109ad1e80020071484b935980b7cce1e4796073aab086a2d6902/onnx-1.6.0-cp36-cp36m-manylinux1_x86_64.whl (4.8MB)\n",
            "\u001b[K     |████████████████████████████████| 4.8MB 57.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.19->skl2onnx) (0.14.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.19->skl2onnx) (1.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->skl2onnx) (45.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.6/dist-packages (from onnx>=1.2.1->skl2onnx) (3.6.6)\n",
            "Installing collected packages: onnx, onnxconverter-common, skl2onnx\n",
            "Successfully installed onnx-1.6.0 onnxconverter-common-1.6.0 skl2onnx-1.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKUe4cE1EaWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting from keras to onnx\n",
        "import os\n",
        "\n",
        "if not os.path.exists('mymodel.onnx'):\n",
        "    from keras2onnx import convert_keras\n",
        "    onx = convert_keras(nn_dropout, 'mymodel.onnx')\n",
        "    with open(\"mymodel.onnx\", \"wb\") as f:\n",
        "        f.write(onx.SerializeToString())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvqE7xDiJIMR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting from sklearn to onnx\n",
        "import os\n",
        "\n",
        "if not os.path.exists('mymodel.onnx'):\n",
        "    from skl2onnx import convert_sklearn\n",
        "    from skl2onnx.common.data_types import FloatTensorType\n",
        "    initial_type = [('mymodel.onnx', FloatTensorType([None, 4]))]\n",
        "    onx = convert_sklearn(log_gscv, initial_types=initial_type)\n",
        "    with open(\"mymodel.onnx\", \"wb\") as f:\n",
        "        f.write(onx.SerializeToString())\n",
        "        \n",
        "# from skl2onnx import convert_sklearn\n",
        "# from skl2onnx.common.data_types import FloatTensorType\n",
        "# initial_type = [('float_input', FloatTensorType([None, 4]))]\n",
        "# onx = convert_sklearn(log_gscv, initial_types=initial_type)\n",
        "# with open(\"rf_iris.onnx\", \"wb\") as f:\n",
        "#     f.write(onx.SerializeToString())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKUqL3a3JIMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saving logistic regression model to pkl file\n",
        "\n",
        "aws_key_password_region = pickle.load(open( \"worldhappiness_modelsubmission_keys (2).pkl\", \"rb\" ) ) \n",
        "# pickle.dump(log_gscv, open( \"rff_model.pkl\", \"wb\" ) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzHTPdVzJIMq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "apiurl=\"https://btuvanmi55.execute-api.us-east-1.amazonaws.com/prod/m\"\n",
        "username = \"Paarth_Malkan\"\n",
        "password = \"Columbia1Temple1!\"\n",
        "\n",
        "region='us-east-1'\n",
        "model_filepath=\"mymodel.onnx\"   \n",
        "preprocessor_filepath=\"preprocessor.pkl\"\n",
        "preprocessor=\"TRUE\"\n",
        "\n",
        "trainingdata=X_train\n",
        "\n",
        "# Set aws keys for this project (these keys give you access to collaborate on a single project)\n",
        "\n",
        "#Importing from object that stores keys so we do not print out keys for others to see.\n",
        "\n",
        "aws_key_password_region = pickle.load( open(\"worldhappiness_modelsubmission_keys (2).pkl\", \"rb\" ) )\n",
        "\n",
        "aws_key=aws_key_password_region[0]\n",
        "aws_password=aws_key_password_region[1]\n",
        "region=aws_key_password_region[2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmSMcQdlIDEG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "outputId": "d6b1df55-c40d-4a1c-98fb-418d33f0a498"
      },
      "source": [
        "# Submitting and viewing leaderboard for vanilla nn results\n",
        "\n",
        "import aimodelshare as ai\n",
        "\n",
        "ai.submit_model(model_filepath=model_filepath, model_eval_metrics=nn_model_eval,apiurl=apiurl, username=username, password=password, aws_key=aws_key,aws_password=aws_password, region=region, trainingdata=trainingdata,preprocessor_filepath=preprocessor_filepath,preprocessor=preprocessor)\n",
        "\n",
        "leaderboard = ai.get_leaderboard(apiurl, username, password, aws_key, aws_password, region)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"mymodel.onnx\" has been loaded to version 119 of your prediction API.\n",
            "This version of the model will be used by your prediction api for all future predictions automatically.\n",
            "If you wish to use an older version of the model, please reference the getting started guide at aimodelshare.com.\n",
            "LEADERBOARD RANKINGS:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>f1_score</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>mse</th>\n",
              "      <th>rmse</th>\n",
              "      <th>mae</th>\n",
              "      <th>r2</th>\n",
              "      <th>username</th>\n",
              "      <th>model_version</th>\n",
              "      <th>avg_ranking_classification</th>\n",
              "      <th>avg_ranking_regression</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.717949</td>\n",
              "      <td>0.717857</td>\n",
              "      <td>0.717857</td>\n",
              "      <td>0.727778</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3scman</td>\n",
              "      <td>85</td>\n",
              "      <td>2.333333</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.717949</td>\n",
              "      <td>0.713796</td>\n",
              "      <td>0.719444</td>\n",
              "      <td>0.725000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3scman</td>\n",
              "      <td>70</td>\n",
              "      <td>2.333333</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.675975</td>\n",
              "      <td>0.754286</td>\n",
              "      <td>0.700952</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>dhoward97</td>\n",
              "      <td>69</td>\n",
              "      <td>2.666667</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.693333</td>\n",
              "      <td>0.700397</td>\n",
              "      <td>0.702778</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3scman</td>\n",
              "      <td>62</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.641026</td>\n",
              "      <td>0.642381</td>\n",
              "      <td>0.743590</td>\n",
              "      <td>0.682273</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>SUN-Wenjun</td>\n",
              "      <td>83</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.337698</td>\n",
              "      <td>0.385556</td>\n",
              "      <td>0.322222</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>username2</td>\n",
              "      <td>2</td>\n",
              "      <td>46.000000</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.337698</td>\n",
              "      <td>0.385556</td>\n",
              "      <td>0.322222</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>username2</td>\n",
              "      <td>3</td>\n",
              "      <td>46.000000</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.337698</td>\n",
              "      <td>0.385556</td>\n",
              "      <td>0.322222</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>username2</td>\n",
              "      <td>6</td>\n",
              "      <td>46.000000</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.336015</td>\n",
              "      <td>0.354048</td>\n",
              "      <td>0.325000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>raquel904</td>\n",
              "      <td>101</td>\n",
              "      <td>46.666667</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.336015</td>\n",
              "      <td>0.354048</td>\n",
              "      <td>0.325000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>raquel904</td>\n",
              "      <td>102</td>\n",
              "      <td>46.666667</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>120 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     accuracy  f1_score  ...  avg_ranking_classification  avg_ranking_regression\n",
              "1    0.717949  0.717857  ...                    2.333333                     1.0\n",
              "2    0.717949  0.713796  ...                    2.333333                     1.0\n",
              "3    0.666667  0.675975  ...                    2.666667                     1.0\n",
              "4    0.692308  0.693333  ...                    4.000000                     1.0\n",
              "5    0.641026  0.642381  ...                    4.000000                     1.0\n",
              "..        ...       ...  ...                         ...                     ...\n",
              "107  0.333333  0.337698  ...                   46.000000                     1.0\n",
              "106  0.333333  0.337698  ...                   46.000000                     1.0\n",
              "105  0.333333  0.337698  ...                   46.000000                     1.0\n",
              "109  0.333333  0.336015  ...                   46.666667                     1.0\n",
              "108  0.333333  0.336015  ...                   46.666667                     1.0\n",
              "\n",
              "[120 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYNCxeJXJIM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}